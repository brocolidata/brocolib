{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"brocolib Development Brocoli Library for Data Processing Rebuild brocolib source distribution+wheel. Prerequisites : Make sure all packages mentioned in src/*/setup.py (in install_requires ) are also in docker_build/requirements.txt and that the Docker image is fresh. Open a terminal inside the container and run python3 setup.py sdist bdist_wheel Run documentation website (does not work for brocolib_utils yet) portray server - m brocolib_extract_load or portray server -m brocolib_transform","title":"Home"},{"location":"#brocolib-development","text":"Brocoli Library for Data Processing","title":"brocolib Development"},{"location":"#rebuild-brocolib-source-distributionwheel","text":"Prerequisites : Make sure all packages mentioned in src/*/setup.py (in install_requires ) are also in docker_build/requirements.txt and that the Docker image is fresh. Open a terminal inside the container and run python3 setup.py sdist bdist_wheel","title":"Rebuild brocolib source distribution+wheel."},{"location":"#run-documentation-website","text":"(does not work for brocolib_utils yet) portray server - m brocolib_extract_load or portray server -m brocolib_transform","title":"Run documentation website"},{"location":"reference/brocolib_extract_load/","text":"Module brocolib_extract_load View Source # from .datalake import * # from .datawarehouse import * # from .ingest import * # from .processing import * Sub-modules brocolib_extract_load.datalake brocolib_extract_load.datawarehouse brocolib_extract_load.goblet_utils brocolib_extract_load.ingest brocolib_extract_load.processing brocolib_extract_load.pubsub","title":"Index"},{"location":"reference/brocolib_extract_load/#module-brocolib_extract_load","text":"View Source # from .datalake import * # from .datawarehouse import * # from .ingest import * # from .processing import *","title":"Module brocolib_extract_load"},{"location":"reference/brocolib_extract_load/#sub-modules","text":"brocolib_extract_load.datalake brocolib_extract_load.datawarehouse brocolib_extract_load.goblet_utils brocolib_extract_load.ingest brocolib_extract_load.processing brocolib_extract_load.pubsub","title":"Sub-modules"},{"location":"reference/brocolib_extract_load/datalake/","text":"Module brocolib_extract_load.datalake View Source import pandas as pd from .pubsub import publish_sources from datetime import datetime def dataframe_to_bucket ( dataframe : pd . DataFrame , bucket_name : str , blob_name : str , file_type : str , logger = None ) -> str : \"\"\"Loads a DataFrame to a GCS bucket Args: dataframe (pd.DataFrame): DataFrame you want to upload to GCS bucket_name (str): Name of the destination bucket blob_name (str): Name of the destination file file_type (str): File format logger (optional): Logging interface. Defaults to None. Parameters example: blob_name= 'folder/subfolder/filename.csv' bucket_name = 'PROJECT_ID-landing' Raises: NotImplementedError: If the file format is not implemented Returns: str: GCS path where the blob is uploaded \"\"\" gcs_path_temp = f \"gs:// { bucket_name } / { blob_name } . {{ file_extension }} \" if file_type . lower () == 'csv' : gcs_path = gcs_path_temp . format ( file_extension = \"csv\" ) dataframe . to_csv ( gcs_path , index = False ) elif file_type . lower () == 'parquet' : gcs_path = gcs_path_temp . format ( file_extension = \"parquet\" ) dataframe . to_parquet ( gcs_path , index = False ) elif file_type . lower () == 'json' : gcs_path = gcs_path_temp . format ( file_extension = \"json\" ) dataframe . to_json ( gcs_path , index = False ) else : raise NotImplementedError ( f \" { file_type } is not implemented.\" ) if logger : logger . info ( f 'Load Destination : { gcs_path } ' ) return gcs_path def bucket_to_dataframe ( bucket_name : str , blob_name : str , file_type : str ) -> pd . DataFrame : ''' Loads a file located in a GCS bucket into a DataFrame Parameters: bucket_name (str): Name of the source bucket blob_name (str): Name of the blob in the source bucket file_type (str): Type of the file in the bucket Returns: pandas.DataFrame: fetched DataFrame ''' file_type = file_type . lower () url = f 'gs:// { bucket_name } / { blob_name } ' print ( f 'using { url } ' ) if file_type == 'csv' : return pd . read_csv ( url ) class ExternalTable : def __init__ ( self , bucket_name : str , partition_keys : dict , bucket_file : str , bucket_table_directory : str , bucket_directory : str , dbt_topic : str , gcp_project : str , logger = None ): \"\"\"Instanciate a ExternalTable object Args: bucket_name (str): Name of the the GCS bucket where the data is located partition_keys (dict): Pairs of partition keys and values bucket_file (str): Name of the file in GCS bucket bucket_table_directory (str): Name of the directory after which the ExternalTable is named bucket_directory (str): Name of the subdirectories under the bucket's root level dbt_topic (str): Name of the Pub/Sub dbt topic gcp_project (str): Namae of the GCP project logger (_type_, optional): Logging interface. Defaults to None. \"\"\" self . bucket_name = bucket_name self . partition_keys = partition_keys self . subfolders = bucket_directory self . source_name = bucket_table_directory self . bucket_table_directory = bucket_table_directory self . file_name = bucket_file self . dbt_topic = dbt_topic self . gcp_project = gcp_project self . logger = logger if logger else None self . blob_name = self . format_filename () self . gcs_path = None def add_partition_keys ( self , path_prefix : str ) -> str : \"\"\"Add partition_keys in Hive Format to a GCS path prefix Args: path_prefix (str) : GCS path prefix Returns: str: GCS Path prefix appended by partition keys \"\"\" now = datetime . now () for key , value in self . partition_keys . items (): if key == \"year\" : value = now . year elif key == \"month\" : value = now . month path_prefix += f \"/ { key } = { value } \" return path_prefix def format_filename ( self ) -> str : now = datetime . now () path_prefix = self . add_partition_keys ( f \" { self . subfolders } / { self . bucket_table_directory } \" ) return f \" { path_prefix } / { self . file_name } _ { str ( now . day ) } \" def to_datalake ( self , df , logger = None ): self . gcs_path = dataframe_to_bucket ( dataframe = df , bucket_name = self . bucket_name , blob_name = self . blob_name , file_type = \"parquet\" , logger = logger ) def publish_message ( self ): publish_sources ( sources = [ self . source_name ], dbt_topic = self . dbt_topic , gcp_project = self . gcp_project , logger = self . logger ) Functions bucket_to_dataframe def bucket_to_dataframe ( bucket_name : str , blob_name : str , file_type : str ) -> pandas . core . frame . DataFrame Loads a file located in a GCS bucket into a DataFrame Parameters: Name Type Description Default bucket_name str Name of the source bucket None blob_name str Name of the blob in the source bucket None file_type str Type of the file in the bucket None Returns: Type Description pandas.DataFrame fetched DataFrame View Source def bucket_to_dataframe ( bucket_name : str , blob_name : str , file_type : str ) -> pd . DataFrame : ''' Loads a file located in a GCS bucket into a DataFrame Parameters: bucket_name (str): Name of the source bucket blob_name (str): Name of the blob in the source bucket file_type (str): Type of the file in the bucket Returns: pandas.DataFrame: fetched DataFrame ''' file_type = file_type . lower () url = f 'gs://{bucket_name}/{blob_name}' print ( f 'using {url}' ) if file_type == 'csv' : return pd . read_csv ( url ) dataframe_to_bucket def dataframe_to_bucket ( dataframe : pandas . core . frame . DataFrame , bucket_name : str , blob_name : str , file_type : str , logger = None ) -> str Loads a DataFrame to a GCS bucket Parameters: Name Type Description Default dataframe pd.DataFrame DataFrame you want to upload to GCS None bucket_name str Name of the destination bucket None blob_name str Name of the destination file None file_type str File format None logger optional Logging interface. Defaults to None. None Returns: Type Description str GCS path where the blob is uploaded Raises: Type Description NotImplementedError If the file format is not implemented View Source def dataframe_to_bucket ( dataframe : pd . DataFrame , bucket_name : str , blob_name : str , file_type : str , logger = None ) -> str : \"\"\"Loads a DataFrame to a GCS bucket Args: dataframe (pd.DataFrame): DataFrame you want to upload to GCS bucket_name (str): Name of the destination bucket blob_name (str): Name of the destination file file_type (str): File format logger (optional): Logging interface. Defaults to None. Parameters example: blob_name= 'folder/subfolder/filename.csv' bucket_name = 'PROJECT_ID-landing' Raises: NotImplementedError: If the file format is not implemented Returns: str: GCS path where the blob is uploaded \"\"\" gcs_path_temp = f \"gs://{bucket_name}/{blob_name}.{{file_extension}}\" if file_type . lower () == 'csv' : gcs_path = gcs_path_temp . format ( file_extension = \"csv\" ) dataframe . to_csv ( gcs_path , index = False ) elif file_type . lower () == 'parquet' : gcs_path = gcs_path_temp . format ( file_extension = \"parquet\" ) dataframe . to_parquet ( gcs_path , index = False ) elif file_type . lower () == 'json' : gcs_path = gcs_path_temp . format ( file_extension = \"json\" ) dataframe . to_json ( gcs_path , index = False ) else : raise NotImplementedError ( f \"{file_type} is not implemented.\" ) if logger : logger . info ( f 'Load Destination : {gcs_path}' ) return gcs_path Classes ExternalTable class ExternalTable ( bucket_name : str , partition_keys : dict , bucket_file : str , bucket_table_directory : str , bucket_directory : str , dbt_topic : str , gcp_project : str , logger = None ) View Source class ExternalTable: def __init__ ( self , bucket_name: str , partition_keys: dict , bucket_file: str , bucket_table_directory: str , bucket_directory: str , dbt_topic: str , gcp_project: str , logger = None ): \"\"\"Instanciate a ExternalTable object Args: bucket_name (str): Name of the the GCS bucket where the data is located partition_keys (dict): Pairs of partition keys and values bucket_file (str): Name of the file in GCS bucket bucket_table_directory (str): Name of the directory after which the ExternalTable is named bucket_directory (str): Name of the subdirectories under the bucket's root level dbt_topic (str): Name of the Pub/Sub dbt topic gcp_project (str): Namae of the GCP project logger (_type_, optional): Logging interface. Defaults to None. \"\"\" self . bucket_name = bucket_name self . partition_keys = partition_keys self . subfolders = bucket_directory self . source_name = bucket_table_directory self . bucket_table_directory = bucket_table_directory self . file_name = bucket_file self . dbt_topic = dbt_topic self . gcp_project = gcp_project self . logger = logger if logger else None self . blob_name = self . format_filename () self . gcs_path = None def add_partition_keys ( self , path_prefix: str ) -> str: \"\"\"Add partition_keys in Hive Format to a GCS path prefix Args: path_prefix (str) : GCS path prefix Returns: str: GCS Path prefix appended by partition keys \"\"\" now = datetime . now () for key , value in self . partition_keys . items (): if key == \"year\" : value = now . year elif key == \"month\" : value = now . month path_prefix += f \"/{key}={value}\" return path_prefix def format_filename ( self ) -> str: now = datetime . now () path_prefix = self . add_partition_keys ( f \"{self.subfolders}/{self.bucket_table_directory}\" ) return f \"{path_prefix}/{self.file_name}_{str(now.day)}\" def to_datalake ( self , df , logger = None ): self . gcs_path = dataframe_to_bucket ( dataframe = df , bucket_name = self . bucket_name , blob_name = self . blob_name , file_type = \"parquet\" , logger = logger ) def publish_message ( self ): publish_sources ( sources =[ self . source_name ], dbt_topic = self . dbt_topic , gcp_project = self . gcp_project , logger = self . logger ) Methods add_partition_keys def add_partition_keys ( self , path_prefix : str ) -> str Add partition_keys in Hive Format to a GCS path prefix Parameters: Name Type Description Default path_prefix str GCS path prefix None Returns: Type Description str GCS Path prefix appended by partition keys View Source def add_partition_keys ( self , path_prefix : str ) -> str : \"\"\"Add partition_keys in Hive Format to a GCS path prefix Args: path_prefix (str) : GCS path prefix Returns: str: GCS Path prefix appended by partition keys \"\"\" now = datetime . now () for key , value in self . partition_keys . items () : if key == \"year\" : value = now . year elif key == \"month\" : value = now . month path_prefix += f \"/{key}={value}\" return path_prefix format_filename def format_filename ( self ) -> str View Source def format_filename ( self ) -> str : now = datetime . now () path_prefix = self . add_partition_keys ( f \"{self.subfolders}/{self.bucket_table_directory}\" ) return f \"{path_prefix}/{self.file_name}_{str(now.day)}\" publish_message def publish_message ( self ) View Source def publish_message(self): publish_sources( sources=[self.source_name], dbt_topic=self.dbt_topic, gcp_project=self.gcp_project, logger=self.logger ) to_datalake def to_datalake ( self , df , logger = None ) View Source def to_datalake(self, df, logger=None): self.gcs_path = dataframe_to_bucket( dataframe=df, bucket_name=self.bucket_name, blob_name=self.blob_name, file_type=\"parquet\", logger=logger )","title":"Datalake"},{"location":"reference/brocolib_extract_load/datalake/#module-brocolib_extract_loaddatalake","text":"View Source import pandas as pd from .pubsub import publish_sources from datetime import datetime def dataframe_to_bucket ( dataframe : pd . DataFrame , bucket_name : str , blob_name : str , file_type : str , logger = None ) -> str : \"\"\"Loads a DataFrame to a GCS bucket Args: dataframe (pd.DataFrame): DataFrame you want to upload to GCS bucket_name (str): Name of the destination bucket blob_name (str): Name of the destination file file_type (str): File format logger (optional): Logging interface. Defaults to None. Parameters example: blob_name= 'folder/subfolder/filename.csv' bucket_name = 'PROJECT_ID-landing' Raises: NotImplementedError: If the file format is not implemented Returns: str: GCS path where the blob is uploaded \"\"\" gcs_path_temp = f \"gs:// { bucket_name } / { blob_name } . {{ file_extension }} \" if file_type . lower () == 'csv' : gcs_path = gcs_path_temp . format ( file_extension = \"csv\" ) dataframe . to_csv ( gcs_path , index = False ) elif file_type . lower () == 'parquet' : gcs_path = gcs_path_temp . format ( file_extension = \"parquet\" ) dataframe . to_parquet ( gcs_path , index = False ) elif file_type . lower () == 'json' : gcs_path = gcs_path_temp . format ( file_extension = \"json\" ) dataframe . to_json ( gcs_path , index = False ) else : raise NotImplementedError ( f \" { file_type } is not implemented.\" ) if logger : logger . info ( f 'Load Destination : { gcs_path } ' ) return gcs_path def bucket_to_dataframe ( bucket_name : str , blob_name : str , file_type : str ) -> pd . DataFrame : ''' Loads a file located in a GCS bucket into a DataFrame Parameters: bucket_name (str): Name of the source bucket blob_name (str): Name of the blob in the source bucket file_type (str): Type of the file in the bucket Returns: pandas.DataFrame: fetched DataFrame ''' file_type = file_type . lower () url = f 'gs:// { bucket_name } / { blob_name } ' print ( f 'using { url } ' ) if file_type == 'csv' : return pd . read_csv ( url ) class ExternalTable : def __init__ ( self , bucket_name : str , partition_keys : dict , bucket_file : str , bucket_table_directory : str , bucket_directory : str , dbt_topic : str , gcp_project : str , logger = None ): \"\"\"Instanciate a ExternalTable object Args: bucket_name (str): Name of the the GCS bucket where the data is located partition_keys (dict): Pairs of partition keys and values bucket_file (str): Name of the file in GCS bucket bucket_table_directory (str): Name of the directory after which the ExternalTable is named bucket_directory (str): Name of the subdirectories under the bucket's root level dbt_topic (str): Name of the Pub/Sub dbt topic gcp_project (str): Namae of the GCP project logger (_type_, optional): Logging interface. Defaults to None. \"\"\" self . bucket_name = bucket_name self . partition_keys = partition_keys self . subfolders = bucket_directory self . source_name = bucket_table_directory self . bucket_table_directory = bucket_table_directory self . file_name = bucket_file self . dbt_topic = dbt_topic self . gcp_project = gcp_project self . logger = logger if logger else None self . blob_name = self . format_filename () self . gcs_path = None def add_partition_keys ( self , path_prefix : str ) -> str : \"\"\"Add partition_keys in Hive Format to a GCS path prefix Args: path_prefix (str) : GCS path prefix Returns: str: GCS Path prefix appended by partition keys \"\"\" now = datetime . now () for key , value in self . partition_keys . items (): if key == \"year\" : value = now . year elif key == \"month\" : value = now . month path_prefix += f \"/ { key } = { value } \" return path_prefix def format_filename ( self ) -> str : now = datetime . now () path_prefix = self . add_partition_keys ( f \" { self . subfolders } / { self . bucket_table_directory } \" ) return f \" { path_prefix } / { self . file_name } _ { str ( now . day ) } \" def to_datalake ( self , df , logger = None ): self . gcs_path = dataframe_to_bucket ( dataframe = df , bucket_name = self . bucket_name , blob_name = self . blob_name , file_type = \"parquet\" , logger = logger ) def publish_message ( self ): publish_sources ( sources = [ self . source_name ], dbt_topic = self . dbt_topic , gcp_project = self . gcp_project , logger = self . logger )","title":"Module brocolib_extract_load.datalake"},{"location":"reference/brocolib_extract_load/datalake/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_extract_load/datalake/#bucket_to_dataframe","text":"def bucket_to_dataframe ( bucket_name : str , blob_name : str , file_type : str ) -> pandas . core . frame . DataFrame Loads a file located in a GCS bucket into a DataFrame Parameters: Name Type Description Default bucket_name str Name of the source bucket None blob_name str Name of the blob in the source bucket None file_type str Type of the file in the bucket None Returns: Type Description pandas.DataFrame fetched DataFrame View Source def bucket_to_dataframe ( bucket_name : str , blob_name : str , file_type : str ) -> pd . DataFrame : ''' Loads a file located in a GCS bucket into a DataFrame Parameters: bucket_name (str): Name of the source bucket blob_name (str): Name of the blob in the source bucket file_type (str): Type of the file in the bucket Returns: pandas.DataFrame: fetched DataFrame ''' file_type = file_type . lower () url = f 'gs://{bucket_name}/{blob_name}' print ( f 'using {url}' ) if file_type == 'csv' : return pd . read_csv ( url )","title":"bucket_to_dataframe"},{"location":"reference/brocolib_extract_load/datalake/#dataframe_to_bucket","text":"def dataframe_to_bucket ( dataframe : pandas . core . frame . DataFrame , bucket_name : str , blob_name : str , file_type : str , logger = None ) -> str Loads a DataFrame to a GCS bucket Parameters: Name Type Description Default dataframe pd.DataFrame DataFrame you want to upload to GCS None bucket_name str Name of the destination bucket None blob_name str Name of the destination file None file_type str File format None logger optional Logging interface. Defaults to None. None Returns: Type Description str GCS path where the blob is uploaded Raises: Type Description NotImplementedError If the file format is not implemented View Source def dataframe_to_bucket ( dataframe : pd . DataFrame , bucket_name : str , blob_name : str , file_type : str , logger = None ) -> str : \"\"\"Loads a DataFrame to a GCS bucket Args: dataframe (pd.DataFrame): DataFrame you want to upload to GCS bucket_name (str): Name of the destination bucket blob_name (str): Name of the destination file file_type (str): File format logger (optional): Logging interface. Defaults to None. Parameters example: blob_name= 'folder/subfolder/filename.csv' bucket_name = 'PROJECT_ID-landing' Raises: NotImplementedError: If the file format is not implemented Returns: str: GCS path where the blob is uploaded \"\"\" gcs_path_temp = f \"gs://{bucket_name}/{blob_name}.{{file_extension}}\" if file_type . lower () == 'csv' : gcs_path = gcs_path_temp . format ( file_extension = \"csv\" ) dataframe . to_csv ( gcs_path , index = False ) elif file_type . lower () == 'parquet' : gcs_path = gcs_path_temp . format ( file_extension = \"parquet\" ) dataframe . to_parquet ( gcs_path , index = False ) elif file_type . lower () == 'json' : gcs_path = gcs_path_temp . format ( file_extension = \"json\" ) dataframe . to_json ( gcs_path , index = False ) else : raise NotImplementedError ( f \"{file_type} is not implemented.\" ) if logger : logger . info ( f 'Load Destination : {gcs_path}' ) return gcs_path","title":"dataframe_to_bucket"},{"location":"reference/brocolib_extract_load/datalake/#classes","text":"","title":"Classes"},{"location":"reference/brocolib_extract_load/datalake/#externaltable","text":"class ExternalTable ( bucket_name : str , partition_keys : dict , bucket_file : str , bucket_table_directory : str , bucket_directory : str , dbt_topic : str , gcp_project : str , logger = None ) View Source class ExternalTable: def __init__ ( self , bucket_name: str , partition_keys: dict , bucket_file: str , bucket_table_directory: str , bucket_directory: str , dbt_topic: str , gcp_project: str , logger = None ): \"\"\"Instanciate a ExternalTable object Args: bucket_name (str): Name of the the GCS bucket where the data is located partition_keys (dict): Pairs of partition keys and values bucket_file (str): Name of the file in GCS bucket bucket_table_directory (str): Name of the directory after which the ExternalTable is named bucket_directory (str): Name of the subdirectories under the bucket's root level dbt_topic (str): Name of the Pub/Sub dbt topic gcp_project (str): Namae of the GCP project logger (_type_, optional): Logging interface. Defaults to None. \"\"\" self . bucket_name = bucket_name self . partition_keys = partition_keys self . subfolders = bucket_directory self . source_name = bucket_table_directory self . bucket_table_directory = bucket_table_directory self . file_name = bucket_file self . dbt_topic = dbt_topic self . gcp_project = gcp_project self . logger = logger if logger else None self . blob_name = self . format_filename () self . gcs_path = None def add_partition_keys ( self , path_prefix: str ) -> str: \"\"\"Add partition_keys in Hive Format to a GCS path prefix Args: path_prefix (str) : GCS path prefix Returns: str: GCS Path prefix appended by partition keys \"\"\" now = datetime . now () for key , value in self . partition_keys . items (): if key == \"year\" : value = now . year elif key == \"month\" : value = now . month path_prefix += f \"/{key}={value}\" return path_prefix def format_filename ( self ) -> str: now = datetime . now () path_prefix = self . add_partition_keys ( f \"{self.subfolders}/{self.bucket_table_directory}\" ) return f \"{path_prefix}/{self.file_name}_{str(now.day)}\" def to_datalake ( self , df , logger = None ): self . gcs_path = dataframe_to_bucket ( dataframe = df , bucket_name = self . bucket_name , blob_name = self . blob_name , file_type = \"parquet\" , logger = logger ) def publish_message ( self ): publish_sources ( sources =[ self . source_name ], dbt_topic = self . dbt_topic , gcp_project = self . gcp_project , logger = self . logger )","title":"ExternalTable"},{"location":"reference/brocolib_extract_load/datalake/#methods","text":"","title":"Methods"},{"location":"reference/brocolib_extract_load/datalake/#add_partition_keys","text":"def add_partition_keys ( self , path_prefix : str ) -> str Add partition_keys in Hive Format to a GCS path prefix Parameters: Name Type Description Default path_prefix str GCS path prefix None Returns: Type Description str GCS Path prefix appended by partition keys View Source def add_partition_keys ( self , path_prefix : str ) -> str : \"\"\"Add partition_keys in Hive Format to a GCS path prefix Args: path_prefix (str) : GCS path prefix Returns: str: GCS Path prefix appended by partition keys \"\"\" now = datetime . now () for key , value in self . partition_keys . items () : if key == \"year\" : value = now . year elif key == \"month\" : value = now . month path_prefix += f \"/{key}={value}\" return path_prefix","title":"add_partition_keys"},{"location":"reference/brocolib_extract_load/datalake/#format_filename","text":"def format_filename ( self ) -> str View Source def format_filename ( self ) -> str : now = datetime . now () path_prefix = self . add_partition_keys ( f \"{self.subfolders}/{self.bucket_table_directory}\" ) return f \"{path_prefix}/{self.file_name}_{str(now.day)}\"","title":"format_filename"},{"location":"reference/brocolib_extract_load/datalake/#publish_message","text":"def publish_message ( self ) View Source def publish_message(self): publish_sources( sources=[self.source_name], dbt_topic=self.dbt_topic, gcp_project=self.gcp_project, logger=self.logger )","title":"publish_message"},{"location":"reference/brocolib_extract_load/datalake/#to_datalake","text":"def to_datalake ( self , df , logger = None ) View Source def to_datalake(self, df, logger=None): self.gcs_path = dataframe_to_bucket( dataframe=df, bucket_name=self.bucket_name, blob_name=self.blob_name, file_type=\"parquet\", logger=logger )","title":"to_datalake"},{"location":"reference/brocolib_extract_load/datawarehouse/","text":"Module brocolib_extract_load.datawarehouse View Source from google.cloud import bigquery def create_external_table ( project_id , bucket_name_destination , file_type , file_name , table_ref , dataset , folder_name , schemas_dict ): ''' Function create an external table from files in GCS Parameters: project_id(str): ID of the project bucket_name_destination (str): Name of the bucket file_type(str) : the type of the file file_name (str) : Name of the file table_ref (str) : Name of the table created dataset (str) : the dataset where the table is created schemas_dict (dict) : dict with fields as keys and types as values Returns: external table (bigquery.Table): created external table ''' client = bigquery . Client ( project_id ) #Define your schema ls_schemas = [] for key , value in schemas_dict . items (): schemafield = bigquery . schema . SchemaField ( key , value ) ls_schemas . append ( schemafield ) dataset_ref = client . dataset ( dataset ) table_ref = bigquery . TableReference ( dataset_ref , table_ref ) table = bigquery . Table ( table_ref , ls_schemas ) external_config = bigquery . ExternalConfig ( file_type ) source_uris = [ f 'gs:// { bucket_name_destination } / { folder_name } /*/ { file_name } ' ] #i.e for a csv file in a Cloud Storage bucket #it would be something like \"gs://<your-bucket>/<your-csv-file>\" external_config . source_uris = source_uris file_type = file_type . upper () if file_type == 'CSV' : external_config . options . field_delimiter = \",\" external_config . options . encoding = \"UTF-8\" external_config . options . skip_leading_rows = 1 else : raise NotImplementedError table . external_data_configuration = external_config # external_config.options.quote_character client . create_table ( table ) Functions create_external_table def create_external_table ( project_id , bucket_name_destination , file_type , file_name , table_ref , dataset , folder_name , schemas_dict ) Function create an external table from files in GCS Parameters: Name Type Description Default project_id str ID of the project None bucket_name_destination str Name of the bucket None file_type str the type of the file None file_name str Name of the file None table_ref str Name of the table created None dataset str the dataset where the table is created None schemas_dict dict dict with fields as keys and types as values None Returns: Type Description None external table (bigquery.Table): created external table View Source def create_external_table ( project_id , bucket_name_destination , file_type , file_name , table_ref , dataset , folder_name , schemas_dict ) : '' ' Function create an external table from files in GCS Parameters: project_id(str): ID of the project bucket_name_destination (str): Name of the bucket file_type(str) : the type of the file file_name (str) : Name of the file table_ref (str) : Name of the table created dataset (str) : the dataset where the table is created schemas_dict (dict) : dict with fields as keys and types as values Returns: external table (bigquery.Table): created external table '' ' client = bigquery.Client(project_id) #Define your schema ls_schemas =[] for key,value in schemas_dict.items(): schemafield = bigquery.schema.SchemaField(key,value) ls_schemas.append(schemafield) dataset_ref = client.dataset(dataset) table_ref = bigquery.TableReference(dataset_ref, table_ref ) table = bigquery.Table(table_ref, ls_schemas) external_config = bigquery.ExternalConfig(file_type) source_uris = [ f 'gs://{bucket_name_destination}/{folder_name}/*/{file_name}' ] # i . e for a csv file in a Cloud Storage bucket # it would be something like \"gs://<your-bucket>/<your-csv-file>\" external_config . source_uris = source_uris file_type = file_type . upper () if file_type == 'CSV' : external_config . options . field_delimiter = \",\" external_config . options . encoding = \"UTF-8\" external_config . options . skip_leading_rows = 1 else : raise NotImplementedError table . external_data_configuration = external_config # external_config . options . quote_character client . create_table ( table )","title":"Datawarehouse"},{"location":"reference/brocolib_extract_load/datawarehouse/#module-brocolib_extract_loaddatawarehouse","text":"View Source from google.cloud import bigquery def create_external_table ( project_id , bucket_name_destination , file_type , file_name , table_ref , dataset , folder_name , schemas_dict ): ''' Function create an external table from files in GCS Parameters: project_id(str): ID of the project bucket_name_destination (str): Name of the bucket file_type(str) : the type of the file file_name (str) : Name of the file table_ref (str) : Name of the table created dataset (str) : the dataset where the table is created schemas_dict (dict) : dict with fields as keys and types as values Returns: external table (bigquery.Table): created external table ''' client = bigquery . Client ( project_id ) #Define your schema ls_schemas = [] for key , value in schemas_dict . items (): schemafield = bigquery . schema . SchemaField ( key , value ) ls_schemas . append ( schemafield ) dataset_ref = client . dataset ( dataset ) table_ref = bigquery . TableReference ( dataset_ref , table_ref ) table = bigquery . Table ( table_ref , ls_schemas ) external_config = bigquery . ExternalConfig ( file_type ) source_uris = [ f 'gs:// { bucket_name_destination } / { folder_name } /*/ { file_name } ' ] #i.e for a csv file in a Cloud Storage bucket #it would be something like \"gs://<your-bucket>/<your-csv-file>\" external_config . source_uris = source_uris file_type = file_type . upper () if file_type == 'CSV' : external_config . options . field_delimiter = \",\" external_config . options . encoding = \"UTF-8\" external_config . options . skip_leading_rows = 1 else : raise NotImplementedError table . external_data_configuration = external_config # external_config.options.quote_character client . create_table ( table )","title":"Module brocolib_extract_load.datawarehouse"},{"location":"reference/brocolib_extract_load/datawarehouse/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_extract_load/datawarehouse/#create_external_table","text":"def create_external_table ( project_id , bucket_name_destination , file_type , file_name , table_ref , dataset , folder_name , schemas_dict ) Function create an external table from files in GCS Parameters: Name Type Description Default project_id str ID of the project None bucket_name_destination str Name of the bucket None file_type str the type of the file None file_name str Name of the file None table_ref str Name of the table created None dataset str the dataset where the table is created None schemas_dict dict dict with fields as keys and types as values None Returns: Type Description None external table (bigquery.Table): created external table View Source def create_external_table ( project_id , bucket_name_destination , file_type , file_name , table_ref , dataset , folder_name , schemas_dict ) : '' ' Function create an external table from files in GCS Parameters: project_id(str): ID of the project bucket_name_destination (str): Name of the bucket file_type(str) : the type of the file file_name (str) : Name of the file table_ref (str) : Name of the table created dataset (str) : the dataset where the table is created schemas_dict (dict) : dict with fields as keys and types as values Returns: external table (bigquery.Table): created external table '' ' client = bigquery.Client(project_id) #Define your schema ls_schemas =[] for key,value in schemas_dict.items(): schemafield = bigquery.schema.SchemaField(key,value) ls_schemas.append(schemafield) dataset_ref = client.dataset(dataset) table_ref = bigquery.TableReference(dataset_ref, table_ref ) table = bigquery.Table(table_ref, ls_schemas) external_config = bigquery.ExternalConfig(file_type) source_uris = [ f 'gs://{bucket_name_destination}/{folder_name}/*/{file_name}' ] # i . e for a csv file in a Cloud Storage bucket # it would be something like \"gs://<your-bucket>/<your-csv-file>\" external_config . source_uris = source_uris file_type = file_type . upper () if file_type == 'CSV' : external_config . options . field_delimiter = \",\" external_config . options . encoding = \"UTF-8\" external_config . options . skip_leading_rows = 1 else : raise NotImplementedError table . external_data_configuration = external_config # external_config . options . quote_character client . create_table ( table )","title":"create_external_table"},{"location":"reference/brocolib_extract_load/goblet_utils/","text":"Module brocolib_extract_load.goblet_utils View Source import base64 import os import json def check_if_key_in_payload ( payload , ls_required ): \"\"\"[summary] Args: body (dict): body of the request ls_required (list): list of keys required in the body Raises: ValueError: if payload is None ValueError: if key not in payload \"\"\" if payload is None : raise ValueError ( \"payload can't be None\" ) for key in ls_required : if key not in payload : raise ValueError ( f \"Missing key : { key } \" ) def encode_body ( body ): \"\"\"base64 encode the body that will be POSTed by Cloud Scheduler Args: body (dict): dict to encode Returns: bytes: encoded body \"\"\" return base64 . b64encode ( json . dumps ( body ) . encode ( 'utf-8' ) ) . decode ( 'ascii' ) def yad ( decoratator_list ): \"\"\"Yet Another Decorator Loop over a list of decorators and create a unique decorator that contains them all. Args: decoratator_list (list): list of decorators \"\"\" def decorator ( f ): for d in reversed ( decoratator_list ): f = d ( f ) return f return decorator def get_schedules (): if \"schedules.json\" in os . listdir ( '.' ): schedules_path = \"./schedules.json\" else : raise FileNotFoundError ( \"schedules.json\" ) with open ( schedules_path ) as f : LS_SCHEDULES = json . load ( f ) return LS_SCHEDULES def get_decorator_list ( app ): \"\"\"For every schedule : - Create a decorator for the schedule - Add it to a list of decorators Args: app (goblet.Goblet): Goblet app Returns: goblet_decoratator_list (list): list of decorators \"\"\" goblet_decoratator_list = [] for schedule in get_schedules (): goblet_decoratator_list . append ( app . schedule ( schedule [ \"cron_schedule\" ], httpMethod = schedule [ \"httpMethod\" ], headers = { \"x-cron\" : schedule [ \"cron_schedule\" ]}, body = encode_body ( schedule [ \"body\" ]), description = schedule [ \"name\" ] ) ) return goblet_decoratator_list def add_schedules ( app ): \"\"\"Decorator that adds all the schedule to a Cloud Function handler Args: app (goblet.Goblet): [description] Returns: yad: decorator containing all the shedules' decorators \"\"\" goblet_decoratator_list = get_decorator_list ( app ) return yad ( goblet_decoratator_list ) Functions add_schedules def add_schedules ( app ) Decorator that adds all the schedule to a Cloud Function handler Parameters: Name Type Description Default app goblet.Goblet [description] None Returns: Type Description yad decorator containing all the shedules' decorators View Source def add_schedules ( app ) : \"\"\"Decorator that adds all the schedule to a Cloud Function handler Args: app (goblet.Goblet): [description] Returns: yad: decorator containing all the shedules' decorators \"\"\" goblet_decoratator_list = get_decorator_list ( app ) return yad ( goblet_decoratator_list ) check_if_key_in_payload def check_if_key_in_payload ( payload , ls_required ) [summary] Parameters: Name Type Description Default body dict body of the request None ls_required list list of keys required in the body None Raises: Type Description ValueError if payload is None ValueError if key not in payload View Source def check_if_key_in_payload ( payload , ls_required ) : \"\"\"[summary] Args: body (dict): body of the request ls_required (list): list of keys required in the body Raises: ValueError: if payload is None ValueError: if key not in payload \"\"\" if payload is None : raise ValueError ( \"payload can't be None\" ) for key in ls_required : if key not in payload : raise ValueError ( f \"Missing key : {key}\" ) encode_body def encode_body ( body ) base64 encode the body that will be POSTed by Cloud Scheduler Parameters: Name Type Description Default body dict dict to encode None Returns: Type Description bytes encoded body View Source def encode_body ( body ) : \"\" \"base64 encode the body that will be POSTed by Cloud Scheduler Args: body (dict): dict to encode Returns: bytes: encoded body \"\" \" return base64.b64encode( json.dumps(body).encode('utf-8') ).decode('ascii') get_decorator_list def get_decorator_list ( app ) For every schedule : Create a decorator for the schedule Add it to a list of decorators Parameters: Name Type Description Default app goblet.Goblet Goblet app None Returns: Type Description None goblet_decoratator_list (list): list of decorators View Source def get_decorator_list ( app ) : \"\" \"For every schedule : - Create a decorator for the schedule - Add it to a list of decorators Args: app (goblet.Goblet): Goblet app Returns: goblet_decoratator_list (list): list of decorators \"\" \" goblet_decoratator_list = [] for schedule in get_schedules(): goblet_decoratator_list.append( app.schedule( schedule [ \"cron_schedule\" ], httpMethod = schedule [ \"httpMethod\" ], headers = { \"x-cron\" : schedule [ \"cron_schedule\" ]}, body = encode_body ( schedule [ \"body\" ] ) , description = schedule [ \"name\" ] ) ) return goblet_decoratator_list get_schedules def get_schedules ( ) View Source def get_schedules (): if \"schedules.json\" in os . listdir ( '.' ): schedules_path = \"./schedules.json\" else : raise FileNotFoundError ( \"schedules.json\" ) with open ( schedules_path ) as f : LS_SCHEDULES = json . load ( f ) return LS_SCHEDULES yad def yad ( decoratator_list ) Yet Another Decorator Loop over a list of decorators and create a unique decorator that contains them all. Parameters: Name Type Description Default decoratator_list list list of decorators None View Source def yad ( decoratator_list ) : \"\" \"Yet Another Decorator Loop over a list of decorators and create a unique decorator that contains them all. Args: decoratator_list (list): list of decorators \"\" \" def decorator(f): for d in reversed(decoratator_list): f = d(f) return f return decorator","title":"Goblet Utils"},{"location":"reference/brocolib_extract_load/goblet_utils/#module-brocolib_extract_loadgoblet_utils","text":"View Source import base64 import os import json def check_if_key_in_payload ( payload , ls_required ): \"\"\"[summary] Args: body (dict): body of the request ls_required (list): list of keys required in the body Raises: ValueError: if payload is None ValueError: if key not in payload \"\"\" if payload is None : raise ValueError ( \"payload can't be None\" ) for key in ls_required : if key not in payload : raise ValueError ( f \"Missing key : { key } \" ) def encode_body ( body ): \"\"\"base64 encode the body that will be POSTed by Cloud Scheduler Args: body (dict): dict to encode Returns: bytes: encoded body \"\"\" return base64 . b64encode ( json . dumps ( body ) . encode ( 'utf-8' ) ) . decode ( 'ascii' ) def yad ( decoratator_list ): \"\"\"Yet Another Decorator Loop over a list of decorators and create a unique decorator that contains them all. Args: decoratator_list (list): list of decorators \"\"\" def decorator ( f ): for d in reversed ( decoratator_list ): f = d ( f ) return f return decorator def get_schedules (): if \"schedules.json\" in os . listdir ( '.' ): schedules_path = \"./schedules.json\" else : raise FileNotFoundError ( \"schedules.json\" ) with open ( schedules_path ) as f : LS_SCHEDULES = json . load ( f ) return LS_SCHEDULES def get_decorator_list ( app ): \"\"\"For every schedule : - Create a decorator for the schedule - Add it to a list of decorators Args: app (goblet.Goblet): Goblet app Returns: goblet_decoratator_list (list): list of decorators \"\"\" goblet_decoratator_list = [] for schedule in get_schedules (): goblet_decoratator_list . append ( app . schedule ( schedule [ \"cron_schedule\" ], httpMethod = schedule [ \"httpMethod\" ], headers = { \"x-cron\" : schedule [ \"cron_schedule\" ]}, body = encode_body ( schedule [ \"body\" ]), description = schedule [ \"name\" ] ) ) return goblet_decoratator_list def add_schedules ( app ): \"\"\"Decorator that adds all the schedule to a Cloud Function handler Args: app (goblet.Goblet): [description] Returns: yad: decorator containing all the shedules' decorators \"\"\" goblet_decoratator_list = get_decorator_list ( app ) return yad ( goblet_decoratator_list )","title":"Module brocolib_extract_load.goblet_utils"},{"location":"reference/brocolib_extract_load/goblet_utils/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_extract_load/goblet_utils/#add_schedules","text":"def add_schedules ( app ) Decorator that adds all the schedule to a Cloud Function handler Parameters: Name Type Description Default app goblet.Goblet [description] None Returns: Type Description yad decorator containing all the shedules' decorators View Source def add_schedules ( app ) : \"\"\"Decorator that adds all the schedule to a Cloud Function handler Args: app (goblet.Goblet): [description] Returns: yad: decorator containing all the shedules' decorators \"\"\" goblet_decoratator_list = get_decorator_list ( app ) return yad ( goblet_decoratator_list )","title":"add_schedules"},{"location":"reference/brocolib_extract_load/goblet_utils/#check_if_key_in_payload","text":"def check_if_key_in_payload ( payload , ls_required ) [summary] Parameters: Name Type Description Default body dict body of the request None ls_required list list of keys required in the body None Raises: Type Description ValueError if payload is None ValueError if key not in payload View Source def check_if_key_in_payload ( payload , ls_required ) : \"\"\"[summary] Args: body (dict): body of the request ls_required (list): list of keys required in the body Raises: ValueError: if payload is None ValueError: if key not in payload \"\"\" if payload is None : raise ValueError ( \"payload can't be None\" ) for key in ls_required : if key not in payload : raise ValueError ( f \"Missing key : {key}\" )","title":"check_if_key_in_payload"},{"location":"reference/brocolib_extract_load/goblet_utils/#encode_body","text":"def encode_body ( body ) base64 encode the body that will be POSTed by Cloud Scheduler Parameters: Name Type Description Default body dict dict to encode None Returns: Type Description bytes encoded body View Source def encode_body ( body ) : \"\" \"base64 encode the body that will be POSTed by Cloud Scheduler Args: body (dict): dict to encode Returns: bytes: encoded body \"\" \" return base64.b64encode( json.dumps(body).encode('utf-8') ).decode('ascii')","title":"encode_body"},{"location":"reference/brocolib_extract_load/goblet_utils/#get_decorator_list","text":"def get_decorator_list ( app ) For every schedule : Create a decorator for the schedule Add it to a list of decorators Parameters: Name Type Description Default app goblet.Goblet Goblet app None Returns: Type Description None goblet_decoratator_list (list): list of decorators View Source def get_decorator_list ( app ) : \"\" \"For every schedule : - Create a decorator for the schedule - Add it to a list of decorators Args: app (goblet.Goblet): Goblet app Returns: goblet_decoratator_list (list): list of decorators \"\" \" goblet_decoratator_list = [] for schedule in get_schedules(): goblet_decoratator_list.append( app.schedule( schedule [ \"cron_schedule\" ], httpMethod = schedule [ \"httpMethod\" ], headers = { \"x-cron\" : schedule [ \"cron_schedule\" ]}, body = encode_body ( schedule [ \"body\" ] ) , description = schedule [ \"name\" ] ) ) return goblet_decoratator_list","title":"get_decorator_list"},{"location":"reference/brocolib_extract_load/goblet_utils/#get_schedules","text":"def get_schedules ( ) View Source def get_schedules (): if \"schedules.json\" in os . listdir ( '.' ): schedules_path = \"./schedules.json\" else : raise FileNotFoundError ( \"schedules.json\" ) with open ( schedules_path ) as f : LS_SCHEDULES = json . load ( f ) return LS_SCHEDULES","title":"get_schedules"},{"location":"reference/brocolib_extract_load/goblet_utils/#yad","text":"def yad ( decoratator_list ) Yet Another Decorator Loop over a list of decorators and create a unique decorator that contains them all. Parameters: Name Type Description Default decoratator_list list list of decorators None View Source def yad ( decoratator_list ) : \"\" \"Yet Another Decorator Loop over a list of decorators and create a unique decorator that contains them all. Args: decoratator_list (list): list of decorators \"\" \" def decorator(f): for d in reversed(decoratator_list): f = d(f) return f return decorator","title":"yad"},{"location":"reference/brocolib_extract_load/ingest/","text":"Module brocolib_extract_load.ingest View Source import pandas as pd import requests as rq def extract ( url , source_type , params = {}, nested_key = None ): ''' Function to extract data - read json from url - convert json to dataframe Parameters: url (str): url of the data source source_type (str): type of the data to fetch params (dict) : request parameters Returns: (pandas.DataFrame): Dataframe created from source Exceptions: NotImplementedError: if the source type is not implemented ''' source_type = source_type . lower () if source_type == 'json' : if nested_key : response = rq . get ( url , params = params ) data = response . json ()[ nested_key ] return pd . DataFrame ( data ) else : response = rq . get ( url , params = params ) data = response . json () return pd . DataFrame ( data ) else : raise NotImplementedError ( \"sources available: json\" ) Functions extract def extract ( url , source_type , params = {}, nested_key = None ) Function to extract data read json from url convert json to dataframe Parameters: Name Type Description Default url str url of the data source None source_type str type of the data to fetch None params dict request parameters None Returns: Type Description (pandas.DataFrame) Dataframe created from source Raises: Type Description NotImplementedError if the source type is not implemented View Source def extract ( url , source_type , params = {} , nested_key = None ) : ''' Function to extract data - read json from url - convert json to dataframe Parameters: url (str): url of the data source source_type (str): type of the data to fetch params (dict) : request parameters Returns: (pandas.DataFrame): Dataframe created from source Exceptions: NotImplementedError: if the source type is not implemented ''' source_type = source_type . lower () if source_type == 'json' : if nested_key : response = rq . get ( url , params = params ) data = response . json () [ nested_key ] return pd . DataFrame ( data ) else : response = rq . get ( url , params = params ) data = response . json () return pd . DataFrame ( data ) else : raise NotImplementedError ( \"sources available: json\" )","title":"Ingest"},{"location":"reference/brocolib_extract_load/ingest/#module-brocolib_extract_loadingest","text":"View Source import pandas as pd import requests as rq def extract ( url , source_type , params = {}, nested_key = None ): ''' Function to extract data - read json from url - convert json to dataframe Parameters: url (str): url of the data source source_type (str): type of the data to fetch params (dict) : request parameters Returns: (pandas.DataFrame): Dataframe created from source Exceptions: NotImplementedError: if the source type is not implemented ''' source_type = source_type . lower () if source_type == 'json' : if nested_key : response = rq . get ( url , params = params ) data = response . json ()[ nested_key ] return pd . DataFrame ( data ) else : response = rq . get ( url , params = params ) data = response . json () return pd . DataFrame ( data ) else : raise NotImplementedError ( \"sources available: json\" )","title":"Module brocolib_extract_load.ingest"},{"location":"reference/brocolib_extract_load/ingest/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_extract_load/ingest/#extract","text":"def extract ( url , source_type , params = {}, nested_key = None ) Function to extract data read json from url convert json to dataframe Parameters: Name Type Description Default url str url of the data source None source_type str type of the data to fetch None params dict request parameters None Returns: Type Description (pandas.DataFrame) Dataframe created from source Raises: Type Description NotImplementedError if the source type is not implemented View Source def extract ( url , source_type , params = {} , nested_key = None ) : ''' Function to extract data - read json from url - convert json to dataframe Parameters: url (str): url of the data source source_type (str): type of the data to fetch params (dict) : request parameters Returns: (pandas.DataFrame): Dataframe created from source Exceptions: NotImplementedError: if the source type is not implemented ''' source_type = source_type . lower () if source_type == 'json' : if nested_key : response = rq . get ( url , params = params ) data = response . json () [ nested_key ] return pd . DataFrame ( data ) else : response = rq . get ( url , params = params ) data = response . json () return pd . DataFrame ( data ) else : raise NotImplementedError ( \"sources available: json\" )","title":"extract"},{"location":"reference/brocolib_extract_load/processing/","text":"Module brocolib_extract_load.processing View Source import pandas as pd def transform ( dataframe , numeric_col , date_col , date_format_str , keep_col = None ): ''' Function to transform dataframe - transpose dataframe - reset and drop the index of dataframe - set numeric columns to numeric - set date column to date Parameters: dataframe (pandas.DataFrame): A dataframe numeric_col(list): A list of column names containing numbers date_col (list): A list of column names containing dates date_format_str (str): A datetime formatting string keep_col (list): A list of column names to keep Returns: dataframe (pandas.DataFrame): Transformed Dataframe ''' test_list = keep_col + numeric_col + date_col test_list_2 = numeric_col + date_col for col in test_list_2 : if col not in keep_col : raise ValueError ( f ' { col } not in cols provided in keep_col' ) dataframe = dataframe . transpose () for col in test_list : if col not in dataframe . columns : raise ValueError ( f ' { col } not in dataframe' ) if keep_col : dataframe = dataframe [ keep_col ] dataframe [ numeric_col ] = dataframe [ numeric_col ] . apply ( pd . to_numeric ) dataframe [ date_col ] = dataframe [ date_col ] . apply ( pd . to_datetime , format = date_format_str ) dataframe = dataframe . reset_index ( drop = True ) return dataframe Functions transform def transform ( dataframe , numeric_col , date_col , date_format_str , keep_col = None ) Function to transform dataframe transpose dataframe reset and drop the index of dataframe set numeric columns to numeric set date column to date Parameters: Name Type Description Default dataframe pandas.DataFrame A dataframe None numeric_col list A list of column names containing numbers None date_col list A list of column names containing dates None date_format_str str A datetime formatting string None keep_col list A list of column names to keep None Returns: Type Description None dataframe (pandas.DataFrame): Transformed Dataframe View Source def transform ( dataframe , numeric_col , date_col , date_format_str , keep_col = None ) : ''' Function to transform dataframe - transpose dataframe - reset and drop the index of dataframe - set numeric columns to numeric - set date column to date Parameters: dataframe (pandas.DataFrame): A dataframe numeric_col(list): A list of column names containing numbers date_col (list): A list of column names containing dates date_format_str (str): A datetime formatting string keep_col (list): A list of column names to keep Returns: dataframe (pandas.DataFrame): Transformed Dataframe ''' test_list = keep_col + numeric_col + date_col test_list_2 = numeric_col + date_col for col in test_list_2 : if col not in keep_col : raise ValueError ( f '{col} not in cols provided in keep_col' ) dataframe = dataframe . transpose () for col in test_list : if col not in dataframe . columns : raise ValueError ( f '{col} not in dataframe' ) if keep_col : dataframe = dataframe [ keep_col ] dataframe [ numeric_col ] = dataframe [ numeric_col ] . apply ( pd . to_numeric ) dataframe [ date_col ] = dataframe [ date_col ] . apply ( pd . to_datetime , format = date_format_str ) dataframe = dataframe . reset_index ( drop = True ) return dataframe","title":"Processing"},{"location":"reference/brocolib_extract_load/processing/#module-brocolib_extract_loadprocessing","text":"View Source import pandas as pd def transform ( dataframe , numeric_col , date_col , date_format_str , keep_col = None ): ''' Function to transform dataframe - transpose dataframe - reset and drop the index of dataframe - set numeric columns to numeric - set date column to date Parameters: dataframe (pandas.DataFrame): A dataframe numeric_col(list): A list of column names containing numbers date_col (list): A list of column names containing dates date_format_str (str): A datetime formatting string keep_col (list): A list of column names to keep Returns: dataframe (pandas.DataFrame): Transformed Dataframe ''' test_list = keep_col + numeric_col + date_col test_list_2 = numeric_col + date_col for col in test_list_2 : if col not in keep_col : raise ValueError ( f ' { col } not in cols provided in keep_col' ) dataframe = dataframe . transpose () for col in test_list : if col not in dataframe . columns : raise ValueError ( f ' { col } not in dataframe' ) if keep_col : dataframe = dataframe [ keep_col ] dataframe [ numeric_col ] = dataframe [ numeric_col ] . apply ( pd . to_numeric ) dataframe [ date_col ] = dataframe [ date_col ] . apply ( pd . to_datetime , format = date_format_str ) dataframe = dataframe . reset_index ( drop = True ) return dataframe","title":"Module brocolib_extract_load.processing"},{"location":"reference/brocolib_extract_load/processing/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_extract_load/processing/#transform","text":"def transform ( dataframe , numeric_col , date_col , date_format_str , keep_col = None ) Function to transform dataframe transpose dataframe reset and drop the index of dataframe set numeric columns to numeric set date column to date Parameters: Name Type Description Default dataframe pandas.DataFrame A dataframe None numeric_col list A list of column names containing numbers None date_col list A list of column names containing dates None date_format_str str A datetime formatting string None keep_col list A list of column names to keep None Returns: Type Description None dataframe (pandas.DataFrame): Transformed Dataframe View Source def transform ( dataframe , numeric_col , date_col , date_format_str , keep_col = None ) : ''' Function to transform dataframe - transpose dataframe - reset and drop the index of dataframe - set numeric columns to numeric - set date column to date Parameters: dataframe (pandas.DataFrame): A dataframe numeric_col(list): A list of column names containing numbers date_col (list): A list of column names containing dates date_format_str (str): A datetime formatting string keep_col (list): A list of column names to keep Returns: dataframe (pandas.DataFrame): Transformed Dataframe ''' test_list = keep_col + numeric_col + date_col test_list_2 = numeric_col + date_col for col in test_list_2 : if col not in keep_col : raise ValueError ( f '{col} not in cols provided in keep_col' ) dataframe = dataframe . transpose () for col in test_list : if col not in dataframe . columns : raise ValueError ( f '{col} not in dataframe' ) if keep_col : dataframe = dataframe [ keep_col ] dataframe [ numeric_col ] = dataframe [ numeric_col ] . apply ( pd . to_numeric ) dataframe [ date_col ] = dataframe [ date_col ] . apply ( pd . to_datetime , format = date_format_str ) dataframe = dataframe . reset_index ( drop = True ) return dataframe","title":"transform"},{"location":"reference/brocolib_extract_load/pubsub/","text":"Module brocolib_extract_load.pubsub View Source import json def publish_message_toPubSub ( project_id , topic_id , message , logger ): from google.cloud import pubsub publisher = pubsub . PublisherClient () topic_path = publisher . topic_path ( project_id , topic_id ) future = publisher . publish ( topic_path , message ) if logger : logger . info ( future . result ()) return future . result () def publish_sources ( sources , dbt_topic , gcp_project , logger = None ): message = { \"sources\" : sources } result = publish_message_toPubSub ( project_id = gcp_project , topic_id = dbt_topic , message = json . dumps ( message ) . encode ( \"utf-8\" ), logger = logger ) if logger : logger . info ( result ) Functions publish_message_toPubSub def publish_message_toPubSub ( project_id , topic_id , message , logger ) View Source def publish_message_toPubSub ( project_id , topic_id , message , logger ): from google.cloud import pubsub publisher = pubsub . PublisherClient () topic_path = publisher . topic_path ( project_id , topic_id ) future = publisher . publish ( topic_path , message ) if logger : logger . info ( future . result ()) return future . result () publish_sources def publish_sources ( sources , dbt_topic , gcp_project , logger = None ) View Source def publish_sources ( sources , dbt_topic , gcp_project , logger = None ) : message = { \"sources\" : sources } result = publish_message_toPubSub ( project_id = gcp_project , topic_id = dbt_topic , message = json . dumps ( message ) . encode ( \"utf-8\" ) , logger = logger ) if logger : logger . info ( result )","title":"Pubsub"},{"location":"reference/brocolib_extract_load/pubsub/#module-brocolib_extract_loadpubsub","text":"View Source import json def publish_message_toPubSub ( project_id , topic_id , message , logger ): from google.cloud import pubsub publisher = pubsub . PublisherClient () topic_path = publisher . topic_path ( project_id , topic_id ) future = publisher . publish ( topic_path , message ) if logger : logger . info ( future . result ()) return future . result () def publish_sources ( sources , dbt_topic , gcp_project , logger = None ): message = { \"sources\" : sources } result = publish_message_toPubSub ( project_id = gcp_project , topic_id = dbt_topic , message = json . dumps ( message ) . encode ( \"utf-8\" ), logger = logger ) if logger : logger . info ( result )","title":"Module brocolib_extract_load.pubsub"},{"location":"reference/brocolib_extract_load/pubsub/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_extract_load/pubsub/#publish_message_topubsub","text":"def publish_message_toPubSub ( project_id , topic_id , message , logger ) View Source def publish_message_toPubSub ( project_id , topic_id , message , logger ): from google.cloud import pubsub publisher = pubsub . PublisherClient () topic_path = publisher . topic_path ( project_id , topic_id ) future = publisher . publish ( topic_path , message ) if logger : logger . info ( future . result ()) return future . result ()","title":"publish_message_toPubSub"},{"location":"reference/brocolib_extract_load/pubsub/#publish_sources","text":"def publish_sources ( sources , dbt_topic , gcp_project , logger = None ) View Source def publish_sources ( sources , dbt_topic , gcp_project , logger = None ) : message = { \"sources\" : sources } result = publish_message_toPubSub ( project_id = gcp_project , topic_id = dbt_topic , message = json . dumps ( message ) . encode ( \"utf-8\" ) , logger = logger ) if logger : logger . info ( result )","title":"publish_sources"},{"location":"reference/brocolib_factory_utils/","text":"Module brocolib_factory_utils Sub-modules brocolib_factory_utils.cookiecutter_utils","title":"Index"},{"location":"reference/brocolib_factory_utils/#module-brocolib_factory_utils","text":"","title":"Module brocolib_factory_utils"},{"location":"reference/brocolib_factory_utils/#sub-modules","text":"brocolib_factory_utils.cookiecutter_utils","title":"Sub-modules"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/","text":"Module brocolib_factory_utils.cookiecutter_utils Sub-modules brocolib_factory_utils.cookiecutter_utils.cookiecut_template brocolib_factory_utils.cookiecutter_utils.cookiecut_template_cli brocolib_factory_utils.cookiecutter_utils.cookiecut_utils","title":"Index"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/#module-brocolib_factory_utilscookiecutter_utils","text":"","title":"Module brocolib_factory_utils.cookiecutter_utils"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/#sub-modules","text":"brocolib_factory_utils.cookiecutter_utils.cookiecut_template brocolib_factory_utils.cookiecutter_utils.cookiecut_template_cli brocolib_factory_utils.cookiecutter_utils.cookiecut_utils","title":"Sub-modules"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/","text":"Module brocolib_factory_utils.cookiecutter_utils.cookiecut_template View Source import os import time from github import Github from git import Repo from cookiecutter.main import cookiecutter from typing import Union from brocolib_factory_utils.cookiecutter_utils.cookiecut_utils import * def create_gith_repo ( client_github_token : str , organisation : str , client_repo : str ): org = gith_connect ( client_github_token , organisation ) org . create_repo ( name = client_repo , private = True , auto_init = True ) print ( 'Created new github repo' ) def clone_locally ( client_github_token , organisation , client_repo , local_dir ): new_repo_url = f \"https:// { client_github_token } @github.com/ { organisation } / { client_repo } .git\" Repo . clone_from ( new_repo_url , local_dir ) print ( f 'Repo cloned successfully' ) def add_gh_secret ( client_github_token : str , organisation : str , client_repo : str , secr_dict : dict , ): \"\"\" function that adds github secrets Args: token (Union[None , str], optional): Client GitHub token. Defaults to None. organisation (Union[None , str]): organisation (or user) name. Defaults to None. \"\"\" org = gith_connect ( client_github_token , organisation ) repo_obj = org . get_repo ( client_repo ) if secr_dict : counter = 0 for key , value in secr_dict . items (): repo_obj . create_secret ( key , value ) counter += 1 time . sleep ( 5 ) print ( f ' { counter } Secrets added successfully' ) def cookiec_from_temp ( templ_repo : str , local_dir : str , source_token : str , source_organisation : str , jason_dict : Union [ None , dict ] = None , directory_name : Union [ None , str ] = None , ): \"\"\" cookiecut repo from given template repo or repo directory Args: templ_repo (Union[None, str]): GitHub repo that contains the cookiecutter template. local_dir (Union[None, str]): local path to spawn the targeted directory/project. Defaults to None. source_token (Union[None, str]): GitHub token of Brocoli to access the cookicutter template. Defaults to None. source_organisation (Union[None, str]): organisation (or user) name of Brocoli to access the cookicutter template. Defaults to None. directory_name (Union[None, str]): targeted directory/project in the templates repo. Defaults to None. jason_dict (Union[None, dict], optional): dict of cookiecutt template variables. Defaults to None. \"\"\" cookiecut_tmp_url = f \"https:// { source_token } @github.com/ { source_organisation } / { templ_repo } .git\" if jason_dict : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , output_dir = local_dir ) else : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , output_dir = local_dir ) print ( \"Cookiecut done successfully\" ) def add_commit_push_all ( local_dir : str , message : str ): repo = Repo ( local_dir ) repo . git . add ( all = True ) repo . index . commit ( message ) repo . remotes . origin . push () print ( \"Pushed successfully\" ) Functions add_commit_push_all def add_commit_push_all ( local_dir : str , message : str ) View Source def add_commit_push_all(local_dir: str, message: str): repo = Repo(local_dir) repo.git.add(all=True) repo.index.commit(message) repo.remotes.origin.push() print(\"Pushed successfully\") add_gh_secret def add_gh_secret ( client_github_token : str , organisation : str , client_repo : str , secr_dict : dict ) function that adds github secrets Parameters: Name Type Description Default token Union[None , str] Client GitHub token. Defaults to None. None organisation Union[None , str] organisation (or user) name. Defaults to None. None View Source def add_gh_secret ( client_github_token : str , organisation : str , client_repo : str , secr_dict : dict , ) : \"\" \" function that adds github secrets Args: token (Union[None , str], optional): Client GitHub token. Defaults to None. organisation (Union[None , str]): organisation (or user) name. Defaults to None. \"\" \" org = gith_connect(client_github_token, organisation) repo_obj = org.get_repo(client_repo) if secr_dict: counter = 0 for key, value in secr_dict.items(): repo_obj.create_secret(key, value) counter += 1 time.sleep(5) print(f'{counter} Secrets added successfully') clone_locally def clone_locally ( client_github_token , organisation , client_repo , local_dir ) View Source def clone_locally ( client_github_token , organisation , client_repo , local_dir ) : new_repo_url = f \"https://{client_github_token}@github.com/{organisation}/{client_repo}.git\" Repo . clone_from ( new_repo_url , local_dir ) print ( f 'Repo cloned successfully' ) cookiec_from_temp def cookiec_from_temp ( templ_repo : str , local_dir : str , source_token : str , source_organisation : str , jason_dict : Union [ NoneType , dict ] = None , directory_name : Union [ NoneType , str ] = None ) cookiecut repo from given template repo or repo directory Parameters: Name Type Description Default templ_repo Union[None, str] GitHub repo that contains the cookiecutter template. None local_dir Union[None, str] local path to spawn the targeted directory/project. Defaults to None. None source_token Union[None, str] GitHub token of Brocoli to access the cookicutter template. Defaults to None. None source_organisation Union[None, str] organisation (or user) name of Brocoli to access the cookicutter template. Defaults to None. None directory_name Union[None, str] targeted directory/project in the templates repo. Defaults to None. None jason_dict Union[None, dict] dict of cookiecutt template variables. Defaults to None. None View Source def cookiec_from_temp ( templ_repo : str , local_dir : str , source_token : str , source_organisation : str , jason_dict : Union [ None , dict ] = None , directory_name : Union [ None , str ] = None , ): \"\"\" cookiecut repo from given template repo or repo directory Args: templ_repo (Union[None, str]): GitHub repo that contains the cookiecutter template. local_dir (Union[None, str]): local path to spawn the targeted directory/project. Defaults to None. source_token (Union[None, str]): GitHub token of Brocoli to access the cookicutter template. Defaults to None. source_organisation (Union[None, str]): organisation (or user) name of Brocoli to access the cookicutter template. Defaults to None. directory_name (Union[None, str]): targeted directory/project in the templates repo. Defaults to None. jason_dict (Union[None, dict], optional): dict of cookiecutt template variables. Defaults to None. \"\"\" cookiecut_tmp_url = f \"https://{source_token}@github.com/{source_organisation}/{templ_repo}.git\" if jason_dict : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , output_dir = local_dir ) else : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , output_dir = local_dir ) print ( \"Cookiecut done successfully\" ) create_gith_repo def create_gith_repo ( client_github_token : str , organisation : str , client_repo : str ) View Source def create_gith_repo(client_github_token: str, organisation: str, client_repo: str ): org = gith_connect(client_github_token, organisation) org.create_repo( name=client_repo, private=True, auto_init=True ) print('Created new github repo')","title":"Cookiecut Template"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/#module-brocolib_factory_utilscookiecutter_utilscookiecut_template","text":"View Source import os import time from github import Github from git import Repo from cookiecutter.main import cookiecutter from typing import Union from brocolib_factory_utils.cookiecutter_utils.cookiecut_utils import * def create_gith_repo ( client_github_token : str , organisation : str , client_repo : str ): org = gith_connect ( client_github_token , organisation ) org . create_repo ( name = client_repo , private = True , auto_init = True ) print ( 'Created new github repo' ) def clone_locally ( client_github_token , organisation , client_repo , local_dir ): new_repo_url = f \"https:// { client_github_token } @github.com/ { organisation } / { client_repo } .git\" Repo . clone_from ( new_repo_url , local_dir ) print ( f 'Repo cloned successfully' ) def add_gh_secret ( client_github_token : str , organisation : str , client_repo : str , secr_dict : dict , ): \"\"\" function that adds github secrets Args: token (Union[None , str], optional): Client GitHub token. Defaults to None. organisation (Union[None , str]): organisation (or user) name. Defaults to None. \"\"\" org = gith_connect ( client_github_token , organisation ) repo_obj = org . get_repo ( client_repo ) if secr_dict : counter = 0 for key , value in secr_dict . items (): repo_obj . create_secret ( key , value ) counter += 1 time . sleep ( 5 ) print ( f ' { counter } Secrets added successfully' ) def cookiec_from_temp ( templ_repo : str , local_dir : str , source_token : str , source_organisation : str , jason_dict : Union [ None , dict ] = None , directory_name : Union [ None , str ] = None , ): \"\"\" cookiecut repo from given template repo or repo directory Args: templ_repo (Union[None, str]): GitHub repo that contains the cookiecutter template. local_dir (Union[None, str]): local path to spawn the targeted directory/project. Defaults to None. source_token (Union[None, str]): GitHub token of Brocoli to access the cookicutter template. Defaults to None. source_organisation (Union[None, str]): organisation (or user) name of Brocoli to access the cookicutter template. Defaults to None. directory_name (Union[None, str]): targeted directory/project in the templates repo. Defaults to None. jason_dict (Union[None, dict], optional): dict of cookiecutt template variables. Defaults to None. \"\"\" cookiecut_tmp_url = f \"https:// { source_token } @github.com/ { source_organisation } / { templ_repo } .git\" if jason_dict : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , output_dir = local_dir ) else : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , output_dir = local_dir ) print ( \"Cookiecut done successfully\" ) def add_commit_push_all ( local_dir : str , message : str ): repo = Repo ( local_dir ) repo . git . add ( all = True ) repo . index . commit ( message ) repo . remotes . origin . push () print ( \"Pushed successfully\" )","title":"Module brocolib_factory_utils.cookiecutter_utils.cookiecut_template"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/#add_commit_push_all","text":"def add_commit_push_all ( local_dir : str , message : str ) View Source def add_commit_push_all(local_dir: str, message: str): repo = Repo(local_dir) repo.git.add(all=True) repo.index.commit(message) repo.remotes.origin.push() print(\"Pushed successfully\")","title":"add_commit_push_all"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/#add_gh_secret","text":"def add_gh_secret ( client_github_token : str , organisation : str , client_repo : str , secr_dict : dict ) function that adds github secrets Parameters: Name Type Description Default token Union[None , str] Client GitHub token. Defaults to None. None organisation Union[None , str] organisation (or user) name. Defaults to None. None View Source def add_gh_secret ( client_github_token : str , organisation : str , client_repo : str , secr_dict : dict , ) : \"\" \" function that adds github secrets Args: token (Union[None , str], optional): Client GitHub token. Defaults to None. organisation (Union[None , str]): organisation (or user) name. Defaults to None. \"\" \" org = gith_connect(client_github_token, organisation) repo_obj = org.get_repo(client_repo) if secr_dict: counter = 0 for key, value in secr_dict.items(): repo_obj.create_secret(key, value) counter += 1 time.sleep(5) print(f'{counter} Secrets added successfully')","title":"add_gh_secret"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/#clone_locally","text":"def clone_locally ( client_github_token , organisation , client_repo , local_dir ) View Source def clone_locally ( client_github_token , organisation , client_repo , local_dir ) : new_repo_url = f \"https://{client_github_token}@github.com/{organisation}/{client_repo}.git\" Repo . clone_from ( new_repo_url , local_dir ) print ( f 'Repo cloned successfully' )","title":"clone_locally"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/#cookiec_from_temp","text":"def cookiec_from_temp ( templ_repo : str , local_dir : str , source_token : str , source_organisation : str , jason_dict : Union [ NoneType , dict ] = None , directory_name : Union [ NoneType , str ] = None ) cookiecut repo from given template repo or repo directory Parameters: Name Type Description Default templ_repo Union[None, str] GitHub repo that contains the cookiecutter template. None local_dir Union[None, str] local path to spawn the targeted directory/project. Defaults to None. None source_token Union[None, str] GitHub token of Brocoli to access the cookicutter template. Defaults to None. None source_organisation Union[None, str] organisation (or user) name of Brocoli to access the cookicutter template. Defaults to None. None directory_name Union[None, str] targeted directory/project in the templates repo. Defaults to None. None jason_dict Union[None, dict] dict of cookiecutt template variables. Defaults to None. None View Source def cookiec_from_temp ( templ_repo : str , local_dir : str , source_token : str , source_organisation : str , jason_dict : Union [ None , dict ] = None , directory_name : Union [ None , str ] = None , ): \"\"\" cookiecut repo from given template repo or repo directory Args: templ_repo (Union[None, str]): GitHub repo that contains the cookiecutter template. local_dir (Union[None, str]): local path to spawn the targeted directory/project. Defaults to None. source_token (Union[None, str]): GitHub token of Brocoli to access the cookicutter template. Defaults to None. source_organisation (Union[None, str]): organisation (or user) name of Brocoli to access the cookicutter template. Defaults to None. directory_name (Union[None, str]): targeted directory/project in the templates repo. Defaults to None. jason_dict (Union[None, dict], optional): dict of cookiecutt template variables. Defaults to None. \"\"\" cookiecut_tmp_url = f \"https://{source_token}@github.com/{source_organisation}/{templ_repo}.git\" if jason_dict : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , extra_context = jason_dict , output_dir = local_dir ) else : if directory_name : cookiecutter ( cookiecut_tmp_url , no_input = True , directory = directory_name , output_dir = local_dir ) else : cookiecutter ( cookiecut_tmp_url , no_input = True , output_dir = local_dir ) print ( \"Cookiecut done successfully\" )","title":"cookiec_from_temp"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template/#create_gith_repo","text":"def create_gith_repo ( client_github_token : str , organisation : str , client_repo : str ) View Source def create_gith_repo(client_github_token: str, organisation: str, client_repo: str ): org = gith_connect(client_github_token, organisation) org.create_repo( name=client_repo, private=True, auto_init=True ) print('Created new github repo')","title":"create_gith_repo"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template_cli/","text":"Module brocolib_factory_utils.cookiecutter_utils.cookiecut_template_cli View Source import os from distutils.dir_util import copy_tree from brocolib_factory_utils.cookiecutter_utils import cookiecut_template if __name__ == \"__main__\" : # variables for the cookicut project cookie_templ_keys = [ \"PROJECT_SLUG\" , \"SOURCE_NAME\" , \"GCP_BACK_PROJECT\" , \"FUNCTIONS_SERVICE_ACCOUNT\" , \"DBT_PUBSUB_TOPIC\" , \"DATALAKE_GCS_BUCKET\" , \"GOBLET_GCP_VERSION\" ] secrets_keys = [] # client variables for github setup client_organisation = os . getenv ( \"CLIENT_ORGANISATION\" ) client_github_token = os . getenv ( \"CLIENT_GITHUB_TOKEN\" ) # ${{ env.CLIENT_GITHUB_TOKEN }} client_repo = os . getenv ( \"CLIENT_REPO\" ) local_dir = os . getenv ( \"LOCAL_DIR\" ) temp_local_dir = f \" { local_dir } _cookiecutter\" # brocoli variables for github setup brocoli_organisation = os . getenv ( \"BROCOLI_ORGANISATION\" ) brocolib_github_token = os . getenv ( \"BROCOLIB_GITHUB_TOKEN\" ) brocoli_templ_repo = os . getenv ( \"BROCOLI_TEMPL_REPO\" ) cookie_temp_directory = os . getenv ( \"COOKIE_TEMP_DIRECTORY\" ) default_message_for_commit = f \"created { client_repo } for { client_organisation } + cookicuted the { brocoli_templ_repo } template + added secrets + pushed\" # verify if client and brocoli variables for github are set for x in [ client_organisation , client_github_token , client_repo , local_dir , brocoli_organisation , brocolib_github_token , brocoli_templ_repo , cookie_temp_directory ]: if x is None : vname = [ name for name in globals () if globals ()[ name ] is x and name != 'x' and name != '__doc__' ] raise ValueError ( f 'This list of variables must be set or given { vname } ' ) # creating repo and cloning locally cookiecut_template . create_gith_repo ( client_github_token = client_github_token , organisation = client_organisation , client_repo = client_repo ) cookiecut_template . clone_locally ( client_github_token = client_github_token , organisation = client_organisation , client_repo = client_repo , local_dir = local_dir ) env_variables_dic = {} for k in cookie_templ_keys : if os . environ . get ( k ) is not None : if k not in env_variables_dic : val = os . getenv ( k . upper ()) env_variables_dic [ k . upper ()] = val if env_variables_dic : cookiecut_template . cookiec_from_temp ( templ_repo = brocoli_templ_repo , local_dir = temp_local_dir , source_token = brocolib_github_token , source_organisation = brocoli_organisation , jason_dict = env_variables_dic , directory_name = cookie_temp_directory ) else : cookiecut_template . cookiec_from_temp ( templ_repo = brocoli_templ_repo , local_dir = temp_local_dir , source_token = brocolib_github_token , source_organisation = brocoli_organisation , directory_name = cookie_temp_directory ) # Copy files from temp_local_dir to local_dir copy_tree ( src = os . path . join ( temp_local_dir , client_repo ), dst = local_dir ) #pushing changes cookiecut_template . add_commit_push_all ( local_dir = local_dir , message = default_message_for_commit ) # adding secrets to dict then add secrets to client repo secrets_dic = {} for k in secrets_keys : if os . environ . get ( k ) is not None : if k not in env_variables_dic : val = os . getenv ( k . upper ()) secrets_dic [ k . upper ()] = val if secrets_dic : cookiecut_template . add_gh_secret ( client_github_token = client_github_token , organisation = client_organisation , client_repo = client_repo , secr_dict = secrets_dic ) print ( f ' { default_message_for_commit } ' )","title":"Cookiecut Template Cli"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_template_cli/#module-brocolib_factory_utilscookiecutter_utilscookiecut_template_cli","text":"View Source import os from distutils.dir_util import copy_tree from brocolib_factory_utils.cookiecutter_utils import cookiecut_template if __name__ == \"__main__\" : # variables for the cookicut project cookie_templ_keys = [ \"PROJECT_SLUG\" , \"SOURCE_NAME\" , \"GCP_BACK_PROJECT\" , \"FUNCTIONS_SERVICE_ACCOUNT\" , \"DBT_PUBSUB_TOPIC\" , \"DATALAKE_GCS_BUCKET\" , \"GOBLET_GCP_VERSION\" ] secrets_keys = [] # client variables for github setup client_organisation = os . getenv ( \"CLIENT_ORGANISATION\" ) client_github_token = os . getenv ( \"CLIENT_GITHUB_TOKEN\" ) # ${{ env.CLIENT_GITHUB_TOKEN }} client_repo = os . getenv ( \"CLIENT_REPO\" ) local_dir = os . getenv ( \"LOCAL_DIR\" ) temp_local_dir = f \" { local_dir } _cookiecutter\" # brocoli variables for github setup brocoli_organisation = os . getenv ( \"BROCOLI_ORGANISATION\" ) brocolib_github_token = os . getenv ( \"BROCOLIB_GITHUB_TOKEN\" ) brocoli_templ_repo = os . getenv ( \"BROCOLI_TEMPL_REPO\" ) cookie_temp_directory = os . getenv ( \"COOKIE_TEMP_DIRECTORY\" ) default_message_for_commit = f \"created { client_repo } for { client_organisation } + cookicuted the { brocoli_templ_repo } template + added secrets + pushed\" # verify if client and brocoli variables for github are set for x in [ client_organisation , client_github_token , client_repo , local_dir , brocoli_organisation , brocolib_github_token , brocoli_templ_repo , cookie_temp_directory ]: if x is None : vname = [ name for name in globals () if globals ()[ name ] is x and name != 'x' and name != '__doc__' ] raise ValueError ( f 'This list of variables must be set or given { vname } ' ) # creating repo and cloning locally cookiecut_template . create_gith_repo ( client_github_token = client_github_token , organisation = client_organisation , client_repo = client_repo ) cookiecut_template . clone_locally ( client_github_token = client_github_token , organisation = client_organisation , client_repo = client_repo , local_dir = local_dir ) env_variables_dic = {} for k in cookie_templ_keys : if os . environ . get ( k ) is not None : if k not in env_variables_dic : val = os . getenv ( k . upper ()) env_variables_dic [ k . upper ()] = val if env_variables_dic : cookiecut_template . cookiec_from_temp ( templ_repo = brocoli_templ_repo , local_dir = temp_local_dir , source_token = brocolib_github_token , source_organisation = brocoli_organisation , jason_dict = env_variables_dic , directory_name = cookie_temp_directory ) else : cookiecut_template . cookiec_from_temp ( templ_repo = brocoli_templ_repo , local_dir = temp_local_dir , source_token = brocolib_github_token , source_organisation = brocoli_organisation , directory_name = cookie_temp_directory ) # Copy files from temp_local_dir to local_dir copy_tree ( src = os . path . join ( temp_local_dir , client_repo ), dst = local_dir ) #pushing changes cookiecut_template . add_commit_push_all ( local_dir = local_dir , message = default_message_for_commit ) # adding secrets to dict then add secrets to client repo secrets_dic = {} for k in secrets_keys : if os . environ . get ( k ) is not None : if k not in env_variables_dic : val = os . getenv ( k . upper ()) secrets_dic [ k . upper ()] = val if secrets_dic : cookiecut_template . add_gh_secret ( client_github_token = client_github_token , organisation = client_organisation , client_repo = client_repo , secr_dict = secrets_dic ) print ( f ' { default_message_for_commit } ' )","title":"Module brocolib_factory_utils.cookiecutter_utils.cookiecut_template_cli"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_utils/","text":"Module brocolib_factory_utils.cookiecutter_utils.cookiecut_utils View Source from github import Github import os from typing import Union def gith_connect ( token : str , organisation : str , ): \"\"\" function that connects to github with token for user or organisation Args: token (Union[None , str], optional): GitHub token. Defaults to None. organisation (Union[None , str], optional): organisation (or user) name. Defaults to None. Returns: class: github.Organization.Organization represents Organizations \"\"\" g = Github ( token ) org = g . get_organization ( organisation ) return org Functions gith_connect def gith_connect ( token : str , organisation : str ) function that connects to github with token for user or organisation Parameters: Name Type Description Default token Union[None , str] GitHub token. Defaults to None. None organisation Union[None , str] organisation (or user) name. Defaults to None. None Returns: Type Description class github.Organization.Organization represents Organizations View Source def gith_connect ( token : str , organisation : str , ) : \"\" \" function that connects to github with token for user or organisation Args: token (Union[None , str], optional): GitHub token. Defaults to None. organisation (Union[None , str], optional): organisation (or user) name. Defaults to None. Returns: class: github.Organization.Organization represents Organizations \"\" \" g = Github(token) org = g.get_organization(organisation) return org","title":"Cookiecut Utils"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_utils/#module-brocolib_factory_utilscookiecutter_utilscookiecut_utils","text":"View Source from github import Github import os from typing import Union def gith_connect ( token : str , organisation : str , ): \"\"\" function that connects to github with token for user or organisation Args: token (Union[None , str], optional): GitHub token. Defaults to None. organisation (Union[None , str], optional): organisation (or user) name. Defaults to None. Returns: class: github.Organization.Organization represents Organizations \"\"\" g = Github ( token ) org = g . get_organization ( organisation ) return org","title":"Module brocolib_factory_utils.cookiecutter_utils.cookiecut_utils"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_utils/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_factory_utils/cookiecutter_utils/cookiecut_utils/#gith_connect","text":"def gith_connect ( token : str , organisation : str ) function that connects to github with token for user or organisation Parameters: Name Type Description Default token Union[None , str] GitHub token. Defaults to None. None organisation Union[None , str] organisation (or user) name. Defaults to None. None Returns: Type Description class github.Organization.Organization represents Organizations View Source def gith_connect ( token : str , organisation : str , ) : \"\" \" function that connects to github with token for user or organisation Args: token (Union[None , str], optional): GitHub token. Defaults to None. organisation (Union[None , str], optional): organisation (or user) name. Defaults to None. Returns: class: github.Organization.Organization represents Organizations \"\" \" g = Github(token) org = g.get_organization(organisation) return org","title":"gith_connect"},{"location":"reference/brocolib_transform/","text":"Module brocolib_transform View Source from .dbt_utils import * Sub-modules brocolib_transform.dbt_utils","title":"Index"},{"location":"reference/brocolib_transform/#module-brocolib_transform","text":"View Source from .dbt_utils import *","title":"Module brocolib_transform"},{"location":"reference/brocolib_transform/#sub-modules","text":"brocolib_transform.dbt_utils","title":"Sub-modules"},{"location":"reference/brocolib_transform/dbt_utils/","text":"Module brocolib_transform.dbt_utils View Source import os import shlex import subprocess def run_subprocess ( ls_commands : list , working_dir : str , logger = None ): \"\"\"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\"\" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy (), encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \" { out } \\n { err } failed\" if logger : logger . error ( msg ) return msg , False except Exception as e : if logger : logger . error ( str ( e )) return str ( e ), False msg = out if logger : logger . info ( msg ) return msg , True def run_dbt_model ( sources : list , project_dir : str , logger = None ): \"\"\"Run `dbt run` and select child of sources provided as arg using `\"--select source:stg.SOURCE+` See [Using Unions Set Operators for Node Selection](https://docs.getdbt.com/reference/node-selection/set-operators#unions) Args: sources (list): all sources that are parent of the branches that will be run project_dir (str): path to the dbt project logger (logging.logger): (optional) for goblet `app.log` \"\"\" ls_commands = [ \"dbt\" , \"run\" , \"--select\" ] ls_commands += [ f \"source:stg. { source } +\" for source in sources ] logger . info ( f 'START dbt run for { str ( sources ) } ' ) _ , dbt_run_ok = run_subprocess ( ls_commands , project_dir , logger ) if dbt_run_ok : logger . info ( f 'END dbt run successful for { str ( sources ) } ' ) else : logger . error ( f 'END dbt run failed for { str ( sources ) } ' ) def stage_table ( sources : list , project_dir : str , logger = None ): \"\"\"stage tables from datalake to staging layer in dwh Args: sources (list): list of tables to stage project_dir (str): path to the dbt project logger (logging.logger): (optional) for goblet `app.log` \"\"\" staging_successful = True for source in sources : ls_commands = [ \"dbt\" , \"run-operation\" , \"stage_external_sources\" , ] ls_commands += shlex . split ( f \"--args \\\" select: stg. { source } \\\" \" ) ls_commands += shlex . split ( f \"--vars \\\" ext_full_refresh: true \\\" \" ) logger . info ( f 'START staging { source } ' ) _ , sources_are_staged = run_subprocess ( ls_commands , project_dir , logger ) if sources_are_staged : logger . info ( f 'END staging successful for { source } ' ) else : logger . error ( f 'END staging successful for { source } ' ) staging_successful = False if staging_successful : return True else : return False Functions run_dbt_model def run_dbt_model ( sources : list , project_dir : str , logger = None ) Run dbt run and select child of sources provided as arg using \"--select source:stg.SOURCE+ See Using Unions Set Operators for Node Selection Parameters: Name Type Description Default sources list all sources that are parent of the branches that will be run None project_dir str path to the dbt project None logger logging.logger (optional) for goblet app.log None View Source def run_dbt_model ( sources : list , project_dir : str , logger = None ): \"\" \" Run ` dbt run ` and select child of sources provided as arg using `\" -- select source : stg . SOURCE + ` See [ Using Unions Set Operators for Node Selection ]( https : // docs . getdbt . com / reference / node - selection / set - operators # unions ) Args : sources ( list ): all sources that are parent of the branches that will be run project_dir ( str ): path to the dbt project logger ( logging . logger ): ( optional ) for goblet ` app . log ` \"\" \" ls_commands = [ \"dbt\" , \"run\" , \"--select\" ] ls_commands += [ f \"source:stg.{source}+\" for source in sources ] logger . info ( f 'START dbt run for {str(sources)}' ) _ , dbt_run_ok = run_subprocess ( ls_commands , project_dir , logger ) if dbt_run_ok : logger . info ( f 'END dbt run successful for {str(sources)}' ) else : logger . error ( f 'END dbt run failed for {str(sources)}' ) run_subprocess def run_subprocess ( ls_commands : list , working_dir : str , logger = None ) Run command provided as arg in path provided as arg Parameters: Name Type Description Default ls_commands list list of string representing the bash command to run None working_dir str path when you want to change directory to before execution None logger logging.logger (optional) for goblet app.log None View Source def run_subprocess ( ls_commands : list , working_dir : str , logger = None ) : \"\" \"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\" \" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy () , encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \"{out}\\n{err} failed\" if logger : logger . error ( msg ) return msg , False except Exception as e : if logger : logger . error ( str ( e )) return str ( e ) , False msg = out if logger : logger . info ( msg ) return msg , True stage_table def stage_table ( sources : list , project_dir : str , logger = None ) stage tables from datalake to staging layer in dwh Parameters: Name Type Description Default sources list list of tables to stage None project_dir str path to the dbt project None logger logging.logger (optional) for goblet app.log None View Source def stage_table ( sources : list , project_dir : str , logger = None ): \"\"\"stage tables from datalake to staging layer in dwh Args: sources (list): list of tables to stage project_dir (str): path to the dbt project logger (logging.logger): (optional) for goblet `app.log` \"\"\" staging_successful = True for source in sources : ls_commands = [ \"dbt\" , \"run-operation\" , \"stage_external_sources\" , ] ls_commands += shlex . split ( f \"--args \\\" select: stg.{source} \\\" \" ) ls_commands += shlex . split ( f \"--vars \\\" ext_full_refresh: true \\\" \" ) logger . info ( f 'START staging {source}' ) _ , sources_are_staged = run_subprocess ( ls_commands , project_dir , logger ) if sources_are_staged : logger . info ( f 'END staging successful for {source}' ) else : logger . error ( f 'END staging successful for {source}' ) staging_successful = False if staging_successful : return True else : return False","title":"Dbt Utils"},{"location":"reference/brocolib_transform/dbt_utils/#module-brocolib_transformdbt_utils","text":"View Source import os import shlex import subprocess def run_subprocess ( ls_commands : list , working_dir : str , logger = None ): \"\"\"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\"\" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy (), encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \" { out } \\n { err } failed\" if logger : logger . error ( msg ) return msg , False except Exception as e : if logger : logger . error ( str ( e )) return str ( e ), False msg = out if logger : logger . info ( msg ) return msg , True def run_dbt_model ( sources : list , project_dir : str , logger = None ): \"\"\"Run `dbt run` and select child of sources provided as arg using `\"--select source:stg.SOURCE+` See [Using Unions Set Operators for Node Selection](https://docs.getdbt.com/reference/node-selection/set-operators#unions) Args: sources (list): all sources that are parent of the branches that will be run project_dir (str): path to the dbt project logger (logging.logger): (optional) for goblet `app.log` \"\"\" ls_commands = [ \"dbt\" , \"run\" , \"--select\" ] ls_commands += [ f \"source:stg. { source } +\" for source in sources ] logger . info ( f 'START dbt run for { str ( sources ) } ' ) _ , dbt_run_ok = run_subprocess ( ls_commands , project_dir , logger ) if dbt_run_ok : logger . info ( f 'END dbt run successful for { str ( sources ) } ' ) else : logger . error ( f 'END dbt run failed for { str ( sources ) } ' ) def stage_table ( sources : list , project_dir : str , logger = None ): \"\"\"stage tables from datalake to staging layer in dwh Args: sources (list): list of tables to stage project_dir (str): path to the dbt project logger (logging.logger): (optional) for goblet `app.log` \"\"\" staging_successful = True for source in sources : ls_commands = [ \"dbt\" , \"run-operation\" , \"stage_external_sources\" , ] ls_commands += shlex . split ( f \"--args \\\" select: stg. { source } \\\" \" ) ls_commands += shlex . split ( f \"--vars \\\" ext_full_refresh: true \\\" \" ) logger . info ( f 'START staging { source } ' ) _ , sources_are_staged = run_subprocess ( ls_commands , project_dir , logger ) if sources_are_staged : logger . info ( f 'END staging successful for { source } ' ) else : logger . error ( f 'END staging successful for { source } ' ) staging_successful = False if staging_successful : return True else : return False","title":"Module brocolib_transform.dbt_utils"},{"location":"reference/brocolib_transform/dbt_utils/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_transform/dbt_utils/#run_dbt_model","text":"def run_dbt_model ( sources : list , project_dir : str , logger = None ) Run dbt run and select child of sources provided as arg using \"--select source:stg.SOURCE+ See Using Unions Set Operators for Node Selection Parameters: Name Type Description Default sources list all sources that are parent of the branches that will be run None project_dir str path to the dbt project None logger logging.logger (optional) for goblet app.log None View Source def run_dbt_model ( sources : list , project_dir : str , logger = None ): \"\" \" Run ` dbt run ` and select child of sources provided as arg using `\" -- select source : stg . SOURCE + ` See [ Using Unions Set Operators for Node Selection ]( https : // docs . getdbt . com / reference / node - selection / set - operators # unions ) Args : sources ( list ): all sources that are parent of the branches that will be run project_dir ( str ): path to the dbt project logger ( logging . logger ): ( optional ) for goblet ` app . log ` \"\" \" ls_commands = [ \"dbt\" , \"run\" , \"--select\" ] ls_commands += [ f \"source:stg.{source}+\" for source in sources ] logger . info ( f 'START dbt run for {str(sources)}' ) _ , dbt_run_ok = run_subprocess ( ls_commands , project_dir , logger ) if dbt_run_ok : logger . info ( f 'END dbt run successful for {str(sources)}' ) else : logger . error ( f 'END dbt run failed for {str(sources)}' )","title":"run_dbt_model"},{"location":"reference/brocolib_transform/dbt_utils/#run_subprocess","text":"def run_subprocess ( ls_commands : list , working_dir : str , logger = None ) Run command provided as arg in path provided as arg Parameters: Name Type Description Default ls_commands list list of string representing the bash command to run None working_dir str path when you want to change directory to before execution None logger logging.logger (optional) for goblet app.log None View Source def run_subprocess ( ls_commands : list , working_dir : str , logger = None ) : \"\" \"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\" \" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy () , encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \"{out}\\n{err} failed\" if logger : logger . error ( msg ) return msg , False except Exception as e : if logger : logger . error ( str ( e )) return str ( e ) , False msg = out if logger : logger . info ( msg ) return msg , True","title":"run_subprocess"},{"location":"reference/brocolib_transform/dbt_utils/#stage_table","text":"def stage_table ( sources : list , project_dir : str , logger = None ) stage tables from datalake to staging layer in dwh Parameters: Name Type Description Default sources list list of tables to stage None project_dir str path to the dbt project None logger logging.logger (optional) for goblet app.log None View Source def stage_table ( sources : list , project_dir : str , logger = None ): \"\"\"stage tables from datalake to staging layer in dwh Args: sources (list): list of tables to stage project_dir (str): path to the dbt project logger (logging.logger): (optional) for goblet `app.log` \"\"\" staging_successful = True for source in sources : ls_commands = [ \"dbt\" , \"run-operation\" , \"stage_external_sources\" , ] ls_commands += shlex . split ( f \"--args \\\" select: stg.{source} \\\" \" ) ls_commands += shlex . split ( f \"--vars \\\" ext_full_refresh: true \\\" \" ) logger . info ( f 'START staging {source}' ) _ , sources_are_staged = run_subprocess ( ls_commands , project_dir , logger ) if sources_are_staged : logger . info ( f 'END staging successful for {source}' ) else : logger . error ( f 'END staging successful for {source}' ) staging_successful = False if staging_successful : return True else : return False","title":"stage_table"},{"location":"reference/brocolib_utils/","text":"Module brocolib_utils Sub-modules brocolib_utils.catalog_gen brocolib_utils.cli brocolib_utils.credentials brocolib_utils.data_studio brocolib_utils.datalake brocolib_utils.drive brocolib_utils.fast_dbt brocolib_utils.settings","title":"Index"},{"location":"reference/brocolib_utils/#module-brocolib_utils","text":"","title":"Module brocolib_utils"},{"location":"reference/brocolib_utils/#sub-modules","text":"brocolib_utils.catalog_gen brocolib_utils.cli brocolib_utils.credentials brocolib_utils.data_studio brocolib_utils.datalake brocolib_utils.drive brocolib_utils.fast_dbt brocolib_utils.settings","title":"Sub-modules"},{"location":"reference/brocolib_utils/cli/","text":"Module brocolib_utils.cli View Source import typer app = typer . Typer () if __name__ == \"__main__\" : app () Variables app","title":"CLI"},{"location":"reference/brocolib_utils/cli/#module-brocolib_utilscli","text":"View Source import typer app = typer . Typer () if __name__ == \"__main__\" : app ()","title":"Module brocolib_utils.cli"},{"location":"reference/brocolib_utils/cli/#variables","text":"app","title":"Variables"},{"location":"reference/brocolib_utils/credentials/","text":"Module brocolib_utils.credentials View Source # from oauth2client.service_account import ServiceAccountCredentials import time import requests from google.auth import jwt from google.oauth2 import service_account import os import json from google.auth import crypt GOOGLE_APPLICATION_CREDENTIALS = os . environ . get ( 'GOOGLE_APPLICATION_CREDENTIALS' ) def get_credential_file_path () -> str : return os . getenv ( 'GOOGLE_APPLICATION_CREDENTIALS' ) def get_jwt_signer () -> crypt . RSASigner : return crypt . Signer ( get_private_key ()) def get_creds ( scopes : list ) -> service_account . Credentials : creds = service_account . Credentials . from_service_account_file ( filename = get_credential_file_path (), scopes = scopes ) return creds def get_credential_file_asdict (): with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json def get_key_from_credential_file ( key_to_get : str ) -> str : with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json . get ( key_to_get ) def get_private_key () -> str : return get_key_from_credential_file ( 'private_key' ) def get_client_email () -> str : return get_key_from_credential_file ( 'client_email' ) def get_jwt_token ( scopes : list ): client_email = get_client_email () iat = int ( time . time ()) exp = iat + 3600 header = { 'alg' : 'RS256' } claim_set = { \"iss\" : client_email , # \"sub\": \"test@leanscale.com\", \"sub\" : client_email , # \"email\": \"test@leanscale.com\", \"email\" : client_email , \"scope\" : \" \" . join ( scopes ), \"aud\" : \"https://oauth2.googleapis.com/token\" , \"exp\" : exp , \"iat\" : iat } # s = jwt.encode(header, claim_set, get_private_key()) s = jwt . encode ( signer = get_jwt_signer (), payload = claim_set , header = header , key_id = get_private_key () ) r = requests . post ( \"https://oauth2.googleapis.com/token\" , params = { \"grant_type\" : \"urn:ietf:params:oauth:grant-type:jwt-bearer\" , \"assertion\" : s }) # Right now you are getting an access token for each time. # If you put this code into a server, you have to control # your token expiration before creating a new token. return r . json ()[ 'access_token' ] def get_jwt_header ( scopes : list ): token = get_jwt_token ( scopes ) return { \"Authorization\" : f \"Bearer { token } \" , } Variables GOOGLE_APPLICATION_CREDENTIALS Functions get_client_email def get_client_email ( ) -> str View Source def get_client_email () -> str : return get_key_from_credential_file ( 'client_email' ) get_credential_file_asdict def get_credential_file_asdict ( ) View Source def get_credential_file_asdict (): with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json get_credential_file_path def get_credential_file_path ( ) -> str View Source def get_credential_file_path () -> str : return os . getenv ( 'GOOGLE_APPLICATION_CREDENTIALS' ) get_creds def get_creds ( scopes : list ) -> google . oauth2 . service_account . Credentials View Source def get_creds ( scopes : list ) -> service_account . Credentials : creds = service_account . Credentials . from_service_account_file ( filename = get_credential_file_path (), scopes = scopes ) return creds get_jwt_header def get_jwt_header ( scopes : list ) View Source def get_jwt_header ( scopes : list ) : token = get_jwt_token ( scopes ) return { \"Authorization\" : f \"Bearer {token}\" , } get_jwt_signer def get_jwt_signer ( ) -> google . auth . crypt . _python_rsa . RSASigner View Source def get_jwt_signer () -> crypt . RSASigner : return crypt . Signer ( get_private_key ()) get_jwt_token def get_jwt_token ( scopes : list ) View Source def get_jwt_token ( scopes : list ): client_email = get_client_email () iat = int ( time . time ()) exp = iat + 3600 header = { 'alg' : 'RS256' } claim_set = { \"iss\" : client_email , # \"sub\": \"test@leanscale.com\", \"sub\" : client_email , # \"email\": \"test@leanscale.com\", \"email\" : client_email , \"scope\" : \" \" . join ( scopes ), \"aud\" : \"https://oauth2.googleapis.com/token\" , \"exp\" : exp , \"iat\" : iat } # s = jwt.encode(header, claim_set, get_private_key()) s = jwt . encode ( signer = get_jwt_signer (), payload = claim_set , header = header , key_id = get_private_key () ) r = requests . post ( \"https://oauth2.googleapis.com/token\" , params = { \"grant_type\" : \"urn:ietf:params:oauth:grant-type:jwt-bearer\" , \"assertion\" : s }) # Right now you are getting an access token for each time. # If you put this code into a server, you have to control # your token expiration before creating a new token. return r . json ()[ 'access_token' ] get_key_from_credential_file def get_key_from_credential_file ( key_to_get : str ) -> str View Source def get_key_from_credential_file ( key_to_get : str ) -> str : with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json . get ( key_to_get ) get_private_key def get_private_key ( ) -> str View Source def get_private_key () -> str : return get_key_from_credential_file ( 'private_key' )","title":"Credentials"},{"location":"reference/brocolib_utils/credentials/#module-brocolib_utilscredentials","text":"View Source # from oauth2client.service_account import ServiceAccountCredentials import time import requests from google.auth import jwt from google.oauth2 import service_account import os import json from google.auth import crypt GOOGLE_APPLICATION_CREDENTIALS = os . environ . get ( 'GOOGLE_APPLICATION_CREDENTIALS' ) def get_credential_file_path () -> str : return os . getenv ( 'GOOGLE_APPLICATION_CREDENTIALS' ) def get_jwt_signer () -> crypt . RSASigner : return crypt . Signer ( get_private_key ()) def get_creds ( scopes : list ) -> service_account . Credentials : creds = service_account . Credentials . from_service_account_file ( filename = get_credential_file_path (), scopes = scopes ) return creds def get_credential_file_asdict (): with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json def get_key_from_credential_file ( key_to_get : str ) -> str : with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json . get ( key_to_get ) def get_private_key () -> str : return get_key_from_credential_file ( 'private_key' ) def get_client_email () -> str : return get_key_from_credential_file ( 'client_email' ) def get_jwt_token ( scopes : list ): client_email = get_client_email () iat = int ( time . time ()) exp = iat + 3600 header = { 'alg' : 'RS256' } claim_set = { \"iss\" : client_email , # \"sub\": \"test@leanscale.com\", \"sub\" : client_email , # \"email\": \"test@leanscale.com\", \"email\" : client_email , \"scope\" : \" \" . join ( scopes ), \"aud\" : \"https://oauth2.googleapis.com/token\" , \"exp\" : exp , \"iat\" : iat } # s = jwt.encode(header, claim_set, get_private_key()) s = jwt . encode ( signer = get_jwt_signer (), payload = claim_set , header = header , key_id = get_private_key () ) r = requests . post ( \"https://oauth2.googleapis.com/token\" , params = { \"grant_type\" : \"urn:ietf:params:oauth:grant-type:jwt-bearer\" , \"assertion\" : s }) # Right now you are getting an access token for each time. # If you put this code into a server, you have to control # your token expiration before creating a new token. return r . json ()[ 'access_token' ] def get_jwt_header ( scopes : list ): token = get_jwt_token ( scopes ) return { \"Authorization\" : f \"Bearer { token } \" , }","title":"Module brocolib_utils.credentials"},{"location":"reference/brocolib_utils/credentials/#variables","text":"GOOGLE_APPLICATION_CREDENTIALS","title":"Variables"},{"location":"reference/brocolib_utils/credentials/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/credentials/#get_client_email","text":"def get_client_email ( ) -> str View Source def get_client_email () -> str : return get_key_from_credential_file ( 'client_email' )","title":"get_client_email"},{"location":"reference/brocolib_utils/credentials/#get_credential_file_asdict","text":"def get_credential_file_asdict ( ) View Source def get_credential_file_asdict (): with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json","title":"get_credential_file_asdict"},{"location":"reference/brocolib_utils/credentials/#get_credential_file_path","text":"def get_credential_file_path ( ) -> str View Source def get_credential_file_path () -> str : return os . getenv ( 'GOOGLE_APPLICATION_CREDENTIALS' )","title":"get_credential_file_path"},{"location":"reference/brocolib_utils/credentials/#get_creds","text":"def get_creds ( scopes : list ) -> google . oauth2 . service_account . Credentials View Source def get_creds ( scopes : list ) -> service_account . Credentials : creds = service_account . Credentials . from_service_account_file ( filename = get_credential_file_path (), scopes = scopes ) return creds","title":"get_creds"},{"location":"reference/brocolib_utils/credentials/#get_jwt_header","text":"def get_jwt_header ( scopes : list ) View Source def get_jwt_header ( scopes : list ) : token = get_jwt_token ( scopes ) return { \"Authorization\" : f \"Bearer {token}\" , }","title":"get_jwt_header"},{"location":"reference/brocolib_utils/credentials/#get_jwt_signer","text":"def get_jwt_signer ( ) -> google . auth . crypt . _python_rsa . RSASigner View Source def get_jwt_signer () -> crypt . RSASigner : return crypt . Signer ( get_private_key ())","title":"get_jwt_signer"},{"location":"reference/brocolib_utils/credentials/#get_jwt_token","text":"def get_jwt_token ( scopes : list ) View Source def get_jwt_token ( scopes : list ): client_email = get_client_email () iat = int ( time . time ()) exp = iat + 3600 header = { 'alg' : 'RS256' } claim_set = { \"iss\" : client_email , # \"sub\": \"test@leanscale.com\", \"sub\" : client_email , # \"email\": \"test@leanscale.com\", \"email\" : client_email , \"scope\" : \" \" . join ( scopes ), \"aud\" : \"https://oauth2.googleapis.com/token\" , \"exp\" : exp , \"iat\" : iat } # s = jwt.encode(header, claim_set, get_private_key()) s = jwt . encode ( signer = get_jwt_signer (), payload = claim_set , header = header , key_id = get_private_key () ) r = requests . post ( \"https://oauth2.googleapis.com/token\" , params = { \"grant_type\" : \"urn:ietf:params:oauth:grant-type:jwt-bearer\" , \"assertion\" : s }) # Right now you are getting an access token for each time. # If you put this code into a server, you have to control # your token expiration before creating a new token. return r . json ()[ 'access_token' ]","title":"get_jwt_token"},{"location":"reference/brocolib_utils/credentials/#get_key_from_credential_file","text":"def get_key_from_credential_file ( key_to_get : str ) -> str View Source def get_key_from_credential_file ( key_to_get : str ) -> str : with open ( get_credential_file_path ()) as f : cred_json = json . load ( f ) return cred_json . get ( key_to_get )","title":"get_key_from_credential_file"},{"location":"reference/brocolib_utils/credentials/#get_private_key","text":"def get_private_key ( ) -> str View Source def get_private_key () -> str : return get_key_from_credential_file ( 'private_key' )","title":"get_private_key"},{"location":"reference/brocolib_utils/data_studio/","text":"Module brocolib_utils.data_studio View Source import json import requests from . import credentials from . import settings from .settings import DATA_STUDIO_API_BASE_URL , DATA_STUDIO_ASSETS_TYPES def response_helper ( ** kwargs ) -> dict : return { \"status\" : kwargs [ 'status' ], \"response\" : kwargs [ 'response' ] } class DataStudio : # This func runs as default when DataStudio class runs. def __init__ ( self , * args , ** kwargs ): self . headers = credentials . get_jwt_header ( settings . DATA_STUDIO_API_SCOPE ) # def get_assets(self, asset_id, asset_type) -> dict: def get_assets ( self , asset_type : DATA_STUDIO_ASSETS_TYPES ) -> dict : r = requests . get ( url = f \" { DATA_STUDIO_API_BASE_URL } /assets:search\" , headers = self . headers , params = { \"assetTypes\" :[ asset_type ]} ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def get_asset_by_title ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_title : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ]: return list ( filter ( lambda asset : asset [ \"title\" ] == asset_title , assets [ \"response\" ][ \"assets\" ]))[ 0 ] def get_asset_by_name ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_name : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ]: return list ( filter ( lambda asset : asset [ \"name\" ] == asset_name , assets [ \"response\" ][ \"assets\" ]))[ 0 ] def get_permissions ( self , asset_id : str ) -> dict : r = requests . get ( f \" { DATA_STUDIO_API_BASE_URL } /assets/ { asset_id } /permissions\" , headers = self . headers ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict : data = { 'permissions' : permission_dict } # data = permission_dict r = requests . patch ( f \" { DATA_STUDIO_API_BASE_URL } /assets/ { asset_id } /permissions\" , headers = self . headers , json = data ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : # return response_helper(status=r.status_code, response=r.reason) return r # def add_member(self, role: str, user_email_array: list) -> dict: # data = { # 'name':assetId, # 'role': role, # 'members': user_email_array # } # r = requests.post(f\"{DATA_STUDIO_API_BASE_URL}/assets/{assetId}/permissions:addMembers\", # headers=self.headers, # data=data) # if r.status_code == 200: # return response_helper(status=r.status_code, response=json.loads(r.text)) # else: # return response_helper(status=r.status_code, response=r.reason) # def revoke_permissions(self, user_email_array: list) -> dict: # data = { # 'name':assetId, # 'members': user_email_array # } # r = requests.post(f\"{DATA_STUDIO_API_BASE_URL}/assets/{assetId}/permissions:revokeAllPermissions\", # headers=self.headers, # data=data) # if r.status_code == 200: # return response_helper(status=r.status_code, response=json.loads(r.text)) # else: # return response_helper(status=r.status_code, response=r.reason) ## -----| TESTS |----- # test_user_array = ['group:administrators@brocoli.tech'] # client = DataStudio() # asset_type = \"REPORT\" # # asset_type = \"DATA_SOURCE\" # # assets = client.get_assets(asset_type) # # # print(assets) # # for element in assets[\"response\"][\"assets\"]: # # print(element[\"title\"]) # # permissions = client.get_permissions(asset_id=element[\"name\"]) # # print(permissions) # # print('=====') # asset = client.get_asset_by_title( # asset_title=\"dbt monitoring\", # asset_type=\"REPORT\" # ) # asset_id = asset[\"name\"] # permissions = client.get_permissions(asset_id) # print(permissions) # # permission_dict = {'permissions': {'EDITOR': {'members': ['group:administrators@brocoli.tech']}, 'OWNER': {'members': ['user:amirb@brocoli.tech']}}} # # updated_permissions = client.update_permissions(asset_id, permission_dict) # # print(updated_permissions) # print('hello') Variables DATA_STUDIO_API_BASE_URL Functions response_helper def response_helper ( ** kwargs ) -> dict View Source def response_helper ( ** kwargs ) -> dict : return { \"status\" : kwargs [ 'status' ], \"response\" : kwargs [ 'response' ] } Classes DataStudio class DataStudio ( * args , ** kwargs ) View Source class DataStudio : # This func runs as default when DataStudio class runs . def __init__ ( self , * args , ** kwargs ) : self . headers = credentials . get_jwt_header ( settings . DATA_STUDIO_API_SCOPE ) # def get_assets ( self , asset_id , asset_type ) -> dict : def get_assets ( self , asset_type : DATA_STUDIO_ASSETS_TYPES ) -> dict : r = requests . get ( url = f \"{DATA_STUDIO_API_BASE_URL}/assets:search\" , headers = self . headers , params = { \"assetTypes\" : [ asset_type ] } ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def get_asset_by_title ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_title : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"title\" ] == asset_title , assets [ \"response\" ][ \"assets\" ] )) [ 0 ] def get_asset_by_name ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_name : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"name\" ] == asset_name , assets [ \"response\" ][ \"assets\" ] )) [ 0 ] def get_permissions ( self , asset_id : str ) -> dict : r = requests . get ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict : data = { 'permissions' : permission_dict } # data = permission_dict r = requests . patch ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers , json = data ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : # return response_helper ( status = r . status_code , response = r . reason ) return r # def add_member ( self , role : str , user_email_array : list ) -> dict : # data = { # 'name' : assetId , # 'role' : role , # 'members' : user_email_array # } # r = requests . post ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{assetId}/permissions:addMembers\" , # headers = self . headers , # data = data ) # if r . status_code == 200 : # return response_helper ( status = r . status_code , response = json . loads ( r . text )) # else : # return response_helper ( status = r . status_code , response = r . reason ) Methods get_asset_by_name def get_asset_by_name ( self , asset_type : Literal [ 'REPORT' , 'DATA_SOURCE' ], asset_name : str ) -> str View Source def get_asset_by_name ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_name : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"name\" ] == asset_name , assets [ \"response\" ][ \"assets\" ]))[ 0 ] get_asset_by_title def get_asset_by_title ( self , asset_type : Literal [ 'REPORT' , 'DATA_SOURCE' ], asset_title : str ) -> str View Source def get_asset_by_title ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_title : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"title\" ] == asset_title , assets [ \"response\" ][ \"assets\" ]))[ 0 ] get_assets def get_assets ( self , asset_type : Literal [ 'REPORT' , 'DATA_SOURCE' ] ) -> dict View Source def get_assets ( self , asset_type : DATA_STUDIO_ASSETS_TYPES ) -> dict : r = requests . get ( url = f \"{DATA_STUDIO_API_BASE_URL}/assets:search\" , headers = self . headers , params = { \"assetTypes\" : [ asset_type ] } ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) get_permissions def get_permissions ( self , asset_id : str ) -> dict View Source def get_permissions ( self , asset_id : str ) -> dict : r = requests . get ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) update_permissions def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict View Source def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict : data = { 'permissions' : permission_dict } # data = permission_dict r = requests . patch ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers , json = data ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : # return response_helper(status=r.status_code, response=r.reason) return r","title":"Data Studio"},{"location":"reference/brocolib_utils/data_studio/#module-brocolib_utilsdata_studio","text":"View Source import json import requests from . import credentials from . import settings from .settings import DATA_STUDIO_API_BASE_URL , DATA_STUDIO_ASSETS_TYPES def response_helper ( ** kwargs ) -> dict : return { \"status\" : kwargs [ 'status' ], \"response\" : kwargs [ 'response' ] } class DataStudio : # This func runs as default when DataStudio class runs. def __init__ ( self , * args , ** kwargs ): self . headers = credentials . get_jwt_header ( settings . DATA_STUDIO_API_SCOPE ) # def get_assets(self, asset_id, asset_type) -> dict: def get_assets ( self , asset_type : DATA_STUDIO_ASSETS_TYPES ) -> dict : r = requests . get ( url = f \" { DATA_STUDIO_API_BASE_URL } /assets:search\" , headers = self . headers , params = { \"assetTypes\" :[ asset_type ]} ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def get_asset_by_title ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_title : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ]: return list ( filter ( lambda asset : asset [ \"title\" ] == asset_title , assets [ \"response\" ][ \"assets\" ]))[ 0 ] def get_asset_by_name ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_name : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ]: return list ( filter ( lambda asset : asset [ \"name\" ] == asset_name , assets [ \"response\" ][ \"assets\" ]))[ 0 ] def get_permissions ( self , asset_id : str ) -> dict : r = requests . get ( f \" { DATA_STUDIO_API_BASE_URL } /assets/ { asset_id } /permissions\" , headers = self . headers ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict : data = { 'permissions' : permission_dict } # data = permission_dict r = requests . patch ( f \" { DATA_STUDIO_API_BASE_URL } /assets/ { asset_id } /permissions\" , headers = self . headers , json = data ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : # return response_helper(status=r.status_code, response=r.reason) return r # def add_member(self, role: str, user_email_array: list) -> dict: # data = { # 'name':assetId, # 'role': role, # 'members': user_email_array # } # r = requests.post(f\"{DATA_STUDIO_API_BASE_URL}/assets/{assetId}/permissions:addMembers\", # headers=self.headers, # data=data) # if r.status_code == 200: # return response_helper(status=r.status_code, response=json.loads(r.text)) # else: # return response_helper(status=r.status_code, response=r.reason) # def revoke_permissions(self, user_email_array: list) -> dict: # data = { # 'name':assetId, # 'members': user_email_array # } # r = requests.post(f\"{DATA_STUDIO_API_BASE_URL}/assets/{assetId}/permissions:revokeAllPermissions\", # headers=self.headers, # data=data) # if r.status_code == 200: # return response_helper(status=r.status_code, response=json.loads(r.text)) # else: # return response_helper(status=r.status_code, response=r.reason) ## -----| TESTS |----- # test_user_array = ['group:administrators@brocoli.tech'] # client = DataStudio() # asset_type = \"REPORT\" # # asset_type = \"DATA_SOURCE\" # # assets = client.get_assets(asset_type) # # # print(assets) # # for element in assets[\"response\"][\"assets\"]: # # print(element[\"title\"]) # # permissions = client.get_permissions(asset_id=element[\"name\"]) # # print(permissions) # # print('=====') # asset = client.get_asset_by_title( # asset_title=\"dbt monitoring\", # asset_type=\"REPORT\" # ) # asset_id = asset[\"name\"] # permissions = client.get_permissions(asset_id) # print(permissions) # # permission_dict = {'permissions': {'EDITOR': {'members': ['group:administrators@brocoli.tech']}, 'OWNER': {'members': ['user:amirb@brocoli.tech']}}} # # updated_permissions = client.update_permissions(asset_id, permission_dict) # # print(updated_permissions) # print('hello')","title":"Module brocolib_utils.data_studio"},{"location":"reference/brocolib_utils/data_studio/#variables","text":"DATA_STUDIO_API_BASE_URL","title":"Variables"},{"location":"reference/brocolib_utils/data_studio/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/data_studio/#response_helper","text":"def response_helper ( ** kwargs ) -> dict View Source def response_helper ( ** kwargs ) -> dict : return { \"status\" : kwargs [ 'status' ], \"response\" : kwargs [ 'response' ] }","title":"response_helper"},{"location":"reference/brocolib_utils/data_studio/#classes","text":"","title":"Classes"},{"location":"reference/brocolib_utils/data_studio/#datastudio","text":"class DataStudio ( * args , ** kwargs ) View Source class DataStudio : # This func runs as default when DataStudio class runs . def __init__ ( self , * args , ** kwargs ) : self . headers = credentials . get_jwt_header ( settings . DATA_STUDIO_API_SCOPE ) # def get_assets ( self , asset_id , asset_type ) -> dict : def get_assets ( self , asset_type : DATA_STUDIO_ASSETS_TYPES ) -> dict : r = requests . get ( url = f \"{DATA_STUDIO_API_BASE_URL}/assets:search\" , headers = self . headers , params = { \"assetTypes\" : [ asset_type ] } ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def get_asset_by_title ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_title : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"title\" ] == asset_title , assets [ \"response\" ][ \"assets\" ] )) [ 0 ] def get_asset_by_name ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_name : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"name\" ] == asset_name , assets [ \"response\" ][ \"assets\" ] )) [ 0 ] def get_permissions ( self , asset_id : str ) -> dict : r = requests . get ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason ) def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict : data = { 'permissions' : permission_dict } # data = permission_dict r = requests . patch ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers , json = data ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : # return response_helper ( status = r . status_code , response = r . reason ) return r # def add_member ( self , role : str , user_email_array : list ) -> dict : # data = { # 'name' : assetId , # 'role' : role , # 'members' : user_email_array # } # r = requests . post ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{assetId}/permissions:addMembers\" , # headers = self . headers , # data = data ) # if r . status_code == 200 : # return response_helper ( status = r . status_code , response = json . loads ( r . text )) # else : # return response_helper ( status = r . status_code , response = r . reason )","title":"DataStudio"},{"location":"reference/brocolib_utils/data_studio/#methods","text":"","title":"Methods"},{"location":"reference/brocolib_utils/data_studio/#get_asset_by_name","text":"def get_asset_by_name ( self , asset_type : Literal [ 'REPORT' , 'DATA_SOURCE' ], asset_name : str ) -> str View Source def get_asset_by_name ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_name : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"name\" ] == asset_name , assets [ \"response\" ][ \"assets\" ]))[ 0 ]","title":"get_asset_by_name"},{"location":"reference/brocolib_utils/data_studio/#get_asset_by_title","text":"def get_asset_by_title ( self , asset_type : Literal [ 'REPORT' , 'DATA_SOURCE' ], asset_title : str ) -> str View Source def get_asset_by_title ( self , asset_type : DATA_STUDIO_ASSETS_TYPES , asset_title : str ) -> str : assets = self . get_assets ( asset_type ) if assets [ \"status\" ] == 200 : if assets [ \"response\" ][ \"assets\" ] : return list ( filter ( lambda asset : asset [ \"title\" ] == asset_title , assets [ \"response\" ][ \"assets\" ]))[ 0 ]","title":"get_asset_by_title"},{"location":"reference/brocolib_utils/data_studio/#get_assets","text":"def get_assets ( self , asset_type : Literal [ 'REPORT' , 'DATA_SOURCE' ] ) -> dict View Source def get_assets ( self , asset_type : DATA_STUDIO_ASSETS_TYPES ) -> dict : r = requests . get ( url = f \"{DATA_STUDIO_API_BASE_URL}/assets:search\" , headers = self . headers , params = { \"assetTypes\" : [ asset_type ] } ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason )","title":"get_assets"},{"location":"reference/brocolib_utils/data_studio/#get_permissions","text":"def get_permissions ( self , asset_id : str ) -> dict View Source def get_permissions ( self , asset_id : str ) -> dict : r = requests . get ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : return response_helper ( status = r . status_code , response = r . reason )","title":"get_permissions"},{"location":"reference/brocolib_utils/data_studio/#update_permissions","text":"def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict View Source def update_permissions ( self , asset_id : str , permission_dict : dict ) -> dict : data = { 'permissions' : permission_dict } # data = permission_dict r = requests . patch ( f \"{DATA_STUDIO_API_BASE_URL}/assets/{asset_id}/permissions\" , headers = self . headers , json = data ) if r . status_code == 200 : return response_helper ( status = r . status_code , response = json . loads ( r . text )) else : # return response_helper(status=r.status_code, response=r.reason) return r","title":"update_permissions"},{"location":"reference/brocolib_utils/settings/","text":"Module brocolib_utils.settings View Source import os from typing import Literal SCOPE = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] ALL_FIELDS_SHEET_NAME = \"all_fields\" GROUP_TABLE_COL = \"table_group\" DESCRIPTION_COL = \"description\" TABLE_NAME_COL = \"table_name\" SOURCE_DATASET_COL = \"source_dataset\" FIELD_NAME_COL = \"field_name\" FIELD_DESCRIPTION_COL = \"field_description\" FILE_FORMAT_COL = \"file_format\" GCS_PREFIX_COL = \"cloud_storage_prefix\" SOURCES_SHEET_NAME = \"sources\" DBT_MODELS_SHEET_NAME = \"dbt_models\" DBT_MODELS_PATH = f \" { os . environ . get ( 'DBT_PATH' ) } /models\" DATALAKE_PROJECT = os . environ . get ( 'BACK_PROJECT_ID' ) DATALAKE_BUCKET = os . environ . get ( 'DATALAKE_BUCKET' ) GOOGLE_SHEETS_API_SCOPES = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] DATA_STUDIO_API_BASE_URL = \"https://datastudio.googleapis.com/v1\" DATA_STUDIO_API_SCOPE = [ 'https://www.googleapis.com/auth/datastudio' , 'https://www.googleapis.com/auth/userinfo.email' , 'https://www.googleapis.com/auth/userinfo.profile' , 'https://www.googleapis.com/auth/admin.directory.user' , 'https://www.googleapis.com/auth/admin.directory.group' , 'openid' ] DATA_STUDIO_ASSETS_TYPES = Literal [ 'REPORT' , 'DATA_SOURCE' ] BIGQUERY_MAPPING_TYPES = { \"integer\" : \"integer\" , \"timestamp without time zone\" : \"timestamp\" , \"boolean\" : \"bool\" , \"character varying\" : \"string\" , \"text\" : \"string\" , \"date\" : \"date\" , \"double precision\" : \"float64\" , \"numeric\" : \"numeric\" , \"bytea\" : \"bytes\" , \"bigint\" : \"bigint\" } Variables ALL_FIELDS_SHEET_NAME BIGQUERY_MAPPING_TYPES DATALAKE_BUCKET DATALAKE_PROJECT DATA_STUDIO_API_BASE_URL DATA_STUDIO_API_SCOPE DATA_STUDIO_ASSETS_TYPES DBT_MODELS_PATH DBT_MODELS_SHEET_NAME DESCRIPTION_COL FIELD_DESCRIPTION_COL FIELD_NAME_COL FILE_FORMAT_COL GCS_PREFIX_COL GOOGLE_SHEETS_API_SCOPES GROUP_TABLE_COL SCOPE SOURCES_SHEET_NAME SOURCE_DATASET_COL TABLE_NAME_COL","title":"Settings"},{"location":"reference/brocolib_utils/settings/#module-brocolib_utilssettings","text":"View Source import os from typing import Literal SCOPE = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] ALL_FIELDS_SHEET_NAME = \"all_fields\" GROUP_TABLE_COL = \"table_group\" DESCRIPTION_COL = \"description\" TABLE_NAME_COL = \"table_name\" SOURCE_DATASET_COL = \"source_dataset\" FIELD_NAME_COL = \"field_name\" FIELD_DESCRIPTION_COL = \"field_description\" FILE_FORMAT_COL = \"file_format\" GCS_PREFIX_COL = \"cloud_storage_prefix\" SOURCES_SHEET_NAME = \"sources\" DBT_MODELS_SHEET_NAME = \"dbt_models\" DBT_MODELS_PATH = f \" { os . environ . get ( 'DBT_PATH' ) } /models\" DATALAKE_PROJECT = os . environ . get ( 'BACK_PROJECT_ID' ) DATALAKE_BUCKET = os . environ . get ( 'DATALAKE_BUCKET' ) GOOGLE_SHEETS_API_SCOPES = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] DATA_STUDIO_API_BASE_URL = \"https://datastudio.googleapis.com/v1\" DATA_STUDIO_API_SCOPE = [ 'https://www.googleapis.com/auth/datastudio' , 'https://www.googleapis.com/auth/userinfo.email' , 'https://www.googleapis.com/auth/userinfo.profile' , 'https://www.googleapis.com/auth/admin.directory.user' , 'https://www.googleapis.com/auth/admin.directory.group' , 'openid' ] DATA_STUDIO_ASSETS_TYPES = Literal [ 'REPORT' , 'DATA_SOURCE' ] BIGQUERY_MAPPING_TYPES = { \"integer\" : \"integer\" , \"timestamp without time zone\" : \"timestamp\" , \"boolean\" : \"bool\" , \"character varying\" : \"string\" , \"text\" : \"string\" , \"date\" : \"date\" , \"double precision\" : \"float64\" , \"numeric\" : \"numeric\" , \"bytea\" : \"bytes\" , \"bigint\" : \"bigint\" }","title":"Module brocolib_utils.settings"},{"location":"reference/brocolib_utils/settings/#variables","text":"ALL_FIELDS_SHEET_NAME BIGQUERY_MAPPING_TYPES DATALAKE_BUCKET DATALAKE_PROJECT DATA_STUDIO_API_BASE_URL DATA_STUDIO_API_SCOPE DATA_STUDIO_ASSETS_TYPES DBT_MODELS_PATH DBT_MODELS_SHEET_NAME DESCRIPTION_COL FIELD_DESCRIPTION_COL FIELD_NAME_COL FILE_FORMAT_COL GCS_PREFIX_COL GOOGLE_SHEETS_API_SCOPES GROUP_TABLE_COL SCOPE SOURCES_SHEET_NAME SOURCE_DATASET_COL TABLE_NAME_COL","title":"Variables"},{"location":"reference/brocolib_utils/catalog_gen/","text":"Module brocolib_utils.catalog_gen Sub-modules brocolib_utils.catalog_gen.cli brocolib_utils.catalog_gen.dbt_catalog","title":"Index"},{"location":"reference/brocolib_utils/catalog_gen/#module-brocolib_utilscatalog_gen","text":"","title":"Module brocolib_utils.catalog_gen"},{"location":"reference/brocolib_utils/catalog_gen/#sub-modules","text":"brocolib_utils.catalog_gen.cli brocolib_utils.catalog_gen.dbt_catalog","title":"Sub-modules"},{"location":"reference/brocolib_utils/catalog_gen/cli/","text":"Module brocolib_utils.catalog_gen.cli View Source import os import argparse from brocolib_utils.catalog_gen.dbt_catalog import ( generate_dbt_docs , get_dbt_populated_index , run_dbt_debug , run_dbt_deps , upload_populated_index ) if __name__ == \"__main__\" : my_parser = argparse . ArgumentParser ( description = 'List the content of a folder' ) my_parser . add_argument ( '--target-path' , dest = 'target_path' , type = str , help = 'dbt target path' , default = '/tmp/target' ) my_parser . add_argument ( '--debug' , dest = 'debug' , help = 'enable debug' , action = \"store_true\" ) my_parser . add_argument ( '--ci' , dest = 'is_CI' , help = 'if the runtime is a CI/CD pipeline' , action = \"store_true\" ) args = my_parser . parse_args () target_path = args . target_path is_CI = args . is_CI debug = args . debug if is_CI : run_dbt_deps () if debug : run_dbt_debug () generate_dbt_docs () new_content = get_dbt_populated_index ( target_folder = target_path ) upload_populated_index ( content = new_content )","title":"CLI"},{"location":"reference/brocolib_utils/catalog_gen/cli/#module-brocolib_utilscatalog_gencli","text":"View Source import os import argparse from brocolib_utils.catalog_gen.dbt_catalog import ( generate_dbt_docs , get_dbt_populated_index , run_dbt_debug , run_dbt_deps , upload_populated_index ) if __name__ == \"__main__\" : my_parser = argparse . ArgumentParser ( description = 'List the content of a folder' ) my_parser . add_argument ( '--target-path' , dest = 'target_path' , type = str , help = 'dbt target path' , default = '/tmp/target' ) my_parser . add_argument ( '--debug' , dest = 'debug' , help = 'enable debug' , action = \"store_true\" ) my_parser . add_argument ( '--ci' , dest = 'is_CI' , help = 'if the runtime is a CI/CD pipeline' , action = \"store_true\" ) args = my_parser . parse_args () target_path = args . target_path is_CI = args . is_CI debug = args . debug if is_CI : run_dbt_deps () if debug : run_dbt_debug () generate_dbt_docs () new_content = get_dbt_populated_index ( target_folder = target_path ) upload_populated_index ( content = new_content )","title":"Module brocolib_utils.catalog_gen.cli"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/","text":"Module brocolib_utils.catalog_gen.dbt_catalog View Source import json import os import re from google.cloud import storage import shlex import subprocess DBT_DOCS_BUCKET = os . environ . get ( 'DBT_DOCS_BUCKET' ) GCP_PROJECT = os . environ . get ( 'FRONT_PROJECT_ID' ) DBT_DOCS_READ_GROUP = os . environ . get ( 'DBT_DOCS_READ_GROUP' ) DBT_PROJECT_DIR = os . environ . get ( 'DBT_PATH' ) MAPPING_DICT = { \"num_rows\" : { \"id\" : \"num_rows\" , \"label\" : \"# Lignes\" , \"description\" : \"Nombre approximatif de lignes dans la table\" }, \"num_bytes\" : { \"id\" : \"num_bytes\" , \"label\" : \"Taille\" , \"description\" : \"Taille de la table (en bytes)\" } } def translate_catalog ( catalog_data ): for element_type in [ 'nodes' , 'sources' ]: # navigate into catalog for node in catalog_data [ element_type ]: if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_rows' ): catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"label\" ] = MAPPING_DICT [ \"num_rows\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"description\" ] = MAPPING_DICT [ \"num_rows\" ][ \"description\" ] if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_bytes' ): catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"label\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"description\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"description\" ] return catalog_data def get_dbt_populated_index ( target_folder ): print ( 'Populating index.html ...' ) base_index_path = os . path . join ( os . path . dirname ( __file__ ), 'base_index.html' ) manifest_path = os . path . join ( target_folder , 'manifest.json' ) catalog_path = os . path . join ( target_folder , 'catalog.json' ) search_str = 'o=[i(\"manifest\",\"manifest.json\"+t),i(\"catalog\",\"catalog.json\"+t)]' with open ( base_index_path , 'r' ) as f : content_index = f . read () with open ( manifest_path , 'r' ) as f : json_manifest = json . loads ( f . read ()) IGNORE_PROJECTS = [ 'dbt' , 'dbt_bigquery' , 'dbt_external_tables' , 'dbt_utils' , 'codegen' ] for element_type in [ 'nodes' , 'sources' , 'macros' , 'parent_map' , 'child_map' ]: # navigate into manifest # We transform to list to not change dict size during iteration, we use default value {} to handle KeyError for key in list ( json_manifest . get ( element_type , {}) . keys ()): for ignore_project in IGNORE_PROJECTS : if re . match ( fr '^.*\\. { ignore_project } \\.' , key ): # match with string that start with '*.<ignore_project>.' del json_manifest [ element_type ][ key ] # delete element with open ( catalog_path , 'r' ) as f : json_catalog = json . loads ( f . read ()) json_catalog = translate_catalog ( json_catalog ) # Write manifest & catalog jsons in index.html new_str = \"o=[{label: 'manifest', data: \" + json . dumps ( json_manifest ) + \"},{label: 'catalog', data: \" + json . dumps ( json_catalog ) + \"}]\" new_content = content_index . replace ( search_str , new_str ) # Select \"Database\" tab by default & Hide \"Project\" tab # new_content = new_content.replace('{e.nav_selected=\"project\"}', '{e.nav_selected=\"database\"}') # new_content = new_content.replace('<div class=\"switch \">', '<div class=\"switch \" hidden>') print ( 'Successfully populated index.html' ) return new_content def upload_populated_index ( content , file_name = 'index.html' ): print ( 'Loading index.html to GCS ...' ) storage_client = storage . Client ( project = GCP_PROJECT ) bucket = storage_client . get_bucket ( DBT_DOCS_BUCKET ) blob = bucket . blob ( file_name ) blob . upload_from_string ( content , content_type = 'text/html' ) print ( 'Successfully loaded index.html to GCS' ) # Manage ACL acl = blob . acl acl . reload () acl . group ( DBT_DOCS_READ_GROUP ) . grant_read () acl . save () blob . acl . save ( acl = acl ) print ( 'Added ACL to index.html' ) def run_subprocess ( ls_commands , working_dir ): \"\"\"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\"\" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy (), encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \" { out } \\n { err } failed\" print ( msg ) return msg , False except Exception as e : raise e msg = out print ( msg ) return msg , True def generate_dbt_docs (): \"\"\" Run `dbt docs generate` \"\"\" ls_commands = [ \"dbt\" , \"docs\" , \"generate\" ] print ( f 'Starting dbt docs generate ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt docs generate' ) else : print ( f 'Failed run dbt docs generate' ) def run_dbt_debug (): \"\"\" Run `dbt debug ` \"\"\" ls_commands = [ \"dbt\" , \"debug\" ] print ( f 'Starting dbt debug ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt debug' ) else : print ( f 'Failed while trying to run dbt debug' ) def run_dbt_deps (): \"\"\" Run `dbt debug ` \"\"\" ls_commands = [ \"dbt\" , \"deps\" ] print ( f 'Starting dbt deps ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt deps' ) else : print ( f 'Failed while trying to run dbt deps' ) Variables DBT_DOCS_BUCKET DBT_DOCS_READ_GROUP DBT_PROJECT_DIR GCP_PROJECT MAPPING_DICT Functions generate_dbt_docs def generate_dbt_docs ( ) Run dbt docs generate View Source def generate_dbt_docs () : \"\" \" Run `dbt docs generate` \"\" \" ls_commands = [ \"dbt\" , \"docs\" , \"generate\" ] print ( f 'Starting dbt docs generate ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt docs generate' ) else : print ( f 'Failed run dbt docs generate' ) get_dbt_populated_index def get_dbt_populated_index ( target_folder ) View Source def get_dbt_populated_index ( target_folder ) : print ( 'Populating index.html ...' ) base_index_path = os . path . join ( os . path . dirname ( __file__ ), 'base_index.html' ) manifest_path = os . path . join ( target_folder , 'manifest.json' ) catalog_path = os . path . join ( target_folder , 'catalog.json' ) search_str = 'o=[i(\"manifest\",\"manifest.json\"+t),i(\"catalog\",\"catalog.json\"+t)]' with open ( base_index_path , 'r' ) as f : content_index = f . read () with open ( manifest_path , 'r' ) as f : json_manifest = json . loads ( f . read ()) IGNORE_PROJECTS = [ 'dbt', 'dbt_bigquery', 'dbt_external_tables', 'dbt_utils', 'codegen' ] for element_type in [ 'nodes', 'sources', 'macros', 'parent_map', 'child_map' ] : # navigate into manifest # We transform to list to not change dict size during iteration , we use default value {} to handle KeyError for key in list ( json_manifest . get ( element_type , {} ). keys ()) : for ignore_project in IGNORE_PROJECTS : if re . match ( fr '^.*\\.{ignore_project}\\.' , key ) : # match with string that start with '*.<ignore_project>.' del json_manifest [ element_type ][ key ] # delete element with open ( catalog_path , 'r' ) as f : json_catalog = json . loads ( f . read ()) json_catalog = translate_catalog ( json_catalog ) # Write manifest & catalog jsons in index . html new_str = \"o=[{label: 'manifest', data: \" + json . dumps ( json_manifest ) + \"},{label: 'catalog', data: \" + json . dumps ( json_catalog ) + \"}]\" new_content = content_index . replace ( search_str , new_str ) # Select \"Database\" tab by default & Hide \"Project\" tab # new_content = new_content . replace ( '{e.nav_selected=\"project\"}' , '{e.nav_selected=\"database\"}' ) # new_content = new_content . replace ( '<div class=\"switch \">' , '<div class=\"switch \" hidden>' ) print ( 'Successfully populated index.html' ) return new_content run_dbt_debug def run_dbt_debug ( ) Run dbt debug View Source def run_dbt_debug () : \"\" \" Run `dbt debug ` \"\" \" ls_commands = [ \"dbt\" , \"debug\" ] print ( f 'Starting dbt debug ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt debug' ) else : print ( f 'Failed while trying to run dbt debug' ) run_dbt_deps def run_dbt_deps ( ) Run dbt debug View Source def run_dbt_deps () : \"\" \" Run `dbt debug ` \"\" \" ls_commands = [ \"dbt\" , \"deps\" ] print ( f 'Starting dbt deps ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt deps' ) else : print ( f 'Failed while trying to run dbt deps' ) run_subprocess def run_subprocess ( ls_commands , working_dir ) Run command provided as arg in path provided as arg Parameters: Name Type Description Default ls_commands list list of string representing the bash command to run None working_dir str path when you want to change directory to before execution None logger logging.logger (optional) for goblet app.log None View Source def run_subprocess ( ls_commands , working_dir ) : \"\" \"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\" \" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy () , encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \"{out}\\n{err} failed\" print ( msg ) return msg , False except Exception as e : raise e msg = out print ( msg ) return msg , True translate_catalog def translate_catalog ( catalog_data ) View Source def translate_catalog ( catalog_data ) : for element_type in [ 'nodes', 'sources' ] : # navigate into catalog for node in catalog_data [ element_type ] : if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_rows' ) : catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"label\" ] = MAPPING_DICT [ \"num_rows\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"description\" ] = MAPPING_DICT [ \"num_rows\" ][ \"description\" ] if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_bytes' ) : catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"label\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"description\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"description\" ] return catalog_data upload_populated_index def upload_populated_index ( content , file_name = 'index.html' ) View Source def upload_populated_index ( content , file_name = 'index.html' ): print ( 'Loading index.html to GCS ...' ) storage_client = storage . Client ( project = GCP_PROJECT ) bucket = storage_client . get_bucket ( DBT_DOCS_BUCKET ) blob = bucket . blob ( file_name ) blob . upload_from_string ( content , content_type = 'text/html' ) print ( 'Successfully loaded index.html to GCS' ) # Manage ACL acl = blob . acl acl . reload () acl . group ( DBT_DOCS_READ_GROUP ) . grant_read () acl . save () blob . acl . save ( acl = acl ) print ( 'Added ACL to index.html' )","title":"Dbt Catalog"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#module-brocolib_utilscatalog_gendbt_catalog","text":"View Source import json import os import re from google.cloud import storage import shlex import subprocess DBT_DOCS_BUCKET = os . environ . get ( 'DBT_DOCS_BUCKET' ) GCP_PROJECT = os . environ . get ( 'FRONT_PROJECT_ID' ) DBT_DOCS_READ_GROUP = os . environ . get ( 'DBT_DOCS_READ_GROUP' ) DBT_PROJECT_DIR = os . environ . get ( 'DBT_PATH' ) MAPPING_DICT = { \"num_rows\" : { \"id\" : \"num_rows\" , \"label\" : \"# Lignes\" , \"description\" : \"Nombre approximatif de lignes dans la table\" }, \"num_bytes\" : { \"id\" : \"num_bytes\" , \"label\" : \"Taille\" , \"description\" : \"Taille de la table (en bytes)\" } } def translate_catalog ( catalog_data ): for element_type in [ 'nodes' , 'sources' ]: # navigate into catalog for node in catalog_data [ element_type ]: if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_rows' ): catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"label\" ] = MAPPING_DICT [ \"num_rows\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"description\" ] = MAPPING_DICT [ \"num_rows\" ][ \"description\" ] if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_bytes' ): catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"label\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"description\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"description\" ] return catalog_data def get_dbt_populated_index ( target_folder ): print ( 'Populating index.html ...' ) base_index_path = os . path . join ( os . path . dirname ( __file__ ), 'base_index.html' ) manifest_path = os . path . join ( target_folder , 'manifest.json' ) catalog_path = os . path . join ( target_folder , 'catalog.json' ) search_str = 'o=[i(\"manifest\",\"manifest.json\"+t),i(\"catalog\",\"catalog.json\"+t)]' with open ( base_index_path , 'r' ) as f : content_index = f . read () with open ( manifest_path , 'r' ) as f : json_manifest = json . loads ( f . read ()) IGNORE_PROJECTS = [ 'dbt' , 'dbt_bigquery' , 'dbt_external_tables' , 'dbt_utils' , 'codegen' ] for element_type in [ 'nodes' , 'sources' , 'macros' , 'parent_map' , 'child_map' ]: # navigate into manifest # We transform to list to not change dict size during iteration, we use default value {} to handle KeyError for key in list ( json_manifest . get ( element_type , {}) . keys ()): for ignore_project in IGNORE_PROJECTS : if re . match ( fr '^.*\\. { ignore_project } \\.' , key ): # match with string that start with '*.<ignore_project>.' del json_manifest [ element_type ][ key ] # delete element with open ( catalog_path , 'r' ) as f : json_catalog = json . loads ( f . read ()) json_catalog = translate_catalog ( json_catalog ) # Write manifest & catalog jsons in index.html new_str = \"o=[{label: 'manifest', data: \" + json . dumps ( json_manifest ) + \"},{label: 'catalog', data: \" + json . dumps ( json_catalog ) + \"}]\" new_content = content_index . replace ( search_str , new_str ) # Select \"Database\" tab by default & Hide \"Project\" tab # new_content = new_content.replace('{e.nav_selected=\"project\"}', '{e.nav_selected=\"database\"}') # new_content = new_content.replace('<div class=\"switch \">', '<div class=\"switch \" hidden>') print ( 'Successfully populated index.html' ) return new_content def upload_populated_index ( content , file_name = 'index.html' ): print ( 'Loading index.html to GCS ...' ) storage_client = storage . Client ( project = GCP_PROJECT ) bucket = storage_client . get_bucket ( DBT_DOCS_BUCKET ) blob = bucket . blob ( file_name ) blob . upload_from_string ( content , content_type = 'text/html' ) print ( 'Successfully loaded index.html to GCS' ) # Manage ACL acl = blob . acl acl . reload () acl . group ( DBT_DOCS_READ_GROUP ) . grant_read () acl . save () blob . acl . save ( acl = acl ) print ( 'Added ACL to index.html' ) def run_subprocess ( ls_commands , working_dir ): \"\"\"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\"\" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy (), encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \" { out } \\n { err } failed\" print ( msg ) return msg , False except Exception as e : raise e msg = out print ( msg ) return msg , True def generate_dbt_docs (): \"\"\" Run `dbt docs generate` \"\"\" ls_commands = [ \"dbt\" , \"docs\" , \"generate\" ] print ( f 'Starting dbt docs generate ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt docs generate' ) else : print ( f 'Failed run dbt docs generate' ) def run_dbt_debug (): \"\"\" Run `dbt debug ` \"\"\" ls_commands = [ \"dbt\" , \"debug\" ] print ( f 'Starting dbt debug ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt debug' ) else : print ( f 'Failed while trying to run dbt debug' ) def run_dbt_deps (): \"\"\" Run `dbt debug ` \"\"\" ls_commands = [ \"dbt\" , \"deps\" ] print ( f 'Starting dbt deps ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt deps' ) else : print ( f 'Failed while trying to run dbt deps' )","title":"Module brocolib_utils.catalog_gen.dbt_catalog"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#variables","text":"DBT_DOCS_BUCKET DBT_DOCS_READ_GROUP DBT_PROJECT_DIR GCP_PROJECT MAPPING_DICT","title":"Variables"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#generate_dbt_docs","text":"def generate_dbt_docs ( ) Run dbt docs generate View Source def generate_dbt_docs () : \"\" \" Run `dbt docs generate` \"\" \" ls_commands = [ \"dbt\" , \"docs\" , \"generate\" ] print ( f 'Starting dbt docs generate ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt docs generate' ) else : print ( f 'Failed run dbt docs generate' )","title":"generate_dbt_docs"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#get_dbt_populated_index","text":"def get_dbt_populated_index ( target_folder ) View Source def get_dbt_populated_index ( target_folder ) : print ( 'Populating index.html ...' ) base_index_path = os . path . join ( os . path . dirname ( __file__ ), 'base_index.html' ) manifest_path = os . path . join ( target_folder , 'manifest.json' ) catalog_path = os . path . join ( target_folder , 'catalog.json' ) search_str = 'o=[i(\"manifest\",\"manifest.json\"+t),i(\"catalog\",\"catalog.json\"+t)]' with open ( base_index_path , 'r' ) as f : content_index = f . read () with open ( manifest_path , 'r' ) as f : json_manifest = json . loads ( f . read ()) IGNORE_PROJECTS = [ 'dbt', 'dbt_bigquery', 'dbt_external_tables', 'dbt_utils', 'codegen' ] for element_type in [ 'nodes', 'sources', 'macros', 'parent_map', 'child_map' ] : # navigate into manifest # We transform to list to not change dict size during iteration , we use default value {} to handle KeyError for key in list ( json_manifest . get ( element_type , {} ). keys ()) : for ignore_project in IGNORE_PROJECTS : if re . match ( fr '^.*\\.{ignore_project}\\.' , key ) : # match with string that start with '*.<ignore_project>.' del json_manifest [ element_type ][ key ] # delete element with open ( catalog_path , 'r' ) as f : json_catalog = json . loads ( f . read ()) json_catalog = translate_catalog ( json_catalog ) # Write manifest & catalog jsons in index . html new_str = \"o=[{label: 'manifest', data: \" + json . dumps ( json_manifest ) + \"},{label: 'catalog', data: \" + json . dumps ( json_catalog ) + \"}]\" new_content = content_index . replace ( search_str , new_str ) # Select \"Database\" tab by default & Hide \"Project\" tab # new_content = new_content . replace ( '{e.nav_selected=\"project\"}' , '{e.nav_selected=\"database\"}' ) # new_content = new_content . replace ( '<div class=\"switch \">' , '<div class=\"switch \" hidden>' ) print ( 'Successfully populated index.html' ) return new_content","title":"get_dbt_populated_index"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#run_dbt_debug","text":"def run_dbt_debug ( ) Run dbt debug View Source def run_dbt_debug () : \"\" \" Run `dbt debug ` \"\" \" ls_commands = [ \"dbt\" , \"debug\" ] print ( f 'Starting dbt debug ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt debug' ) else : print ( f 'Failed while trying to run dbt debug' )","title":"run_dbt_debug"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#run_dbt_deps","text":"def run_dbt_deps ( ) Run dbt debug View Source def run_dbt_deps () : \"\" \" Run `dbt debug ` \"\" \" ls_commands = [ \"dbt\" , \"deps\" ] print ( f 'Starting dbt deps ...' ) _ , dbt_run_ok = run_subprocess ( ls_commands , DBT_PROJECT_DIR ) if dbt_run_ok : print ( f 'Successfully run dbt deps' ) else : print ( f 'Failed while trying to run dbt deps' )","title":"run_dbt_deps"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#run_subprocess","text":"def run_subprocess ( ls_commands , working_dir ) Run command provided as arg in path provided as arg Parameters: Name Type Description Default ls_commands list list of string representing the bash command to run None working_dir str path when you want to change directory to before execution None logger logging.logger (optional) for goblet app.log None View Source def run_subprocess ( ls_commands , working_dir ) : \"\" \"Run command provided as arg in path provided as arg Args: ls_commands (list): list of string representing the bash command to run working_dir (str): path when you want to change directory to before execution logger (logging.logger): (optional) for goblet `app.log` \"\" \" out = \"\" err = \"\" try : process = subprocess . Popen ( ls_commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , cwd = working_dir , env = os . environ . copy () , encoding = 'utf-8' ) out , err = process . communicate () if process . returncode != 0 : msg = f \"{out}\\n{err} failed\" print ( msg ) return msg , False except Exception as e : raise e msg = out print ( msg ) return msg , True","title":"run_subprocess"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#translate_catalog","text":"def translate_catalog ( catalog_data ) View Source def translate_catalog ( catalog_data ) : for element_type in [ 'nodes', 'sources' ] : # navigate into catalog for node in catalog_data [ element_type ] : if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_rows' ) : catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"label\" ] = MAPPING_DICT [ \"num_rows\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_rows\" ][ \"description\" ] = MAPPING_DICT [ \"num_rows\" ][ \"description\" ] if catalog_data [ element_type ][ node ][ \"stats\" ] . get ( 'num_bytes' ) : catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"label\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"label\" ] catalog_data [ element_type ][ node ][ \"stats\" ][ \"num_bytes\" ][ \"description\" ] = MAPPING_DICT [ \"num_bytes\" ][ \"description\" ] return catalog_data","title":"translate_catalog"},{"location":"reference/brocolib_utils/catalog_gen/dbt_catalog/#upload_populated_index","text":"def upload_populated_index ( content , file_name = 'index.html' ) View Source def upload_populated_index ( content , file_name = 'index.html' ): print ( 'Loading index.html to GCS ...' ) storage_client = storage . Client ( project = GCP_PROJECT ) bucket = storage_client . get_bucket ( DBT_DOCS_BUCKET ) blob = bucket . blob ( file_name ) blob . upload_from_string ( content , content_type = 'text/html' ) print ( 'Successfully loaded index.html to GCS' ) # Manage ACL acl = blob . acl acl . reload () acl . group ( DBT_DOCS_READ_GROUP ) . grant_read () acl . save () blob . acl . save ( acl = acl ) print ( 'Added ACL to index.html' )","title":"upload_populated_index"},{"location":"reference/brocolib_utils/datalake/","text":"Module brocolib_utils.datalake View Source from .datalake import * Sub-modules brocolib_utils.datalake.datalake brocolib_utils.datalake.gcs_utils","title":"Index"},{"location":"reference/brocolib_utils/datalake/#module-brocolib_utilsdatalake","text":"View Source from .datalake import *","title":"Module brocolib_utils.datalake"},{"location":"reference/brocolib_utils/datalake/#sub-modules","text":"brocolib_utils.datalake.datalake brocolib_utils.datalake.gcs_utils","title":"Sub-modules"},{"location":"reference/brocolib_utils/datalake/datalake/","text":"Module brocolib_utils.datalake.datalake View Source import re import pandas as pd from google.cloud import storage from pandas_gbq.schema import generate_bq_schema from brocolib_utils import credentials , settings from brocolib_utils.datalake import gcs_utils def get_storage_client ( gcp_project ): return storage . Client ( project = gcp_project ) def get_gsutil_uri ( bucket : str , blob : str ) -> str : \"\"\"Construct a gsutil uri Args: bucket (str): Google Cloud Storage bucket name blob (str): Suffix of the gsutil uri Returns: str: The gsutil uri \"\"\" return f \"gs:// { bucket } / { blob } \" def add_to_source_list ( client : storage . client . Client , datalake_bucket : str , prefix : str = \"\" , sources_dict : dict = {} ): \"\"\"_summary_ Args: client (storage.client.Client): Storage client datalake_bucket (str): Google Cloud Storage bucket name prefix (str, optional): gsutil uri prefix. Defaults to \"\". sources_dict (dict, optional): dict containing the dbt sources. Defaults to {}. Returns: _type_: _description_ \"\"\" result = client . list_blobs ( bucket_or_name = datalake_bucket , prefix = prefix , delimiter = '/' ) FILE_EXTENSION_PATTERN = r \"[\\d\\D]*\\.parquet\" for element in result : if re . match ( FILE_EXTENSION_PATTERN , element . name . replace ( prefix , \"\" )): table_name = prefix . split ( '/' )[ - 2 ] sources_dict [ table_name ] = prefix return sources_dict pass HIVE_PARTITIONING_PATTERN = r \"[\\d\\D]*\\=[\\d\\D]*\" for element in result . prefixes : if re . match ( HIVE_PARTITIONING_PATTERN , element . replace ( prefix , \"\" )): table_name = prefix . split ( '/' )[ - 2 ] sources_dict [ table_name ] = prefix else : add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = element , sources_dict = sources_dict ) return sources_dict def get_raw_source ( gcp_project : str , datalake_bucket : str , source_name : str ) -> dict : \"\"\"_summary_ Args: gcp_project (str): _description_ datalake_bucket (str): _description_ first_partition_key (str): _description_ Returns: dict: _description_ \"\"\" client = gcs_utils . get_storage_client ( gcp_project ) source_dict = add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = source_name ) return source_dict def get_source ( source_name : str , gcp_project : str = None , datalake_bucket : str = None ): gcp_project = gcp_project or settings . DATALAKE_PROJECT datalake_bucket = datalake_bucket or settings . DATALAKE_BUCKET raw_source_dict = get_raw_source ( gcp_project = gcp_project , datalake_bucket = datalake_bucket , source_name = f \" { source_name } /\" ) source_dict = { table : get_gsutil_uri ( datalake_bucket , blob ) for table , blob in raw_source_dict . items () } return source_dict def generate_bigquery_schema_from_df ( df : pd . DataFrame ) -> list : raw_schema = generate_bq_schema ( df ) return raw_schema [ \"fields\" ] Functions add_to_source_list def add_to_source_list ( client : google . cloud . storage . client . Client , datalake_bucket : str , prefix : str = '' , sources_dict : dict = {} ) summary Parameters: Name Type Description Default client storage.client.Client Storage client None datalake_bucket str Google Cloud Storage bucket name None prefix str gsutil uri prefix. Defaults to \"\". \"\" sources_dict dict dict containing the dbt sources. Defaults to {}. {} Returns: Type Description type description View Source def add_to_source_list ( client : storage . client . Client , datalake_bucket : str , prefix : str = \"\" , sources_dict : dict = {} ) : \"\"\"_summary_ Args: client (storage.client.Client): Storage client datalake_bucket (str): Google Cloud Storage bucket name prefix (str, optional): gsutil uri prefix. Defaults to \"\". sources_dict (dict, optional): dict containing the dbt sources. Defaults to {}. Returns: _type_: _description_ \"\"\" result = client . list_blobs ( bucket_or_name = datalake_bucket , prefix = prefix , delimiter = '/' ) FILE_EXTENSION_PATTERN = r \"[\\d\\D]*\\.parquet\" for element in result : if re . match ( FILE_EXTENSION_PATTERN , element . name . replace ( prefix , \"\" )) : table_name = prefix . split ( '/' ) [ -2 ] sources_dict [ table_name ] = prefix return sources_dict pass HIVE_PARTITIONING_PATTERN = r \"[\\d\\D]*\\=[\\d\\D]*\" for element in result . prefixes : if re . match ( HIVE_PARTITIONING_PATTERN , element . replace ( prefix , \"\" )) : table_name = prefix . split ( '/' ) [ -2 ] sources_dict [ table_name ] = prefix else : add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = element , sources_dict = sources_dict ) return sources_dict generate_bigquery_schema_from_df def generate_bigquery_schema_from_df ( df : pandas . core . frame . DataFrame ) -> list View Source def generate_bigquery_schema_from_df ( df : pd . DataFrame ) -> list : raw_schema = generate_bq_schema ( df ) return raw_schema [ \"fields\" ] get_gsutil_uri def get_gsutil_uri ( bucket : str , blob : str ) -> str Construct a gsutil uri Parameters: Name Type Description Default bucket str Google Cloud Storage bucket name None blob str Suffix of the gsutil uri None Returns: Type Description str The gsutil uri View Source def get_gsutil_uri ( bucket : str , blob : str ) -> str : \"\"\"Construct a gsutil uri Args: bucket (str): Google Cloud Storage bucket name blob (str): Suffix of the gsutil uri Returns: str: The gsutil uri \"\"\" return f \"gs://{bucket}/{blob}\" get_raw_source def get_raw_source ( gcp_project : str , datalake_bucket : str , source_name : str ) -> dict summary Parameters: Name Type Description Default gcp_project str description None datalake_bucket str description None first_partition_key str description None Returns: Type Description dict description View Source def get_raw_source ( gcp_project : str , datalake_bucket : str , source_name : str ) -> dict : \"\"\"_summary_ Args: gcp_project (str): _description_ datalake_bucket (str): _description_ first_partition_key (str): _description_ Returns: dict: _description_ \"\"\" client = gcs_utils . get_storage_client ( gcp_project ) source_dict = add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = source_name ) return source_dict get_source def get_source ( source_name : str , gcp_project : str = None , datalake_bucket : str = None ) View Source def get_source ( source_name : str , gcp_project : str = None , datalake_bucket : str = None ) : gcp_project = gcp_project or settings . DATALAKE_PROJECT datalake_bucket = datalake_bucket or settings . DATALAKE_BUCKET raw_source_dict = get_raw_source ( gcp_project = gcp_project , datalake_bucket = datalake_bucket , source_name = f \"{source_name}/\" ) source_dict = { table : get_gsutil_uri ( datalake_bucket , blob ) for table , blob in raw_source_dict . items () } return source_dict get_storage_client def get_storage_client ( gcp_project ) View Source def get_storage_client ( gcp_project ) : return storage . Client ( project = gcp_project )","title":"Datalake"},{"location":"reference/brocolib_utils/datalake/datalake/#module-brocolib_utilsdatalakedatalake","text":"View Source import re import pandas as pd from google.cloud import storage from pandas_gbq.schema import generate_bq_schema from brocolib_utils import credentials , settings from brocolib_utils.datalake import gcs_utils def get_storage_client ( gcp_project ): return storage . Client ( project = gcp_project ) def get_gsutil_uri ( bucket : str , blob : str ) -> str : \"\"\"Construct a gsutil uri Args: bucket (str): Google Cloud Storage bucket name blob (str): Suffix of the gsutil uri Returns: str: The gsutil uri \"\"\" return f \"gs:// { bucket } / { blob } \" def add_to_source_list ( client : storage . client . Client , datalake_bucket : str , prefix : str = \"\" , sources_dict : dict = {} ): \"\"\"_summary_ Args: client (storage.client.Client): Storage client datalake_bucket (str): Google Cloud Storage bucket name prefix (str, optional): gsutil uri prefix. Defaults to \"\". sources_dict (dict, optional): dict containing the dbt sources. Defaults to {}. Returns: _type_: _description_ \"\"\" result = client . list_blobs ( bucket_or_name = datalake_bucket , prefix = prefix , delimiter = '/' ) FILE_EXTENSION_PATTERN = r \"[\\d\\D]*\\.parquet\" for element in result : if re . match ( FILE_EXTENSION_PATTERN , element . name . replace ( prefix , \"\" )): table_name = prefix . split ( '/' )[ - 2 ] sources_dict [ table_name ] = prefix return sources_dict pass HIVE_PARTITIONING_PATTERN = r \"[\\d\\D]*\\=[\\d\\D]*\" for element in result . prefixes : if re . match ( HIVE_PARTITIONING_PATTERN , element . replace ( prefix , \"\" )): table_name = prefix . split ( '/' )[ - 2 ] sources_dict [ table_name ] = prefix else : add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = element , sources_dict = sources_dict ) return sources_dict def get_raw_source ( gcp_project : str , datalake_bucket : str , source_name : str ) -> dict : \"\"\"_summary_ Args: gcp_project (str): _description_ datalake_bucket (str): _description_ first_partition_key (str): _description_ Returns: dict: _description_ \"\"\" client = gcs_utils . get_storage_client ( gcp_project ) source_dict = add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = source_name ) return source_dict def get_source ( source_name : str , gcp_project : str = None , datalake_bucket : str = None ): gcp_project = gcp_project or settings . DATALAKE_PROJECT datalake_bucket = datalake_bucket or settings . DATALAKE_BUCKET raw_source_dict = get_raw_source ( gcp_project = gcp_project , datalake_bucket = datalake_bucket , source_name = f \" { source_name } /\" ) source_dict = { table : get_gsutil_uri ( datalake_bucket , blob ) for table , blob in raw_source_dict . items () } return source_dict def generate_bigquery_schema_from_df ( df : pd . DataFrame ) -> list : raw_schema = generate_bq_schema ( df ) return raw_schema [ \"fields\" ]","title":"Module brocolib_utils.datalake.datalake"},{"location":"reference/brocolib_utils/datalake/datalake/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/datalake/datalake/#add_to_source_list","text":"def add_to_source_list ( client : google . cloud . storage . client . Client , datalake_bucket : str , prefix : str = '' , sources_dict : dict = {} ) summary Parameters: Name Type Description Default client storage.client.Client Storage client None datalake_bucket str Google Cloud Storage bucket name None prefix str gsutil uri prefix. Defaults to \"\". \"\" sources_dict dict dict containing the dbt sources. Defaults to {}. {} Returns: Type Description type description View Source def add_to_source_list ( client : storage . client . Client , datalake_bucket : str , prefix : str = \"\" , sources_dict : dict = {} ) : \"\"\"_summary_ Args: client (storage.client.Client): Storage client datalake_bucket (str): Google Cloud Storage bucket name prefix (str, optional): gsutil uri prefix. Defaults to \"\". sources_dict (dict, optional): dict containing the dbt sources. Defaults to {}. Returns: _type_: _description_ \"\"\" result = client . list_blobs ( bucket_or_name = datalake_bucket , prefix = prefix , delimiter = '/' ) FILE_EXTENSION_PATTERN = r \"[\\d\\D]*\\.parquet\" for element in result : if re . match ( FILE_EXTENSION_PATTERN , element . name . replace ( prefix , \"\" )) : table_name = prefix . split ( '/' ) [ -2 ] sources_dict [ table_name ] = prefix return sources_dict pass HIVE_PARTITIONING_PATTERN = r \"[\\d\\D]*\\=[\\d\\D]*\" for element in result . prefixes : if re . match ( HIVE_PARTITIONING_PATTERN , element . replace ( prefix , \"\" )) : table_name = prefix . split ( '/' ) [ -2 ] sources_dict [ table_name ] = prefix else : add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = element , sources_dict = sources_dict ) return sources_dict","title":"add_to_source_list"},{"location":"reference/brocolib_utils/datalake/datalake/#generate_bigquery_schema_from_df","text":"def generate_bigquery_schema_from_df ( df : pandas . core . frame . DataFrame ) -> list View Source def generate_bigquery_schema_from_df ( df : pd . DataFrame ) -> list : raw_schema = generate_bq_schema ( df ) return raw_schema [ \"fields\" ]","title":"generate_bigquery_schema_from_df"},{"location":"reference/brocolib_utils/datalake/datalake/#get_gsutil_uri","text":"def get_gsutil_uri ( bucket : str , blob : str ) -> str Construct a gsutil uri Parameters: Name Type Description Default bucket str Google Cloud Storage bucket name None blob str Suffix of the gsutil uri None Returns: Type Description str The gsutil uri View Source def get_gsutil_uri ( bucket : str , blob : str ) -> str : \"\"\"Construct a gsutil uri Args: bucket (str): Google Cloud Storage bucket name blob (str): Suffix of the gsutil uri Returns: str: The gsutil uri \"\"\" return f \"gs://{bucket}/{blob}\"","title":"get_gsutil_uri"},{"location":"reference/brocolib_utils/datalake/datalake/#get_raw_source","text":"def get_raw_source ( gcp_project : str , datalake_bucket : str , source_name : str ) -> dict summary Parameters: Name Type Description Default gcp_project str description None datalake_bucket str description None first_partition_key str description None Returns: Type Description dict description View Source def get_raw_source ( gcp_project : str , datalake_bucket : str , source_name : str ) -> dict : \"\"\"_summary_ Args: gcp_project (str): _description_ datalake_bucket (str): _description_ first_partition_key (str): _description_ Returns: dict: _description_ \"\"\" client = gcs_utils . get_storage_client ( gcp_project ) source_dict = add_to_source_list ( client = client , datalake_bucket = datalake_bucket , prefix = source_name ) return source_dict","title":"get_raw_source"},{"location":"reference/brocolib_utils/datalake/datalake/#get_source","text":"def get_source ( source_name : str , gcp_project : str = None , datalake_bucket : str = None ) View Source def get_source ( source_name : str , gcp_project : str = None , datalake_bucket : str = None ) : gcp_project = gcp_project or settings . DATALAKE_PROJECT datalake_bucket = datalake_bucket or settings . DATALAKE_BUCKET raw_source_dict = get_raw_source ( gcp_project = gcp_project , datalake_bucket = datalake_bucket , source_name = f \"{source_name}/\" ) source_dict = { table : get_gsutil_uri ( datalake_bucket , blob ) for table , blob in raw_source_dict . items () } return source_dict","title":"get_source"},{"location":"reference/brocolib_utils/datalake/datalake/#get_storage_client","text":"def get_storage_client ( gcp_project ) View Source def get_storage_client ( gcp_project ) : return storage . Client ( project = gcp_project )","title":"get_storage_client"},{"location":"reference/brocolib_utils/datalake/gcs_utils/","text":"Module brocolib_utils.datalake.gcs_utils View Source from google.cloud import storage from brocolib_utils import settings from typing import Literal STORAGE_CLASSES = Literal [ \"STANDARD\" , \"NEARLINE\" , \"COLDLINE\" , \"ARCHIVE\" ] DEFAULT_STORAGE_CLASS = \"STANDARD\" def get_storage_client ( project : str = None ): project = project or settings . DATALAKE_PROJECT return storage . Client ( project = project ) def create_bucket ( bucket_name : str , bucket_location : str = None , project : str = None , storage_class : STORAGE_CLASSES = None ): storage_client = get_storage_client ( project = project ) bucket = storage_client . bucket ( bucket_name ) bucket_exists = bucket . exists () if not bucket_exists : bucket . storage_class = storage_class or DEFAULT_STORAGE_CLASS bucket = storage_client . create_bucket ( bucket , location = bucket_location ) return bucket , bucket_exists def delete_bucket ( bucket_name : str , project : str = None ): storage_client = get_storage_client ( project ) bucket = storage_client . get_bucket ( bucket_name ) bucket . delete () Variables DEFAULT_STORAGE_CLASS STORAGE_CLASSES Functions create_bucket def create_bucket ( bucket_name : str , bucket_location : str = None , project : str = None , storage_class : Literal [ 'STANDARD' , 'NEARLINE' , 'COLDLINE' , 'ARCHIVE' ] = None ) View Source def create_bucket ( bucket_name : str , bucket_location : str = None , project : str = None , storage_class : STORAGE_CLASSES = None ) : storage_client = get_storage_client ( project = project ) bucket = storage_client . bucket ( bucket_name ) bucket_exists = bucket . exists () if not bucket_exists : bucket . storage_class = storage_class or DEFAULT_STORAGE_CLASS bucket = storage_client . create_bucket ( bucket , location = bucket_location ) return bucket , bucket_exists delete_bucket def delete_bucket ( bucket_name : str , project : str = None ) View Source def delete_bucket(bucket_name:str, project:str = None): storage_client = get_storage_client(project) bucket = storage_client.get_bucket(bucket_name) bucket.delete() get_storage_client def get_storage_client ( project : str = None ) View Source def get_storage_client ( project : str = None ) : project = project or settings . DATALAKE_PROJECT return storage . Client ( project = project )","title":"Gcs Utils"},{"location":"reference/brocolib_utils/datalake/gcs_utils/#module-brocolib_utilsdatalakegcs_utils","text":"View Source from google.cloud import storage from brocolib_utils import settings from typing import Literal STORAGE_CLASSES = Literal [ \"STANDARD\" , \"NEARLINE\" , \"COLDLINE\" , \"ARCHIVE\" ] DEFAULT_STORAGE_CLASS = \"STANDARD\" def get_storage_client ( project : str = None ): project = project or settings . DATALAKE_PROJECT return storage . Client ( project = project ) def create_bucket ( bucket_name : str , bucket_location : str = None , project : str = None , storage_class : STORAGE_CLASSES = None ): storage_client = get_storage_client ( project = project ) bucket = storage_client . bucket ( bucket_name ) bucket_exists = bucket . exists () if not bucket_exists : bucket . storage_class = storage_class or DEFAULT_STORAGE_CLASS bucket = storage_client . create_bucket ( bucket , location = bucket_location ) return bucket , bucket_exists def delete_bucket ( bucket_name : str , project : str = None ): storage_client = get_storage_client ( project ) bucket = storage_client . get_bucket ( bucket_name ) bucket . delete ()","title":"Module brocolib_utils.datalake.gcs_utils"},{"location":"reference/brocolib_utils/datalake/gcs_utils/#variables","text":"DEFAULT_STORAGE_CLASS STORAGE_CLASSES","title":"Variables"},{"location":"reference/brocolib_utils/datalake/gcs_utils/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/datalake/gcs_utils/#create_bucket","text":"def create_bucket ( bucket_name : str , bucket_location : str = None , project : str = None , storage_class : Literal [ 'STANDARD' , 'NEARLINE' , 'COLDLINE' , 'ARCHIVE' ] = None ) View Source def create_bucket ( bucket_name : str , bucket_location : str = None , project : str = None , storage_class : STORAGE_CLASSES = None ) : storage_client = get_storage_client ( project = project ) bucket = storage_client . bucket ( bucket_name ) bucket_exists = bucket . exists () if not bucket_exists : bucket . storage_class = storage_class or DEFAULT_STORAGE_CLASS bucket = storage_client . create_bucket ( bucket , location = bucket_location ) return bucket , bucket_exists","title":"create_bucket"},{"location":"reference/brocolib_utils/datalake/gcs_utils/#delete_bucket","text":"def delete_bucket ( bucket_name : str , project : str = None ) View Source def delete_bucket(bucket_name:str, project:str = None): storage_client = get_storage_client(project) bucket = storage_client.get_bucket(bucket_name) bucket.delete()","title":"delete_bucket"},{"location":"reference/brocolib_utils/datalake/gcs_utils/#get_storage_client","text":"def get_storage_client ( project : str = None ) View Source def get_storage_client ( project : str = None ) : project = project or settings . DATALAKE_PROJECT return storage . Client ( project = project )","title":"get_storage_client"},{"location":"reference/brocolib_utils/drive/","text":"Module brocolib_utils.drive Sub-modules brocolib_utils.drive.sheets","title":"Index"},{"location":"reference/brocolib_utils/drive/#module-brocolib_utilsdrive","text":"","title":"Module brocolib_utils.drive"},{"location":"reference/brocolib_utils/drive/#sub-modules","text":"brocolib_utils.drive.sheets","title":"Sub-modules"},{"location":"reference/brocolib_utils/drive/sheets/","text":"Module brocolib_utils.drive.sheets View Source # importing the required libraries import os from functools import lru_cache import gspread import gspread_pandas import pandas as pd from brocolib_utils import credentials from brocolib_utils import settings def get_sheets_credentials (): return gspread_pandas . conf . get_creds ( config = credentials . get_credential_file_asdict () ) # return credentials.get_creds(settings.GOOGLE_SHEETS_API_SCOPES) # @lru_cache(maxsize=1) # def get_google_sheet(sheet_id:str) -> gspread.Spreadsheet: # creds = get_sheets_credentials() # client = gspread.authorize(creds) # google_sheet = client.open_by_key(sheet_id) # return google_sheet # @lru_cache(maxsize=1) def get_google_sheet ( sheet_id : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () # client = gspread.authorize(creds) google_sheet = gspread_pandas . Spread ( spread = sheet_id , creds = creds ) # google_sheet = client.open_by_key(sheet_id) return google_sheet def get_google_worksheet ( sheet_id : str , worksheet_name : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () google_worksheet = gspread_pandas . Spread ( spread = sheet_id , sheet = worksheet_name , creds = creds ) return google_worksheet def clean_columns_name ( dataframe ): dc_rename = { col : col . replace ( ' ' , '_' ) for col in dataframe . columns } dataframe = dataframe . rename ( columns = dc_rename ) return dataframe def explode_sources ( dataframe ): dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ] . str . replace ( \" \" , \"\" ) dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ] . str . split ( ',' ) return dataframe . explode ( \"model_source\" ) def get_sheet_title ( sheet_url ): creds = get_sheets_credentials () # authorize the clientsheet client = gspread . authorize ( creds ) sheet = client . open_by_url ( sheet_url ) return sheet . title . replace ( ' ' , '_' ) . lower () Functions clean_columns_name def clean_columns_name ( dataframe ) View Source def clean_columns_name ( dataframe ) : dc_rename = { col : col . replace ( ' ' , '_' ) for col in dataframe . columns } dataframe = dataframe . rename ( columns = dc_rename ) return dataframe explode_sources def explode_sources ( dataframe ) View Source def explode_sources ( dataframe ) : dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ]. str . replace ( \" \" , \"\" ) dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ]. str . split ( ',' ) return dataframe . explode ( \"model_source\" ) get_google_sheet def get_google_sheet ( sheet_id : str ) -> gspread_pandas . spread . Spread View Source def get_google_sheet ( sheet_id : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () # client = gspread . authorize ( creds ) google_sheet = gspread_pandas . Spread ( spread = sheet_id , creds = creds ) # google_sheet = client . open_by_key ( sheet_id ) return google_sheet get_google_worksheet def get_google_worksheet ( sheet_id : str , worksheet_name : str ) -> gspread_pandas . spread . Spread View Source def get_google_worksheet ( sheet_id : str , worksheet_name : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () google_worksheet = gspread_pandas . Spread ( spread = sheet_id , sheet = worksheet_name , creds = creds ) return google_worksheet get_sheet_title def get_sheet_title ( sheet_url ) View Source def get_sheet_title ( sheet_url ) : creds = get_sheets_credentials () # authorize the clientsheet client = gspread . authorize ( creds ) sheet = client . open_by_url ( sheet_url ) return sheet . title . replace ( ' ' , '_' ) . lower () get_sheets_credentials def get_sheets_credentials ( ) View Source def get_sheets_credentials () : return gspread_pandas . conf . get_creds ( config = credentials . get_credential_file_asdict () ) # return credentials . get_creds ( settings . GOOGLE_SHEETS_API_SCOPES )","title":"Sheets"},{"location":"reference/brocolib_utils/drive/sheets/#module-brocolib_utilsdrivesheets","text":"View Source # importing the required libraries import os from functools import lru_cache import gspread import gspread_pandas import pandas as pd from brocolib_utils import credentials from brocolib_utils import settings def get_sheets_credentials (): return gspread_pandas . conf . get_creds ( config = credentials . get_credential_file_asdict () ) # return credentials.get_creds(settings.GOOGLE_SHEETS_API_SCOPES) # @lru_cache(maxsize=1) # def get_google_sheet(sheet_id:str) -> gspread.Spreadsheet: # creds = get_sheets_credentials() # client = gspread.authorize(creds) # google_sheet = client.open_by_key(sheet_id) # return google_sheet # @lru_cache(maxsize=1) def get_google_sheet ( sheet_id : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () # client = gspread.authorize(creds) google_sheet = gspread_pandas . Spread ( spread = sheet_id , creds = creds ) # google_sheet = client.open_by_key(sheet_id) return google_sheet def get_google_worksheet ( sheet_id : str , worksheet_name : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () google_worksheet = gspread_pandas . Spread ( spread = sheet_id , sheet = worksheet_name , creds = creds ) return google_worksheet def clean_columns_name ( dataframe ): dc_rename = { col : col . replace ( ' ' , '_' ) for col in dataframe . columns } dataframe = dataframe . rename ( columns = dc_rename ) return dataframe def explode_sources ( dataframe ): dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ] . str . replace ( \" \" , \"\" ) dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ] . str . split ( ',' ) return dataframe . explode ( \"model_source\" ) def get_sheet_title ( sheet_url ): creds = get_sheets_credentials () # authorize the clientsheet client = gspread . authorize ( creds ) sheet = client . open_by_url ( sheet_url ) return sheet . title . replace ( ' ' , '_' ) . lower ()","title":"Module brocolib_utils.drive.sheets"},{"location":"reference/brocolib_utils/drive/sheets/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/drive/sheets/#clean_columns_name","text":"def clean_columns_name ( dataframe ) View Source def clean_columns_name ( dataframe ) : dc_rename = { col : col . replace ( ' ' , '_' ) for col in dataframe . columns } dataframe = dataframe . rename ( columns = dc_rename ) return dataframe","title":"clean_columns_name"},{"location":"reference/brocolib_utils/drive/sheets/#explode_sources","text":"def explode_sources ( dataframe ) View Source def explode_sources ( dataframe ) : dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ]. str . replace ( \" \" , \"\" ) dataframe [ \"model_source\" ] = dataframe [ \"model_source\" ]. str . split ( ',' ) return dataframe . explode ( \"model_source\" )","title":"explode_sources"},{"location":"reference/brocolib_utils/drive/sheets/#get_google_sheet","text":"def get_google_sheet ( sheet_id : str ) -> gspread_pandas . spread . Spread View Source def get_google_sheet ( sheet_id : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () # client = gspread . authorize ( creds ) google_sheet = gspread_pandas . Spread ( spread = sheet_id , creds = creds ) # google_sheet = client . open_by_key ( sheet_id ) return google_sheet","title":"get_google_sheet"},{"location":"reference/brocolib_utils/drive/sheets/#get_google_worksheet","text":"def get_google_worksheet ( sheet_id : str , worksheet_name : str ) -> gspread_pandas . spread . Spread View Source def get_google_worksheet ( sheet_id : str , worksheet_name : str ) -> gspread_pandas . Spread : creds = get_sheets_credentials () google_worksheet = gspread_pandas . Spread ( spread = sheet_id , sheet = worksheet_name , creds = creds ) return google_worksheet","title":"get_google_worksheet"},{"location":"reference/brocolib_utils/drive/sheets/#get_sheet_title","text":"def get_sheet_title ( sheet_url ) View Source def get_sheet_title ( sheet_url ) : creds = get_sheets_credentials () # authorize the clientsheet client = gspread . authorize ( creds ) sheet = client . open_by_url ( sheet_url ) return sheet . title . replace ( ' ' , '_' ) . lower ()","title":"get_sheet_title"},{"location":"reference/brocolib_utils/drive/sheets/#get_sheets_credentials","text":"def get_sheets_credentials ( ) View Source def get_sheets_credentials () : return gspread_pandas . conf . get_creds ( config = credentials . get_credential_file_asdict () ) # return credentials . get_creds ( settings . GOOGLE_SHEETS_API_SCOPES )","title":"get_sheets_credentials"},{"location":"reference/brocolib_utils/fast_dbt/","text":"Module brocolib_utils.fast_dbt View Source if __name__ == \"__main__\" : from cli import main main () Sub-modules brocolib_utils.fast_dbt.cli brocolib_utils.fast_dbt.dbt_parser brocolib_utils.fast_dbt.generator brocolib_utils.fast_dbt.new_generator","title":"Index"},{"location":"reference/brocolib_utils/fast_dbt/#module-brocolib_utilsfast_dbt","text":"View Source if __name__ == \"__main__\" : from cli import main main ()","title":"Module brocolib_utils.fast_dbt"},{"location":"reference/brocolib_utils/fast_dbt/#sub-modules","text":"brocolib_utils.fast_dbt.cli brocolib_utils.fast_dbt.dbt_parser brocolib_utils.fast_dbt.generator brocolib_utils.fast_dbt.new_generator","title":"Sub-modules"},{"location":"reference/brocolib_utils/fast_dbt/cli/","text":"Module brocolib_utils.fast_dbt.cli View Source # import os # import json # import argparse # from ruamel.yaml import YAML # from dbt_parser import update_exposures # from ruamel.yaml import YAML # from dbt_parser import update_exposures # from brocolib_utils.settings import DBT_MODELS_PATH # from brocolib_utils.fast_dbt.generator import (init_dbt_sources, # generate_loaded_tables_specs, dict_to_yaml) # from brocolib_utils.datalake import get_sources # from collections import OrderedDict # DATALAKE_BUCKET = os.environ.get('DATALAKE_BUCKET') # GCP_PROJECT = os.environ.get('BACK_PROJECT_ID') # DEFAULT_GCS_PARTITIONNING_KEYS = json.loads(os.environ.get('DEFAULT_GCS_PARTITIONNING_KEYS')) # def main(): # # Create the parser # fast_dbt_parser = argparse.ArgumentParser(description='fast_dbt Python CLI ') # command_parser = fast_dbt_parser.add_subparsers(dest='command') # ## Exposures # exposure_parser = command_parser.add_parser('exposure', help='Manage exposures') # exposure_subcommand_parser = exposure_parser.add_subparsers(dest='exposure_subcommand') # update_permissions_parser = exposure_subcommand_parser.add_parser('update_permissions', help='Parse dbt exposure .yml file and update data studio permissions') # update_permissions_parser.add_argument( # 'file-path', # dest='exposure_file_path', # help='path to the dbt exposures .yml file' # ) # args = fast_dbt_parser.parse_args() # if args.command == \"exposure\": # if args.exposure_subcommand = \"update_permissions\" # yaml_parser = YAML() # with open(args.exposure_file_path, 'r') as f: # data = yaml_parser.load(f) # update_exposures(data)","title":"CLI"},{"location":"reference/brocolib_utils/fast_dbt/cli/#module-brocolib_utilsfast_dbtcli","text":"View Source # import os # import json # import argparse # from ruamel.yaml import YAML # from dbt_parser import update_exposures # from ruamel.yaml import YAML # from dbt_parser import update_exposures # from brocolib_utils.settings import DBT_MODELS_PATH # from brocolib_utils.fast_dbt.generator import (init_dbt_sources, # generate_loaded_tables_specs, dict_to_yaml) # from brocolib_utils.datalake import get_sources # from collections import OrderedDict # DATALAKE_BUCKET = os.environ.get('DATALAKE_BUCKET') # GCP_PROJECT = os.environ.get('BACK_PROJECT_ID') # DEFAULT_GCS_PARTITIONNING_KEYS = json.loads(os.environ.get('DEFAULT_GCS_PARTITIONNING_KEYS')) # def main(): # # Create the parser # fast_dbt_parser = argparse.ArgumentParser(description='fast_dbt Python CLI ') # command_parser = fast_dbt_parser.add_subparsers(dest='command') # ## Exposures # exposure_parser = command_parser.add_parser('exposure', help='Manage exposures') # exposure_subcommand_parser = exposure_parser.add_subparsers(dest='exposure_subcommand') # update_permissions_parser = exposure_subcommand_parser.add_parser('update_permissions', help='Parse dbt exposure .yml file and update data studio permissions') # update_permissions_parser.add_argument( # 'file-path', # dest='exposure_file_path', # help='path to the dbt exposures .yml file' # ) # args = fast_dbt_parser.parse_args() # if args.command == \"exposure\": # if args.exposure_subcommand = \"update_permissions\" # yaml_parser = YAML() # with open(args.exposure_file_path, 'r') as f: # data = yaml_parser.load(f) # update_exposures(data)","title":"Module brocolib_utils.fast_dbt.cli"},{"location":"reference/brocolib_utils/fast_dbt/dbt_parser/","text":"Module brocolib_utils.fast_dbt.dbt_parser View Source import logging from brocolib_utils.data_studio import DataStudio def update_exposures ( data : dict ): for element in data [ \"exposures\" ]: exposure_title = element [ \"name\" ] exposure_meta_dict = element [ \"meta\" ] asset_id = exposure_meta_dict . get ( \"asset_id\" ) grants_dict = exposure_meta_dict . get ( \"grants\" ) if grants_dict : client = DataStudio () permissions = client . get_permissions ( asset_id = asset_id ) if permissions [ \"response\" ] != 200 : logging . error ( permissions [ \"response\" ]) elif permissions [ \"response\" ][ \"permissions\" ] != grants_dict : permissions_updated = client . update_permissions ( asset_id = asset_id , permission_dict = { \"permissions\" : grants_dict } ) if permissions_updated [ \"response\" ] != 200 : logging . error ( permissions_updated [ \"response\" ]) else : logging . info ( f \"Permissions for { exposure_title } have been updated successfully !\" ) else : logging . info ( f \"No changes in permissions detected for { exposure_title } \" ) logging . info ( 'DONE' ) Functions update_exposures def update_exposures ( data : dict ) View Source def update_exposures ( data : dict ) : for element in data [ \"exposures\" ]: exposure_title = element [ \"name\" ] exposure_meta_dict = element [ \"meta\" ] asset_id = exposure_meta_dict . get ( \"asset_id\" ) grants_dict = exposure_meta_dict . get ( \"grants\" ) if grants_dict : client = DataStudio () permissions = client . get_permissions ( asset_id = asset_id ) if permissions [ \"response\" ] != 200 : logging . error ( permissions [ \"response\" ] ) elif permissions [ \"response\" ][ \"permissions\" ] != grants_dict : permissions_updated = client . update_permissions ( asset_id = asset_id , permission_dict = { \"permissions\" : grants_dict } ) if permissions_updated [ \"response\" ] != 200 : logging . error ( permissions_updated [ \"response\" ] ) else : logging . info ( f \"Permissions for {exposure_title} have been updated successfully !\" ) else : logging . info ( f \"No changes in permissions detected for {exposure_title}\" ) logging . info ( 'DONE' )","title":"Dbt Parser"},{"location":"reference/brocolib_utils/fast_dbt/dbt_parser/#module-brocolib_utilsfast_dbtdbt_parser","text":"View Source import logging from brocolib_utils.data_studio import DataStudio def update_exposures ( data : dict ): for element in data [ \"exposures\" ]: exposure_title = element [ \"name\" ] exposure_meta_dict = element [ \"meta\" ] asset_id = exposure_meta_dict . get ( \"asset_id\" ) grants_dict = exposure_meta_dict . get ( \"grants\" ) if grants_dict : client = DataStudio () permissions = client . get_permissions ( asset_id = asset_id ) if permissions [ \"response\" ] != 200 : logging . error ( permissions [ \"response\" ]) elif permissions [ \"response\" ][ \"permissions\" ] != grants_dict : permissions_updated = client . update_permissions ( asset_id = asset_id , permission_dict = { \"permissions\" : grants_dict } ) if permissions_updated [ \"response\" ] != 200 : logging . error ( permissions_updated [ \"response\" ]) else : logging . info ( f \"Permissions for { exposure_title } have been updated successfully !\" ) else : logging . info ( f \"No changes in permissions detected for { exposure_title } \" ) logging . info ( 'DONE' )","title":"Module brocolib_utils.fast_dbt.dbt_parser"},{"location":"reference/brocolib_utils/fast_dbt/dbt_parser/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/fast_dbt/dbt_parser/#update_exposures","text":"def update_exposures ( data : dict ) View Source def update_exposures ( data : dict ) : for element in data [ \"exposures\" ]: exposure_title = element [ \"name\" ] exposure_meta_dict = element [ \"meta\" ] asset_id = exposure_meta_dict . get ( \"asset_id\" ) grants_dict = exposure_meta_dict . get ( \"grants\" ) if grants_dict : client = DataStudio () permissions = client . get_permissions ( asset_id = asset_id ) if permissions [ \"response\" ] != 200 : logging . error ( permissions [ \"response\" ] ) elif permissions [ \"response\" ][ \"permissions\" ] != grants_dict : permissions_updated = client . update_permissions ( asset_id = asset_id , permission_dict = { \"permissions\" : grants_dict } ) if permissions_updated [ \"response\" ] != 200 : logging . error ( permissions_updated [ \"response\" ] ) else : logging . info ( f \"Permissions for {exposure_title} have been updated successfully !\" ) else : logging . info ( f \"No changes in permissions detected for {exposure_title}\" ) logging . info ( 'DONE' )","title":"update_exposures"},{"location":"reference/brocolib_utils/fast_dbt/generator/","text":"Module brocolib_utils.fast_dbt.generator View Source from collections import OrderedDict from ruamel.yaml import YAML from ruamel.yaml.scalarstring import DoubleQuotedScalarString from brocolib_utils.settings import ( TABLE_NAME_COL ) def quoted_presenter ( dumper , data ): return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '\"' ) # define a custom representer for strings class quoted ( str ): pass def quoted_presenter ( dumper , data ): return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '\"' ) def get_loaded_sources ( dataframe ): ls_loaded = [] for table in dataframe . itertuples (): if table . EL != \"\" : ls_loaded . append ( getattr ( table , TABLE_NAME_COL )) return ls_loaded def init_dbt_sources ( database , loader = None , version = 2 ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = version dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = \"stg\" dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources def dict_to_yaml ( yaml_dict , yaml_file_path = \"./stg.yml\" ): yaml = YAML () yaml . default_flow_style = False with open ( yaml_file_path , 'w' ) as file : yaml . dump ( yaml_dict , file ) Variables TABLE_NAME_COL Functions dict_to_yaml def dict_to_yaml ( yaml_dict , yaml_file_path = './stg.yml' ) View Source def dict_to_yaml(yaml_dict, yaml_file_path=\"./stg.yml\"): yaml=YAML() yaml.default_flow_style = False with open(yaml_file_path, 'w') as file: yaml.dump(yaml_dict, file) get_loaded_sources def get_loaded_sources ( dataframe ) View Source def get_loaded_sources ( dataframe ): ls_loaded = [] for table in dataframe . itertuples (): if table . EL != \"\" : ls_loaded . append ( getattr ( table , TABLE_NAME_COL )) return ls_loaded init_dbt_sources def init_dbt_sources ( database , loader = None , version = 2 ) View Source def init_dbt_sources ( database , loader = None , version = 2 ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = version dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = \"stg\" dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources quoted_presenter def quoted_presenter ( dumper , data ) View Source def quoted_presenter ( dumper , data ) : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '\"' ) Classes quoted class quoted ( / , * args , ** kwargs ) str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. View Source class quoted ( str ): pass Ancestors (in MRO) builtins.str Static methods maketrans def maketrans ( ... ) Return a translation table usable for str.translate(). If there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters to Unicode ordinals, strings or None. Character keys will be then converted to ordinals. If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in x will be mapped to the character at the same position in y. If there is a third argument, it must be a string, whose characters will be mapped to None in the result. Methods capitalize def capitalize ( self , / ) Return a capitalized version of the string. More specifically, make the first character have upper case and the rest lower case. casefold def casefold ( self , / ) Return a version of the string suitable for caseless comparisons. center def center ( self , width , fillchar = ' ' , / ) Return a centered string of length width. Padding is done using the specified fill character (default is a space). count def count ( ... ) S.count(sub[, start[, end]]) -> int Return the number of non-overlapping occurrences of substring sub in string S[start:end]. Optional arguments start and end are interpreted as in slice notation. encode def encode ( self , / , encoding = 'utf-8' , errors = 'strict' ) Encode the string using the codec registered for encoding. encoding The encoding in which to encode the string. errors The error handling scheme to use for encoding errors. The default is 'strict' meaning that encoding errors raise a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and 'xmlcharrefreplace' as well as any other name registered with codecs.register_error that can handle UnicodeEncodeErrors. endswith def endswith ( ... ) S.endswith(suffix[, start[, end]]) -> bool Return True if S ends with the specified suffix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. suffix can also be a tuple of strings to try. expandtabs def expandtabs ( self , / , tabsize = 8 ) Return a copy where all tab characters are expanded using spaces. If tabsize is not given, a tab size of 8 characters is assumed. find def find ( ... ) S.find(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. format def format ( ... ) S.format( args, *kwargs) -> str Return a formatted version of S, using substitutions from args and kwargs. The substitutions are identified by braces ('{' and '}'). format_map def format_map ( ... ) S.format_map(mapping) -> str Return a formatted version of S, using substitutions from mapping. The substitutions are identified by braces ('{' and '}'). index def index ( ... ) S.index(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found. isalnum def isalnum ( self , / ) Return True if the string is an alpha-numeric string, False otherwise. A string is alpha-numeric if all characters in the string are alpha-numeric and there is at least one character in the string. isalpha def isalpha ( self , / ) Return True if the string is an alphabetic string, False otherwise. A string is alphabetic if all characters in the string are alphabetic and there is at least one character in the string. isascii def isascii ( self , / ) Return True if all characters in the string are ASCII, False otherwise. ASCII characters have code points in the range U+0000-U+007F. Empty string is ASCII too. isdecimal def isdecimal ( self , / ) Return True if the string is a decimal string, False otherwise. A string is a decimal string if all characters in the string are decimal and there is at least one character in the string. isdigit def isdigit ( self , / ) Return True if the string is a digit string, False otherwise. A string is a digit string if all characters in the string are digits and there is at least one character in the string. isidentifier def isidentifier ( self , / ) Return True if the string is a valid Python identifier, False otherwise. Call keyword.iskeyword(s) to test whether string s is a reserved identifier, such as \"def\" or \"class\". islower def islower ( self , / ) Return True if the string is a lowercase string, False otherwise. A string is lowercase if all cased characters in the string are lowercase and there is at least one cased character in the string. isnumeric def isnumeric ( self , / ) Return True if the string is a numeric string, False otherwise. A string is numeric if all characters in the string are numeric and there is at least one character in the string. isprintable def isprintable ( self , / ) Return True if the string is printable, False otherwise. A string is printable if all of its characters are considered printable in repr() or if it is empty. isspace def isspace ( self , / ) Return True if the string is a whitespace string, False otherwise. A string is whitespace if all characters in the string are whitespace and there is at least one character in the string. istitle def istitle ( self , / ) Return True if the string is a title-cased string, False otherwise. In a title-cased string, upper- and title-case characters may only follow uncased characters and lowercase characters only cased ones. isupper def isupper ( self , / ) Return True if the string is an uppercase string, False otherwise. A string is uppercase if all cased characters in the string are uppercase and there is at least one cased character in the string. join def join ( self , iterable , / ) Concatenate any number of strings. The string whose method is called is inserted in between each given string. The result is returned as a new string. Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs' ljust def ljust ( self , width , fillchar = ' ' , / ) Return a left-justified string of length width. Padding is done using the specified fill character (default is a space). lower def lower ( self , / ) Return a copy of the string converted to lowercase. lstrip def lstrip ( self , chars = None , / ) Return a copy of the string with leading whitespace removed. If chars is given and not None, remove characters in chars instead. partition def partition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing the original string and two empty strings. replace def replace ( self , old , new , count =- 1 , / ) Return a copy with all occurrences of substring old replaced by new. count Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences. If the optional argument count is given, only the first count occurrences are replaced. rfind def rfind ( ... ) S.rfind(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. rindex def rindex ( ... ) S.rindex(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found. rjust def rjust ( self , width , fillchar = ' ' , / ) Return a right-justified string of length width. Padding is done using the specified fill character (default is a space). rpartition def rpartition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string, starting at the end. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing two empty strings and the original string. rsplit def rsplit ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. Splits are done starting at the end of the string and working to the front. rstrip def rstrip ( self , chars = None , / ) Return a copy of the string with trailing whitespace removed. If chars is given and not None, remove characters in chars instead. split def split ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. splitlines def splitlines ( self , / , keepends = False ) Return a list of the lines in the string, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true. startswith def startswith ( ... ) S.startswith(prefix[, start[, end]]) -> bool Return True if S starts with the specified prefix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. prefix can also be a tuple of strings to try. strip def strip ( self , chars = None , / ) Return a copy of the string with leading and trailing whitespace removed. If chars is given and not None, remove characters in chars instead. swapcase def swapcase ( self , / ) Convert uppercase characters to lowercase and lowercase characters to uppercase. title def title ( self , / ) Return a version of the string where each word is titlecased. More specifically, words start with uppercased characters and all remaining cased characters have lower case. translate def translate ( self , table , / ) Replace each character in the string using the given translation table. table Translation table, which must be a mapping of Unicode ordinals to Unicode ordinals, strings, or None. The table must implement lookup/indexing via getitem , for instance a dictionary or list. If this operation raises LookupError, the character is left untouched. Characters mapped to None are deleted. upper def upper ( self , / ) Return a copy of the string converted to uppercase. zfill def zfill ( self , width , / ) Pad a numeric string with zeros on the left, to fill a field of the given width. The string is never truncated.","title":"Generator"},{"location":"reference/brocolib_utils/fast_dbt/generator/#module-brocolib_utilsfast_dbtgenerator","text":"View Source from collections import OrderedDict from ruamel.yaml import YAML from ruamel.yaml.scalarstring import DoubleQuotedScalarString from brocolib_utils.settings import ( TABLE_NAME_COL ) def quoted_presenter ( dumper , data ): return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '\"' ) # define a custom representer for strings class quoted ( str ): pass def quoted_presenter ( dumper , data ): return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '\"' ) def get_loaded_sources ( dataframe ): ls_loaded = [] for table in dataframe . itertuples (): if table . EL != \"\" : ls_loaded . append ( getattr ( table , TABLE_NAME_COL )) return ls_loaded def init_dbt_sources ( database , loader = None , version = 2 ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = version dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = \"stg\" dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources def dict_to_yaml ( yaml_dict , yaml_file_path = \"./stg.yml\" ): yaml = YAML () yaml . default_flow_style = False with open ( yaml_file_path , 'w' ) as file : yaml . dump ( yaml_dict , file )","title":"Module brocolib_utils.fast_dbt.generator"},{"location":"reference/brocolib_utils/fast_dbt/generator/#variables","text":"TABLE_NAME_COL","title":"Variables"},{"location":"reference/brocolib_utils/fast_dbt/generator/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/fast_dbt/generator/#dict_to_yaml","text":"def dict_to_yaml ( yaml_dict , yaml_file_path = './stg.yml' ) View Source def dict_to_yaml(yaml_dict, yaml_file_path=\"./stg.yml\"): yaml=YAML() yaml.default_flow_style = False with open(yaml_file_path, 'w') as file: yaml.dump(yaml_dict, file)","title":"dict_to_yaml"},{"location":"reference/brocolib_utils/fast_dbt/generator/#get_loaded_sources","text":"def get_loaded_sources ( dataframe ) View Source def get_loaded_sources ( dataframe ): ls_loaded = [] for table in dataframe . itertuples (): if table . EL != \"\" : ls_loaded . append ( getattr ( table , TABLE_NAME_COL )) return ls_loaded","title":"get_loaded_sources"},{"location":"reference/brocolib_utils/fast_dbt/generator/#init_dbt_sources","text":"def init_dbt_sources ( database , loader = None , version = 2 ) View Source def init_dbt_sources ( database , loader = None , version = 2 ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = version dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = \"stg\" dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources","title":"init_dbt_sources"},{"location":"reference/brocolib_utils/fast_dbt/generator/#quoted_presenter","text":"def quoted_presenter ( dumper , data ) View Source def quoted_presenter ( dumper , data ) : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '\"' )","title":"quoted_presenter"},{"location":"reference/brocolib_utils/fast_dbt/generator/#classes","text":"","title":"Classes"},{"location":"reference/brocolib_utils/fast_dbt/generator/#quoted","text":"class quoted ( / , * args , ** kwargs ) str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. View Source class quoted ( str ): pass","title":"quoted"},{"location":"reference/brocolib_utils/fast_dbt/generator/#ancestors-in-mro","text":"builtins.str","title":"Ancestors (in MRO)"},{"location":"reference/brocolib_utils/fast_dbt/generator/#static-methods","text":"","title":"Static methods"},{"location":"reference/brocolib_utils/fast_dbt/generator/#maketrans","text":"def maketrans ( ... ) Return a translation table usable for str.translate(). If there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters to Unicode ordinals, strings or None. Character keys will be then converted to ordinals. If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in x will be mapped to the character at the same position in y. If there is a third argument, it must be a string, whose characters will be mapped to None in the result.","title":"maketrans"},{"location":"reference/brocolib_utils/fast_dbt/generator/#methods","text":"","title":"Methods"},{"location":"reference/brocolib_utils/fast_dbt/generator/#capitalize","text":"def capitalize ( self , / ) Return a capitalized version of the string. More specifically, make the first character have upper case and the rest lower case.","title":"capitalize"},{"location":"reference/brocolib_utils/fast_dbt/generator/#casefold","text":"def casefold ( self , / ) Return a version of the string suitable for caseless comparisons.","title":"casefold"},{"location":"reference/brocolib_utils/fast_dbt/generator/#center","text":"def center ( self , width , fillchar = ' ' , / ) Return a centered string of length width. Padding is done using the specified fill character (default is a space).","title":"center"},{"location":"reference/brocolib_utils/fast_dbt/generator/#count","text":"def count ( ... ) S.count(sub[, start[, end]]) -> int Return the number of non-overlapping occurrences of substring sub in string S[start:end]. Optional arguments start and end are interpreted as in slice notation.","title":"count"},{"location":"reference/brocolib_utils/fast_dbt/generator/#encode","text":"def encode ( self , / , encoding = 'utf-8' , errors = 'strict' ) Encode the string using the codec registered for encoding. encoding The encoding in which to encode the string. errors The error handling scheme to use for encoding errors. The default is 'strict' meaning that encoding errors raise a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and 'xmlcharrefreplace' as well as any other name registered with codecs.register_error that can handle UnicodeEncodeErrors.","title":"encode"},{"location":"reference/brocolib_utils/fast_dbt/generator/#endswith","text":"def endswith ( ... ) S.endswith(suffix[, start[, end]]) -> bool Return True if S ends with the specified suffix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. suffix can also be a tuple of strings to try.","title":"endswith"},{"location":"reference/brocolib_utils/fast_dbt/generator/#expandtabs","text":"def expandtabs ( self , / , tabsize = 8 ) Return a copy where all tab characters are expanded using spaces. If tabsize is not given, a tab size of 8 characters is assumed.","title":"expandtabs"},{"location":"reference/brocolib_utils/fast_dbt/generator/#find","text":"def find ( ... ) S.find(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.","title":"find"},{"location":"reference/brocolib_utils/fast_dbt/generator/#format","text":"def format ( ... ) S.format( args, *kwargs) -> str Return a formatted version of S, using substitutions from args and kwargs. The substitutions are identified by braces ('{' and '}').","title":"format"},{"location":"reference/brocolib_utils/fast_dbt/generator/#format_map","text":"def format_map ( ... ) S.format_map(mapping) -> str Return a formatted version of S, using substitutions from mapping. The substitutions are identified by braces ('{' and '}').","title":"format_map"},{"location":"reference/brocolib_utils/fast_dbt/generator/#index","text":"def index ( ... ) S.index(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found.","title":"index"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isalnum","text":"def isalnum ( self , / ) Return True if the string is an alpha-numeric string, False otherwise. A string is alpha-numeric if all characters in the string are alpha-numeric and there is at least one character in the string.","title":"isalnum"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isalpha","text":"def isalpha ( self , / ) Return True if the string is an alphabetic string, False otherwise. A string is alphabetic if all characters in the string are alphabetic and there is at least one character in the string.","title":"isalpha"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isascii","text":"def isascii ( self , / ) Return True if all characters in the string are ASCII, False otherwise. ASCII characters have code points in the range U+0000-U+007F. Empty string is ASCII too.","title":"isascii"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isdecimal","text":"def isdecimal ( self , / ) Return True if the string is a decimal string, False otherwise. A string is a decimal string if all characters in the string are decimal and there is at least one character in the string.","title":"isdecimal"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isdigit","text":"def isdigit ( self , / ) Return True if the string is a digit string, False otherwise. A string is a digit string if all characters in the string are digits and there is at least one character in the string.","title":"isdigit"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isidentifier","text":"def isidentifier ( self , / ) Return True if the string is a valid Python identifier, False otherwise. Call keyword.iskeyword(s) to test whether string s is a reserved identifier, such as \"def\" or \"class\".","title":"isidentifier"},{"location":"reference/brocolib_utils/fast_dbt/generator/#islower","text":"def islower ( self , / ) Return True if the string is a lowercase string, False otherwise. A string is lowercase if all cased characters in the string are lowercase and there is at least one cased character in the string.","title":"islower"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isnumeric","text":"def isnumeric ( self , / ) Return True if the string is a numeric string, False otherwise. A string is numeric if all characters in the string are numeric and there is at least one character in the string.","title":"isnumeric"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isprintable","text":"def isprintable ( self , / ) Return True if the string is printable, False otherwise. A string is printable if all of its characters are considered printable in repr() or if it is empty.","title":"isprintable"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isspace","text":"def isspace ( self , / ) Return True if the string is a whitespace string, False otherwise. A string is whitespace if all characters in the string are whitespace and there is at least one character in the string.","title":"isspace"},{"location":"reference/brocolib_utils/fast_dbt/generator/#istitle","text":"def istitle ( self , / ) Return True if the string is a title-cased string, False otherwise. In a title-cased string, upper- and title-case characters may only follow uncased characters and lowercase characters only cased ones.","title":"istitle"},{"location":"reference/brocolib_utils/fast_dbt/generator/#isupper","text":"def isupper ( self , / ) Return True if the string is an uppercase string, False otherwise. A string is uppercase if all cased characters in the string are uppercase and there is at least one cased character in the string.","title":"isupper"},{"location":"reference/brocolib_utils/fast_dbt/generator/#join","text":"def join ( self , iterable , / ) Concatenate any number of strings. The string whose method is called is inserted in between each given string. The result is returned as a new string. Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'","title":"join"},{"location":"reference/brocolib_utils/fast_dbt/generator/#ljust","text":"def ljust ( self , width , fillchar = ' ' , / ) Return a left-justified string of length width. Padding is done using the specified fill character (default is a space).","title":"ljust"},{"location":"reference/brocolib_utils/fast_dbt/generator/#lower","text":"def lower ( self , / ) Return a copy of the string converted to lowercase.","title":"lower"},{"location":"reference/brocolib_utils/fast_dbt/generator/#lstrip","text":"def lstrip ( self , chars = None , / ) Return a copy of the string with leading whitespace removed. If chars is given and not None, remove characters in chars instead.","title":"lstrip"},{"location":"reference/brocolib_utils/fast_dbt/generator/#partition","text":"def partition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing the original string and two empty strings.","title":"partition"},{"location":"reference/brocolib_utils/fast_dbt/generator/#replace","text":"def replace ( self , old , new , count =- 1 , / ) Return a copy with all occurrences of substring old replaced by new. count Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences. If the optional argument count is given, only the first count occurrences are replaced.","title":"replace"},{"location":"reference/brocolib_utils/fast_dbt/generator/#rfind","text":"def rfind ( ... ) S.rfind(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.","title":"rfind"},{"location":"reference/brocolib_utils/fast_dbt/generator/#rindex","text":"def rindex ( ... ) S.rindex(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found.","title":"rindex"},{"location":"reference/brocolib_utils/fast_dbt/generator/#rjust","text":"def rjust ( self , width , fillchar = ' ' , / ) Return a right-justified string of length width. Padding is done using the specified fill character (default is a space).","title":"rjust"},{"location":"reference/brocolib_utils/fast_dbt/generator/#rpartition","text":"def rpartition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string, starting at the end. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing two empty strings and the original string.","title":"rpartition"},{"location":"reference/brocolib_utils/fast_dbt/generator/#rsplit","text":"def rsplit ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. Splits are done starting at the end of the string and working to the front.","title":"rsplit"},{"location":"reference/brocolib_utils/fast_dbt/generator/#rstrip","text":"def rstrip ( self , chars = None , / ) Return a copy of the string with trailing whitespace removed. If chars is given and not None, remove characters in chars instead.","title":"rstrip"},{"location":"reference/brocolib_utils/fast_dbt/generator/#split","text":"def split ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit.","title":"split"},{"location":"reference/brocolib_utils/fast_dbt/generator/#splitlines","text":"def splitlines ( self , / , keepends = False ) Return a list of the lines in the string, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true.","title":"splitlines"},{"location":"reference/brocolib_utils/fast_dbt/generator/#startswith","text":"def startswith ( ... ) S.startswith(prefix[, start[, end]]) -> bool Return True if S starts with the specified prefix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. prefix can also be a tuple of strings to try.","title":"startswith"},{"location":"reference/brocolib_utils/fast_dbt/generator/#strip","text":"def strip ( self , chars = None , / ) Return a copy of the string with leading and trailing whitespace removed. If chars is given and not None, remove characters in chars instead.","title":"strip"},{"location":"reference/brocolib_utils/fast_dbt/generator/#swapcase","text":"def swapcase ( self , / ) Convert uppercase characters to lowercase and lowercase characters to uppercase.","title":"swapcase"},{"location":"reference/brocolib_utils/fast_dbt/generator/#title","text":"def title ( self , / ) Return a version of the string where each word is titlecased. More specifically, words start with uppercased characters and all remaining cased characters have lower case.","title":"title"},{"location":"reference/brocolib_utils/fast_dbt/generator/#translate","text":"def translate ( self , table , / ) Replace each character in the string using the given translation table. table Translation table, which must be a mapping of Unicode ordinals to Unicode ordinals, strings, or None. The table must implement lookup/indexing via getitem , for instance a dictionary or list. If this operation raises LookupError, the character is left untouched. Characters mapped to None are deleted.","title":"translate"},{"location":"reference/brocolib_utils/fast_dbt/generator/#upper","text":"def upper ( self , / ) Return a copy of the string converted to uppercase.","title":"upper"},{"location":"reference/brocolib_utils/fast_dbt/generator/#zfill","text":"def zfill ( self , width , / ) Pad a numeric string with zeros on the left, to fill a field of the given width. The string is never truncated.","title":"zfill"},{"location":"reference/brocolib_utils/fast_dbt/new_generator/","text":"Module brocolib_utils.fast_dbt.new_generator View Source from ruamel.yaml.scalarstring import DoubleQuotedScalarString from brocolib_utils.fast_dbt.ddm import sources_parser , sheet_parser from brocolib_utils.fast_dbt.ddm import settings as codegen_settings from brocolib_utils.datalake import datalake from brocolib_utils import settings import pandas as pd def generate_source_yaml_asdict ( source_name : str , datalake_bucket : str = None ): dc_source_tables = datalake . get_source ( source_name = source_name , datalake_bucket = datalake_bucket ) all_sources_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCES ) source_description = all_sources_df . query ( f \"source_name==' { source_name } '\" )[ \"description\" ] . iloc [ 0 ] or None all_tables_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_TABLES ) all_columns_df , _ = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_COLUMNS , worksheet = spreadsheet ) init_dbt_sources_dict = init_dbt_sources ( database = settings . DATALAKE_PROJECT , source_name = source_name , source_description = source_description ) dbt_sources_dict = generate_loaded_tables_specs ( loaded_sources = dc_source_tables , init_dbt_sources_dict = init_dbt_sources_dict , all_tables = all_tables_df , all_columns = all_columns_df ) return dbt_sources_dict def init_dbt_sources ( database : str , source_name : str , source_description : str = None ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = \"2\" dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = source_name dc_source [ \"description\" ] = DoubleQuotedScalarString ( source_description ) dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources def generate_loaded_tables_specs ( loaded_sources : dict , init_dbt_sources_dict : dict , all_tables : pd . DataFrame , all_columns : pd . DataFrame ): for table , path in loaded_sources . items (): table_description = all_tables . query ( f \"table_name==' { table } '\" )[ \"description\" ] . iloc [ 0 ] dc_table = {} dc_table [ \"name\" ] = table dc_table [ \"description\" ] = DoubleQuotedScalarString ( table_description ) dc_table [ \"external\" ] = {} dc_table [ \"external\" ][ \"location\" ] = DoubleQuotedScalarString ( f \" { path } *\" ) dc_table [ \"external\" ][ \"options\" ] = {} dc_table [ \"external\" ][ \"options\" ][ \"format\" ] = \"parquet\" dc_table [ \"external\" ][ \"options\" ][ \"hive_partition_uri_prefix\" ] = DoubleQuotedScalarString ( path ) # dc_table[\"external\"][\"partitions\"] = [{\"name\":\"year\",\"data_type\":\"integer\"}, # {\"name\":\"month\",\"data_type\":\"integer\"}] df_table_columns = all_columns . query ( f \"table_name==' { table } '\" ) dc_table [ \"columns\" ] = [] for col in df_table_columns . itertuples (): dc_table [ \"columns\" ] . append ( { \"name\" : col . column_name , \"data_type\" : col . data_type , \"description\" : DoubleQuotedScalarString ( col . description ) } ) init_dbt_sources_dict [ \"sources\" ][ 0 ][ \"tables\" ] . append ( dc_table ) return init_dbt_sources_dict Functions generate_loaded_tables_specs def generate_loaded_tables_specs ( loaded_sources : dict , init_dbt_sources_dict : dict , all_tables : pandas . core . frame . DataFrame , all_columns : pandas . core . frame . DataFrame ) View Source def generate_loaded_tables_specs ( loaded_sources : dict , init_dbt_sources_dict : dict , all_tables : pd . DataFrame , all_columns : pd . DataFrame ): for table , path in loaded_sources . items (): table_description = all_tables . query ( f \"table_name=='{table}'\" )[ \"description\" ] . iloc [ 0 ] dc_table = {} dc_table [ \"name\" ] = table dc_table [ \"description\" ] = DoubleQuotedScalarString ( table_description ) dc_table [ \"external\" ] = {} dc_table [ \"external\" ][ \"location\" ] = DoubleQuotedScalarString ( f \"{path}*\" ) dc_table [ \"external\" ][ \"options\" ] = {} dc_table [ \"external\" ][ \"options\" ][ \"format\" ] = \"parquet\" dc_table [ \"external\" ][ \"options\" ][ \"hive_partition_uri_prefix\" ] = DoubleQuotedScalarString ( path ) # dc_table[\"external\"][\"partitions\"] = [{\"name\":\"year\",\"data_type\":\"integer\"}, # {\"name\":\"month\",\"data_type\":\"integer\"}] df_table_columns = all_columns . query ( f \"table_name=='{table}'\" ) dc_table [ \"columns\" ] = [] for col in df_table_columns . itertuples (): dc_table [ \"columns\" ] . append ( { \"name\" : col . column_name , \"data_type\" : col . data_type , \"description\" : DoubleQuotedScalarString ( col . description ) } ) init_dbt_sources_dict [ \"sources\" ][ 0 ][ \"tables\" ] . append ( dc_table ) return init_dbt_sources_dict generate_source_yaml_asdict def generate_source_yaml_asdict ( source_name : str , datalake_bucket : str = None ) View Source def generate_source_yaml_asdict ( source_name : str , datalake_bucket : str = None ): dc_source_tables = datalake . get_source ( source_name = source_name , datalake_bucket = datalake_bucket ) all_sources_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCES ) source_description = all_sources_df . query ( f \"source_name=='{source_name}'\" )[ \"description\" ] . iloc [ 0 ] or None all_tables_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_TABLES ) all_columns_df , _ = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_COLUMNS , worksheet = spreadsheet ) init_dbt_sources_dict = init_dbt_sources ( database = settings . DATALAKE_PROJECT , source_name = source_name , source_description = source_description ) dbt_sources_dict = generate_loaded_tables_specs ( loaded_sources = dc_source_tables , init_dbt_sources_dict = init_dbt_sources_dict , all_tables = all_tables_df , all_columns = all_columns_df ) return dbt_sources_dict init_dbt_sources def init_dbt_sources ( database : str , source_name : str , source_description : str = None ) View Source def init_dbt_sources ( database : str , source_name : str , source_description : str = None ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = \"2\" dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = source_name dc_source [ \"description\" ] = DoubleQuotedScalarString ( source_description ) dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources","title":"New Generator"},{"location":"reference/brocolib_utils/fast_dbt/new_generator/#module-brocolib_utilsfast_dbtnew_generator","text":"View Source from ruamel.yaml.scalarstring import DoubleQuotedScalarString from brocolib_utils.fast_dbt.ddm import sources_parser , sheet_parser from brocolib_utils.fast_dbt.ddm import settings as codegen_settings from brocolib_utils.datalake import datalake from brocolib_utils import settings import pandas as pd def generate_source_yaml_asdict ( source_name : str , datalake_bucket : str = None ): dc_source_tables = datalake . get_source ( source_name = source_name , datalake_bucket = datalake_bucket ) all_sources_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCES ) source_description = all_sources_df . query ( f \"source_name==' { source_name } '\" )[ \"description\" ] . iloc [ 0 ] or None all_tables_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_TABLES ) all_columns_df , _ = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_COLUMNS , worksheet = spreadsheet ) init_dbt_sources_dict = init_dbt_sources ( database = settings . DATALAKE_PROJECT , source_name = source_name , source_description = source_description ) dbt_sources_dict = generate_loaded_tables_specs ( loaded_sources = dc_source_tables , init_dbt_sources_dict = init_dbt_sources_dict , all_tables = all_tables_df , all_columns = all_columns_df ) return dbt_sources_dict def init_dbt_sources ( database : str , source_name : str , source_description : str = None ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = \"2\" dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = source_name dc_source [ \"description\" ] = DoubleQuotedScalarString ( source_description ) dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources def generate_loaded_tables_specs ( loaded_sources : dict , init_dbt_sources_dict : dict , all_tables : pd . DataFrame , all_columns : pd . DataFrame ): for table , path in loaded_sources . items (): table_description = all_tables . query ( f \"table_name==' { table } '\" )[ \"description\" ] . iloc [ 0 ] dc_table = {} dc_table [ \"name\" ] = table dc_table [ \"description\" ] = DoubleQuotedScalarString ( table_description ) dc_table [ \"external\" ] = {} dc_table [ \"external\" ][ \"location\" ] = DoubleQuotedScalarString ( f \" { path } *\" ) dc_table [ \"external\" ][ \"options\" ] = {} dc_table [ \"external\" ][ \"options\" ][ \"format\" ] = \"parquet\" dc_table [ \"external\" ][ \"options\" ][ \"hive_partition_uri_prefix\" ] = DoubleQuotedScalarString ( path ) # dc_table[\"external\"][\"partitions\"] = [{\"name\":\"year\",\"data_type\":\"integer\"}, # {\"name\":\"month\",\"data_type\":\"integer\"}] df_table_columns = all_columns . query ( f \"table_name==' { table } '\" ) dc_table [ \"columns\" ] = [] for col in df_table_columns . itertuples (): dc_table [ \"columns\" ] . append ( { \"name\" : col . column_name , \"data_type\" : col . data_type , \"description\" : DoubleQuotedScalarString ( col . description ) } ) init_dbt_sources_dict [ \"sources\" ][ 0 ][ \"tables\" ] . append ( dc_table ) return init_dbt_sources_dict","title":"Module brocolib_utils.fast_dbt.new_generator"},{"location":"reference/brocolib_utils/fast_dbt/new_generator/#functions","text":"","title":"Functions"},{"location":"reference/brocolib_utils/fast_dbt/new_generator/#generate_loaded_tables_specs","text":"def generate_loaded_tables_specs ( loaded_sources : dict , init_dbt_sources_dict : dict , all_tables : pandas . core . frame . DataFrame , all_columns : pandas . core . frame . DataFrame ) View Source def generate_loaded_tables_specs ( loaded_sources : dict , init_dbt_sources_dict : dict , all_tables : pd . DataFrame , all_columns : pd . DataFrame ): for table , path in loaded_sources . items (): table_description = all_tables . query ( f \"table_name=='{table}'\" )[ \"description\" ] . iloc [ 0 ] dc_table = {} dc_table [ \"name\" ] = table dc_table [ \"description\" ] = DoubleQuotedScalarString ( table_description ) dc_table [ \"external\" ] = {} dc_table [ \"external\" ][ \"location\" ] = DoubleQuotedScalarString ( f \"{path}*\" ) dc_table [ \"external\" ][ \"options\" ] = {} dc_table [ \"external\" ][ \"options\" ][ \"format\" ] = \"parquet\" dc_table [ \"external\" ][ \"options\" ][ \"hive_partition_uri_prefix\" ] = DoubleQuotedScalarString ( path ) # dc_table[\"external\"][\"partitions\"] = [{\"name\":\"year\",\"data_type\":\"integer\"}, # {\"name\":\"month\",\"data_type\":\"integer\"}] df_table_columns = all_columns . query ( f \"table_name=='{table}'\" ) dc_table [ \"columns\" ] = [] for col in df_table_columns . itertuples (): dc_table [ \"columns\" ] . append ( { \"name\" : col . column_name , \"data_type\" : col . data_type , \"description\" : DoubleQuotedScalarString ( col . description ) } ) init_dbt_sources_dict [ \"sources\" ][ 0 ][ \"tables\" ] . append ( dc_table ) return init_dbt_sources_dict","title":"generate_loaded_tables_specs"},{"location":"reference/brocolib_utils/fast_dbt/new_generator/#generate_source_yaml_asdict","text":"def generate_source_yaml_asdict ( source_name : str , datalake_bucket : str = None ) View Source def generate_source_yaml_asdict ( source_name : str , datalake_bucket : str = None ): dc_source_tables = datalake . get_source ( source_name = source_name , datalake_bucket = datalake_bucket ) all_sources_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCES ) source_description = all_sources_df . query ( f \"source_name=='{source_name}'\" )[ \"description\" ] . iloc [ 0 ] or None all_tables_df , spreadsheet = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_TABLES ) all_columns_df , _ = sheet_parser . ddm_sheet_to_df ( sheet_name = codegen_settings . DDM_SHEET_NAMES . SOURCE_COLUMNS , worksheet = spreadsheet ) init_dbt_sources_dict = init_dbt_sources ( database = settings . DATALAKE_PROJECT , source_name = source_name , source_description = source_description ) dbt_sources_dict = generate_loaded_tables_specs ( loaded_sources = dc_source_tables , init_dbt_sources_dict = init_dbt_sources_dict , all_tables = all_tables_df , all_columns = all_columns_df ) return dbt_sources_dict","title":"generate_source_yaml_asdict"},{"location":"reference/brocolib_utils/fast_dbt/new_generator/#init_dbt_sources","text":"def init_dbt_sources ( database : str , source_name : str , source_description : str = None ) View Source def init_dbt_sources ( database : str , source_name : str , source_description : str = None ): # dc_dbt_sources = OrderedDict() dc_dbt_sources = {} dc_dbt_sources [ \"version\" ] = \"2\" dc_dbt_sources [ \"sources\" ] = [] # for source in getattr(sources_dataframe, SOURCE_DATASET_COL).unique(): # dc_source = OrderedDict() dc_source = {} dc_source [ \"name\" ] = source_name dc_source [ \"description\" ] = DoubleQuotedScalarString ( source_description ) dc_source [ \"database\" ] = database dc_source [ \"loader\" ] = \"gcloud storage\" dc_source [ \"tables\" ] = [] dc_dbt_sources [ \"sources\" ] . append ( dict ( dc_source )) return dc_dbt_sources","title":"init_dbt_sources"}]}